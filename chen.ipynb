{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./venv/lib/python3.10/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk) (1.2.0)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from nltk) (4.64.1)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.10/site-packages (from nltk) (2022.10.31)\r\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk) (8.1.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "\n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn\n",
    "\n",
    "train_file = \"data/yue-eng2.txt\"\n",
    "dev_file = \"data/yue-eng.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BOS', 'begin', '.', 'EOS'], ['BOS', 'hello', '!', 'EOS']]\n",
      "[['BOS', '開', '始', '啦', '。', 'EOS'], ['BOS', '哈', '佬', '。', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(dev_en[:2])\n",
    "print(dev_cn[:2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1\n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 2\n",
    "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
    "    word_dict[\"UNK\"] = UNK_IDX\n",
    "    word_dict[\"PAD\"] = PAD_IDX\n",
    "    return word_dict, total_words      #total_words所有单词数，最大50002\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}    #英文：索引到单词\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}    #中文：索引到字"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "\n",
    "    # sort sentences by word lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "\n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 81, 1749, 3]\n",
      "['BOS', '手', '鏈', 'EOS']\n",
      "['BOS', 'bracelet', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "print(train_cn[2])\n",
    "print([inv_cn_dict[i] for i in train_cn[2]])\n",
    "print([inv_en_dict[i] for i in train_en[2]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=True):  #n是传进来的句子数\n",
    "    idx_list = np.arange(0, n, minibatch_size)   #[0, 1, ..., n-1]按minibatch_size大小分割\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),\n array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]),\n array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]),\n array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])]"
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_minibatches(100, 15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "outputs": [],
   "source": [
    "def prepare_data(seqs):   #seqs传入的是minibatches中的一个minibatch对应的batch_size个句子索引（嵌套列表），此处batch_size=64\n",
    "\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)  #batch_size个句子中最长句子长度\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex     #返回内容依次是batch_size个英文句子索引，英文句子长度，中文句子索引，中文句子长度\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):   #把mask的部分忽略掉\n",
    "        # input: (batch_size * seq_len) * vocab_size\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        # target: batch_size * 1\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)          #双向，所以拼接\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
    "\n",
    "        return out, hid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):  # a mask of shape x_len * y_len\n",
    "\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
    "        mask = ( ~(x_mask[:, :, None]) * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
    "\n",
    "        output, attn = self.attention(output_seq, ctx, mask)  #根据原来的output_seq和context来计算\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "\n",
    "        return output, hid, attn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out,\n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, attn\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out,\n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        # input: (batch_size * seq_len) * vocab_size\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        # target: batch_size * 1\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#0.2\n",
    "dropout = 0.2\n",
    "#100\n",
    "embed_size = hidden_size = 500\n",
    "encoder = Encoder(vocab_size=en_total_words, embed_size=embed_size, enc_hidden_size=hidden_size, dec_hidden_size=hidden_size, dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words, embed_size=embed_size, enc_hidden_size=hidden_size, dec_hidden_size=hidden_size, dropout=dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)\n",
    "    eval_losses.append(total_loss/total_num_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# def train(model, train_data, dev_data, num_epochs=20):\n",
    "#     train_losses = []\n",
    "#     dev_losses = []\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         train_num_words = train_loss = 0.\n",
    "#         for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(train_data):\n",
    "#             mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "#             mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "#             mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "#             mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "#             mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "#             mb_y_len[mb_y_len<=0] = 1\n",
    "#\n",
    "#             mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "#\n",
    "#             mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "#             mb_out_mask = mb_out_mask.float()\n",
    "#\n",
    "#             loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "#\n",
    "#             num_words = torch.sum(mb_y_len).item()\n",
    "#             train_loss += loss.item() * num_words\n",
    "#             train_num_words += num_words\n",
    "#\n",
    "#             # 更新模型\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "#             optimizer.step()\n",
    "#\n",
    "#             if it % 100 == 0:\n",
    "#                 print(\"Epoch\", epoch, \"iteration\", it, \"train loss\", loss.item())\n",
    "#\n",
    "#         train_loss /= train_num_words\n",
    "#         train_losses.append(train_loss)\n",
    "#         print(\"Epoch\", epoch, \"Training loss\", train_loss)\n",
    "#\n",
    "#         dev_num_words = dev_loss = 0.\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(dev_data):\n",
    "#                 mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "#                 mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "#                 mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "#                 mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "#                 mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "#                 mb_y_len[mb_y_len<=0] = 1\n",
    "#\n",
    "#                 mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "#\n",
    "#                 mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "#                 mb_out_mask = mb_out_mask.float()\n",
    "#\n",
    "#                 loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "#\n",
    "#                 num_words = torch.sum(mb_y_len).item()\n",
    "#                 dev_loss += loss.item() * num_words\n",
    "#                 dev_num_words += num_words\n",
    "#\n",
    "#             dev_loss /= dev_num_words\n",
    "#             dev_losses.append(dev_loss)\n",
    "#             print(\"Epoch\", epoch, \"Validation loss\", dev_loss)\n",
    "#\n",
    "#         if epoch % 5 == 0:\n",
    "#             evaluate(model, dev_data)\n",
    "#\n",
    "#     # Generate the graph\n",
    "#     plt.plot(range(num_epochs), train_losses, label='Train')\n",
    "#     plt.plot(range(num_epochs), dev_losses, label='Validation')\n",
    "#     plt.title('Training and Validation Loss Over Time')\n",
    "#     plt.xlabel('Epoch')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "def train(model, data, num_epochs=20):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "\n",
    "            # 更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "\n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "\n",
    "        train_loss = total_loss/total_num_words\n",
    "        train_losses.append(train_loss)\n",
    "        print(\"Epoch\", epoch, \"Training loss\", train_loss)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/xc_yp0q16ps2v4zqf84_bbxc0000gn/T/ipykernel_77389/1649215179.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  attn.data.masked_fill(mask, -1e6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.114058494567871\n",
      "Epoch 0 iteration 100 loss 5.036167144775391\n",
      "Epoch 0 iteration 200 loss 3.99971866607666\n",
      "Epoch 0 Training loss 5.283748876407639\n",
      "Evaluation loss 4.40329669223758\n",
      "Epoch 1 iteration 0 loss 4.484957218170166\n",
      "Epoch 1 iteration 100 loss 4.0497145652771\n",
      "Epoch 1 iteration 200 loss 3.272650718688965\n",
      "Epoch 1 Training loss 4.312491681022675\n",
      "Epoch 2 iteration 0 loss 3.8279590606689453\n",
      "Epoch 2 iteration 100 loss 3.522714376449585\n",
      "Epoch 2 iteration 200 loss 2.616412878036499\n",
      "Epoch 2 Training loss 3.7586550394879232\n",
      "Epoch 3 iteration 0 loss 3.3052632808685303\n",
      "Epoch 3 iteration 100 loss 3.1178224086761475\n",
      "Epoch 3 iteration 200 loss 2.069932222366333\n",
      "Epoch 3 Training loss 3.3292234561675085\n",
      "Epoch 4 iteration 0 loss 2.783249616622925\n",
      "Epoch 4 iteration 100 loss 2.7529373168945312\n",
      "Epoch 4 iteration 200 loss 1.640042781829834\n",
      "Epoch 4 Training loss 2.958110513023686\n",
      "Epoch 5 iteration 0 loss 2.4327805042266846\n",
      "Epoch 5 iteration 100 loss 2.4295401573181152\n",
      "Epoch 5 iteration 200 loss 1.267321228981018\n",
      "Epoch 5 Training loss 2.629351006837794\n",
      "Evaluation loss 3.794731282134689\n",
      "Epoch 6 iteration 0 loss 2.0926082134246826\n",
      "Epoch 6 iteration 100 loss 2.125645637512207\n",
      "Epoch 6 iteration 200 loss 0.9399430751800537\n",
      "Epoch 6 Training loss 2.333364686448987\n",
      "Epoch 7 iteration 0 loss 1.6972441673278809\n",
      "Epoch 7 iteration 100 loss 1.8329424858093262\n",
      "Epoch 7 iteration 200 loss 0.7314753532409668\n",
      "Epoch 7 Training loss 2.069129560853707\n",
      "Epoch 8 iteration 0 loss 1.472142219543457\n",
      "Epoch 8 iteration 100 loss 1.5965709686279297\n",
      "Epoch 8 iteration 200 loss 0.5756929516792297\n",
      "Epoch 8 Training loss 1.8508082363935938\n",
      "Epoch 9 iteration 0 loss 1.2575196027755737\n",
      "Epoch 9 iteration 100 loss 1.4312169551849365\n",
      "Epoch 9 iteration 200 loss 0.4827430546283722\n",
      "Epoch 9 Training loss 1.6731592754120406\n",
      "Epoch 10 iteration 0 loss 1.0846543312072754\n",
      "Epoch 10 iteration 100 loss 1.224231243133545\n",
      "Epoch 10 iteration 200 loss 0.39359861612319946\n",
      "Epoch 10 Training loss 1.5153205886651269\n",
      "Evaluation loss 4.21728604800039\n",
      "Epoch 11 iteration 0 loss 0.9771147966384888\n",
      "Epoch 11 iteration 100 loss 1.043882131576538\n",
      "Epoch 11 iteration 200 loss 0.31757962703704834\n",
      "Epoch 11 Training loss 1.365297600640477\n",
      "Epoch 12 iteration 0 loss 0.8337415456771851\n",
      "Epoch 12 iteration 100 loss 0.9307092428207397\n",
      "Epoch 12 iteration 200 loss 0.2663743495941162\n",
      "Epoch 12 Training loss 1.238735862429309\n",
      "Epoch 13 iteration 0 loss 0.7273722887039185\n",
      "Epoch 13 iteration 100 loss 0.8475175499916077\n",
      "Epoch 13 iteration 200 loss 0.2627345621585846\n",
      "Epoch 13 Training loss 1.148306216781321\n",
      "Epoch 14 iteration 0 loss 0.6476489901542664\n",
      "Epoch 14 iteration 100 loss 0.7323724031448364\n",
      "Epoch 14 iteration 200 loss 0.20596307516098022\n",
      "Epoch 14 Training loss 1.0432519795628925\n",
      "Epoch 15 iteration 0 loss 0.5534988641738892\n",
      "Epoch 15 iteration 100 loss 0.6708003282546997\n",
      "Epoch 15 iteration 200 loss 0.20407335460186005\n",
      "Epoch 15 Training loss 0.9427176251155633\n",
      "Evaluation loss 4.659370646636954\n",
      "Epoch 16 iteration 0 loss 0.5268934965133667\n",
      "Epoch 16 iteration 100 loss 0.5876796245574951\n",
      "Epoch 16 iteration 200 loss 0.1573285460472107\n",
      "Epoch 16 Training loss 0.8559843173821501\n",
      "Epoch 17 iteration 0 loss 0.45183923840522766\n",
      "Epoch 17 iteration 100 loss 0.4946872889995575\n",
      "Epoch 17 iteration 200 loss 0.17657925188541412\n",
      "Epoch 17 Training loss 0.7913743819896357\n",
      "Epoch 18 iteration 0 loss 0.4261704087257385\n",
      "Epoch 18 iteration 100 loss 0.4745427370071411\n",
      "Epoch 18 iteration 200 loss 0.1746172159910202\n",
      "Epoch 18 Training loss 0.7363029936724436\n",
      "Epoch 19 iteration 0 loss 0.41294118762016296\n",
      "Epoch 19 iteration 100 loss 0.422879695892334\n",
      "Epoch 19 iteration 200 loss 0.15056025981903076\n",
      "Epoch 19 Training loss 0.6885730354349503\n",
      "Epoch 20 iteration 0 loss 0.32671278715133667\n",
      "Epoch 20 iteration 100 loss 0.37882736325263977\n",
      "Epoch 20 iteration 200 loss 0.1544439047574997\n",
      "Epoch 20 Training loss 0.6405740675806608\n",
      "Evaluation loss 5.118617371364818\n",
      "Epoch 21 iteration 0 loss 0.3702872097492218\n",
      "Epoch 21 iteration 100 loss 0.34559616446495056\n",
      "Epoch 21 iteration 200 loss 0.13012316823005676\n",
      "Epoch 21 Training loss 0.611757647188475\n",
      "Epoch 22 iteration 0 loss 0.32307785749435425\n",
      "Epoch 22 iteration 100 loss 0.33837753534317017\n",
      "Epoch 22 iteration 200 loss 0.14111395180225372\n",
      "Epoch 22 Training loss 0.570414481529668\n",
      "Epoch 23 iteration 0 loss 0.28641900420188904\n",
      "Epoch 23 iteration 100 loss 0.32894909381866455\n",
      "Epoch 23 iteration 200 loss 0.1252250075340271\n",
      "Epoch 23 Training loss 0.5369238225072219\n",
      "Epoch 24 iteration 0 loss 0.2894282341003418\n",
      "Epoch 24 iteration 100 loss 0.3445281982421875\n",
      "Epoch 24 iteration 200 loss 0.19038134813308716\n",
      "Epoch 24 Training loss 0.5258248308385632\n",
      "Epoch 25 iteration 0 loss 0.3100739121437073\n",
      "Epoch 25 iteration 100 loss 0.32643669843673706\n",
      "Epoch 25 iteration 200 loss 0.11128939688205719\n",
      "Epoch 25 Training loss 0.505472461957427\n",
      "Evaluation loss 5.453535795048929\n",
      "Epoch 26 iteration 0 loss 0.2981170415878296\n",
      "Epoch 26 iteration 100 loss 0.2794056236743927\n",
      "Epoch 26 iteration 200 loss 0.12594206631183624\n",
      "Epoch 26 Training loss 0.483959563317963\n",
      "Epoch 27 iteration 0 loss 0.25360143184661865\n",
      "Epoch 27 iteration 100 loss 0.2791459560394287\n",
      "Epoch 27 iteration 200 loss 0.10946641862392426\n",
      "Epoch 27 Training loss 0.4581346452920605\n",
      "Epoch 28 iteration 0 loss 0.2647791802883148\n",
      "Epoch 28 iteration 100 loss 0.2527795135974884\n",
      "Epoch 28 iteration 200 loss 0.0987095907330513\n",
      "Epoch 28 Training loss 0.4461134315435096\n",
      "Epoch 29 iteration 0 loss 0.24486182630062103\n",
      "Epoch 29 iteration 100 loss 0.26865193247795105\n",
      "Epoch 29 iteration 200 loss 0.15012532472610474\n",
      "Epoch 29 Training loss 0.43835483501883765\n",
      "Epoch 30 iteration 0 loss 0.26599088311195374\n",
      "Epoch 30 iteration 100 loss 0.316342830657959\n",
      "Epoch 30 iteration 200 loss 0.14324849843978882\n",
      "Epoch 30 Training loss 0.44387707513547586\n",
      "Evaluation loss 5.7137503326527295\n",
      "Epoch 31 iteration 0 loss 0.2813085615634918\n",
      "Epoch 31 iteration 100 loss 0.2759716510772705\n",
      "Epoch 31 iteration 200 loss 0.10648380219936371\n",
      "Epoch 31 Training loss 0.425903582588172\n",
      "Epoch 32 iteration 0 loss 0.22731105983257294\n",
      "Epoch 32 iteration 100 loss 0.2743523120880127\n",
      "Epoch 32 iteration 200 loss 0.10788334906101227\n",
      "Epoch 32 Training loss 0.40146114002187605\n",
      "Epoch 33 iteration 0 loss 0.2517518699169159\n",
      "Epoch 33 iteration 100 loss 0.24378934502601624\n",
      "Epoch 33 iteration 200 loss 0.11172158271074295\n",
      "Epoch 33 Training loss 0.37931715807282845\n",
      "Epoch 34 iteration 0 loss 0.24221481382846832\n",
      "Epoch 34 iteration 100 loss 0.2504759430885315\n",
      "Epoch 34 iteration 200 loss 0.1105542778968811\n",
      "Epoch 34 Training loss 0.370068671055077\n",
      "Epoch 35 iteration 0 loss 0.23253744840621948\n",
      "Epoch 35 iteration 100 loss 0.2519192099571228\n",
      "Epoch 35 iteration 200 loss 0.102884940803051\n",
      "Epoch 35 Training loss 0.36213017953290483\n",
      "Evaluation loss 5.980005027926572\n",
      "Epoch 36 iteration 0 loss 0.22546957433223724\n",
      "Epoch 36 iteration 100 loss 0.2525036931037903\n",
      "Epoch 36 iteration 200 loss 0.1081053614616394\n",
      "Epoch 36 Training loss 0.36701595371255735\n",
      "Epoch 37 iteration 0 loss 0.24612067639827728\n",
      "Epoch 37 iteration 100 loss 0.25204917788505554\n",
      "Epoch 37 iteration 200 loss 0.15264733135700226\n",
      "Epoch 37 Training loss 0.37246968642266076\n",
      "Epoch 38 iteration 0 loss 0.23704718053340912\n",
      "Epoch 38 iteration 100 loss 0.23863254487514496\n",
      "Epoch 38 iteration 200 loss 0.10835081338882446\n",
      "Epoch 38 Training loss 0.35931230440219136\n",
      "Epoch 39 iteration 0 loss 0.21365070343017578\n",
      "Epoch 39 iteration 100 loss 0.22188158333301544\n",
      "Epoch 39 iteration 200 loss 0.09400905668735504\n",
      "Epoch 39 Training loss 0.35473104483499757\n",
      "Epoch 40 iteration 0 loss 0.2118419110774994\n",
      "Epoch 40 iteration 100 loss 0.23751206696033478\n",
      "Epoch 40 iteration 200 loss 0.11957012116909027\n",
      "Epoch 40 Training loss 0.35262440886186386\n",
      "Evaluation loss 6.198195886679496\n",
      "Epoch 41 iteration 0 loss 0.22300520539283752\n",
      "Epoch 41 iteration 100 loss 0.24322839081287384\n",
      "Epoch 41 iteration 200 loss 0.09167996048927307\n",
      "Epoch 41 Training loss 0.3511883217716476\n",
      "Epoch 42 iteration 0 loss 0.22480203211307526\n",
      "Epoch 42 iteration 100 loss 0.21349948644638062\n",
      "Epoch 42 iteration 200 loss 0.10533225536346436\n",
      "Epoch 42 Training loss 0.35578762927562746\n",
      "Epoch 43 iteration 0 loss 0.22830337285995483\n",
      "Epoch 43 iteration 100 loss 0.23096823692321777\n",
      "Epoch 43 iteration 200 loss 0.10846488922834396\n",
      "Epoch 43 Training loss 0.3471238258081737\n",
      "Epoch 44 iteration 0 loss 0.19638432562351227\n",
      "Epoch 44 iteration 100 loss 0.22326236963272095\n",
      "Epoch 44 iteration 200 loss 0.12030747532844543\n",
      "Epoch 44 Training loss 0.33739649675209366\n",
      "Epoch 45 iteration 0 loss 0.21570198237895966\n",
      "Epoch 45 iteration 100 loss 0.21849368512630463\n",
      "Epoch 45 iteration 200 loss 0.13113683462142944\n",
      "Epoch 45 Training loss 0.3293054227279562\n",
      "Evaluation loss 6.3902736928355255\n",
      "Epoch 46 iteration 0 loss 0.2250753939151764\n",
      "Epoch 46 iteration 100 loss 0.2177773118019104\n",
      "Epoch 46 iteration 200 loss 0.107535220682621\n",
      "Epoch 46 Training loss 0.318648881861258\n",
      "Epoch 47 iteration 0 loss 0.21533848345279694\n",
      "Epoch 47 iteration 100 loss 0.19356226921081543\n",
      "Epoch 47 iteration 200 loss 0.08963847905397415\n",
      "Epoch 47 Training loss 0.3139614380856284\n",
      "Epoch 48 iteration 0 loss 0.22564736008644104\n",
      "Epoch 48 iteration 100 loss 0.23149868845939636\n",
      "Epoch 48 iteration 200 loss 0.08341655135154724\n",
      "Epoch 48 Training loss 0.31246057039601044\n",
      "Epoch 49 iteration 0 loss 0.20234131813049316\n",
      "Epoch 49 iteration 100 loss 0.19477173686027527\n",
      "Epoch 49 iteration 200 loss 0.09646070748567581\n",
      "Epoch 49 Training loss 0.3051490193107287\n",
      "Epoch 50 iteration 0 loss 0.19313572347164154\n",
      "Epoch 50 iteration 100 loss 0.22203119099140167\n",
      "Epoch 50 iteration 200 loss 0.09753932058811188\n",
      "Epoch 50 Training loss 0.31049336626945867\n",
      "Evaluation loss 6.513383021198086\n",
      "Epoch 51 iteration 0 loss 0.20947414636611938\n",
      "Epoch 51 iteration 100 loss 0.22308047115802765\n",
      "Epoch 51 iteration 200 loss 0.13462963700294495\n",
      "Epoch 51 Training loss 0.3175527770118828\n",
      "Epoch 52 iteration 0 loss 0.2105826884508133\n",
      "Epoch 52 iteration 100 loss 0.2007095217704773\n",
      "Epoch 52 iteration 200 loss 0.06946024298667908\n",
      "Epoch 52 Training loss 0.32458426338301427\n",
      "Epoch 53 iteration 0 loss 0.25088438391685486\n",
      "Epoch 53 iteration 100 loss 0.20892451703548431\n",
      "Epoch 53 iteration 200 loss 0.1273767650127411\n",
      "Epoch 53 Training loss 0.31723307980019433\n",
      "Epoch 54 iteration 0 loss 0.2063533365726471\n",
      "Epoch 54 iteration 100 loss 0.20708398520946503\n",
      "Epoch 54 iteration 200 loss 0.1309954822063446\n",
      "Epoch 54 Training loss 0.31919052731605746\n",
      "Epoch 55 iteration 0 loss 0.20456740260124207\n",
      "Epoch 55 iteration 100 loss 0.20656445622444153\n",
      "Epoch 55 iteration 200 loss 0.15797552466392517\n",
      "Epoch 55 Training loss 0.31359392465319436\n",
      "Evaluation loss 6.664242043413385\n",
      "Epoch 56 iteration 0 loss 0.21088367700576782\n",
      "Epoch 56 iteration 100 loss 0.20617790520191193\n",
      "Epoch 56 iteration 200 loss 0.07377748936414719\n",
      "Epoch 56 Training loss 0.30812792835511776\n",
      "Epoch 57 iteration 0 loss 0.2186773419380188\n",
      "Epoch 57 iteration 100 loss 0.21405194699764252\n",
      "Epoch 57 iteration 200 loss 0.12506534159183502\n",
      "Epoch 57 Training loss 0.3045828775775641\n",
      "Epoch 58 iteration 0 loss 0.22122783958911896\n",
      "Epoch 58 iteration 100 loss 0.20339730381965637\n",
      "Epoch 58 iteration 200 loss 0.08690109848976135\n",
      "Epoch 58 Training loss 0.30264560520817363\n",
      "Epoch 59 iteration 0 loss 0.20005165040493011\n",
      "Epoch 59 iteration 100 loss 0.20387579500675201\n",
      "Epoch 59 iteration 200 loss 0.10579461604356766\n",
      "Epoch 59 Training loss 0.3061931393757672\n",
      "Epoch 60 iteration 0 loss 0.195604145526886\n",
      "Epoch 60 iteration 100 loss 0.22453850507736206\n",
      "Epoch 60 iteration 200 loss 0.12206987291574478\n",
      "Epoch 60 Training loss 0.3036202522423707\n",
      "Evaluation loss 6.801875169319535\n",
      "Epoch 61 iteration 0 loss 0.22658385336399078\n",
      "Epoch 61 iteration 100 loss 0.23353153467178345\n",
      "Epoch 61 iteration 200 loss 0.05669983848929405\n",
      "Epoch 61 Training loss 0.30377445183254514\n",
      "Epoch 62 iteration 0 loss 0.1839272826910019\n",
      "Epoch 62 iteration 100 loss 0.22707147896289825\n",
      "Epoch 62 iteration 200 loss 0.10727214813232422\n",
      "Epoch 62 Training loss 0.2994976482128712\n",
      "Epoch 63 iteration 0 loss 0.18483379483222961\n",
      "Epoch 63 iteration 100 loss 0.1986595094203949\n",
      "Epoch 63 iteration 200 loss 0.16728267073631287\n",
      "Epoch 63 Training loss 0.30200046968686545\n",
      "Epoch 64 iteration 0 loss 0.2013661414384842\n",
      "Epoch 64 iteration 100 loss 0.18305203318595886\n",
      "Epoch 64 iteration 200 loss 0.08781001716852188\n",
      "Epoch 64 Training loss 0.29929755321500084\n",
      "Epoch 65 iteration 0 loss 0.1748981475830078\n",
      "Epoch 65 iteration 100 loss 0.20086847245693207\n",
      "Epoch 65 iteration 200 loss 0.08291564881801605\n",
      "Epoch 65 Training loss 0.2953420382184476\n",
      "Evaluation loss 6.910037557623837\n",
      "Epoch 66 iteration 0 loss 0.17802733182907104\n",
      "Epoch 66 iteration 100 loss 0.24490797519683838\n",
      "Epoch 66 iteration 200 loss 0.12042367458343506\n",
      "Epoch 66 Training loss 0.30276164713739073\n",
      "Epoch 67 iteration 0 loss 0.19384253025054932\n",
      "Epoch 67 iteration 100 loss 0.24960856139659882\n",
      "Epoch 67 iteration 200 loss 0.11151280999183655\n",
      "Epoch 67 Training loss 0.3010284612126597\n",
      "Epoch 68 iteration 0 loss 0.23408298194408417\n",
      "Epoch 68 iteration 100 loss 0.18812638521194458\n",
      "Epoch 68 iteration 200 loss 0.10538269579410553\n",
      "Epoch 68 Training loss 0.3000607935088457\n",
      "Epoch 69 iteration 0 loss 0.2038976550102234\n",
      "Epoch 69 iteration 100 loss 0.2214711755514145\n",
      "Epoch 69 iteration 200 loss 0.11800205707550049\n",
      "Epoch 69 Training loss 0.29129656448682467\n",
      "Epoch 70 iteration 0 loss 0.1931675374507904\n",
      "Epoch 70 iteration 100 loss 0.1938425600528717\n",
      "Epoch 70 iteration 200 loss 0.1204952523112297\n",
      "Epoch 70 Training loss 0.28806866364795325\n",
      "Evaluation loss 7.005005309608659\n",
      "Epoch 71 iteration 0 loss 0.17607973515987396\n",
      "Epoch 71 iteration 100 loss 0.23551656305789948\n",
      "Epoch 71 iteration 200 loss 0.1274682879447937\n",
      "Epoch 71 Training loss 0.28817056183184986\n",
      "Epoch 72 iteration 0 loss 0.19520282745361328\n",
      "Epoch 72 iteration 100 loss 0.20355461537837982\n",
      "Epoch 72 iteration 200 loss 0.11923645436763763\n",
      "Epoch 72 Training loss 0.2961558504455092\n",
      "Epoch 73 iteration 0 loss 0.221811443567276\n",
      "Epoch 73 iteration 100 loss 0.22054092586040497\n",
      "Epoch 73 iteration 200 loss 0.10732685774564743\n",
      "Epoch 73 Training loss 0.29692035700886443\n",
      "Epoch 74 iteration 0 loss 0.18106766045093536\n",
      "Epoch 74 iteration 100 loss 0.204754039645195\n",
      "Epoch 74 iteration 200 loss 0.11366694420576096\n",
      "Epoch 74 Training loss 0.2968457855548779\n",
      "Epoch 75 iteration 0 loss 0.18789462745189667\n",
      "Epoch 75 iteration 100 loss 0.21859844028949738\n",
      "Epoch 75 iteration 200 loss 0.08397138863801956\n",
      "Epoch 75 Training loss 0.29275258387130587\n",
      "Evaluation loss 7.081071822561125\n",
      "Epoch 76 iteration 0 loss 0.18115869164466858\n",
      "Epoch 76 iteration 100 loss 0.2123357504606247\n",
      "Epoch 76 iteration 200 loss 0.09791982173919678\n",
      "Epoch 76 Training loss 0.29267207612509494\n",
      "Epoch 77 iteration 0 loss 0.186691015958786\n",
      "Epoch 77 iteration 100 loss 0.17493627965450287\n",
      "Epoch 77 iteration 200 loss 0.10037196427583694\n",
      "Epoch 77 Training loss 0.2977527892911503\n",
      "Epoch 78 iteration 0 loss 0.1887473464012146\n",
      "Epoch 78 iteration 100 loss 0.19238010048866272\n",
      "Epoch 78 iteration 200 loss 0.10787513852119446\n",
      "Epoch 78 Training loss 0.299112656372529\n",
      "Epoch 79 iteration 0 loss 0.20074817538261414\n",
      "Epoch 79 iteration 100 loss 0.20868881046772003\n",
      "Epoch 79 iteration 200 loss 0.12249263375997543\n",
      "Epoch 79 Training loss 0.29791419300292094\n",
      "Epoch 80 iteration 0 loss 0.20199847221374512\n",
      "Epoch 80 iteration 100 loss 0.24052679538726807\n",
      "Epoch 80 iteration 200 loss 0.141728475689888\n",
      "Epoch 80 Training loss 0.2951741865417551\n",
      "Evaluation loss 7.1330069569943495\n",
      "Epoch 81 iteration 0 loss 0.1823275089263916\n",
      "Epoch 81 iteration 100 loss 0.242544487118721\n",
      "Epoch 81 iteration 200 loss 0.1315615326166153\n",
      "Epoch 81 Training loss 0.30351642604455076\n",
      "Epoch 82 iteration 0 loss 0.19909144937992096\n",
      "Epoch 82 iteration 100 loss 0.21643896400928497\n",
      "Epoch 82 iteration 200 loss 0.11179070919752121\n",
      "Epoch 82 Training loss 0.300322375606837\n",
      "Epoch 83 iteration 0 loss 0.20959840714931488\n",
      "Epoch 83 iteration 100 loss 0.23098675906658173\n",
      "Epoch 83 iteration 200 loss 0.11467763781547546\n",
      "Epoch 83 Training loss 0.2954381679773638\n",
      "Epoch 84 iteration 0 loss 0.181300550699234\n",
      "Epoch 84 iteration 100 loss 0.1842304766178131\n",
      "Epoch 84 iteration 200 loss 0.1228424459695816\n",
      "Epoch 84 Training loss 0.29489049221001995\n",
      "Epoch 85 iteration 0 loss 0.23116444051265717\n",
      "Epoch 85 iteration 100 loss 0.20435169339179993\n",
      "Epoch 85 iteration 200 loss 0.11823781579732895\n",
      "Epoch 85 Training loss 0.2983299622874205\n",
      "Evaluation loss 7.195144355282009\n",
      "Epoch 86 iteration 0 loss 0.18956062197685242\n",
      "Epoch 86 iteration 100 loss 0.24968646466732025\n",
      "Epoch 86 iteration 200 loss 0.11138010025024414\n",
      "Epoch 86 Training loss 0.3010045841792482\n",
      "Epoch 87 iteration 0 loss 0.225340336561203\n",
      "Epoch 87 iteration 100 loss 0.191153421998024\n",
      "Epoch 87 iteration 200 loss 0.12402813881635666\n",
      "Epoch 87 Training loss 0.29694128402917663\n",
      "Epoch 88 iteration 0 loss 0.17449010908603668\n",
      "Epoch 88 iteration 100 loss 0.20713597536087036\n",
      "Epoch 88 iteration 200 loss 0.15108053386211395\n",
      "Epoch 88 Training loss 0.29975208973108286\n",
      "Epoch 89 iteration 0 loss 0.20186138153076172\n",
      "Epoch 89 iteration 100 loss 0.22629086673259735\n",
      "Epoch 89 iteration 200 loss 0.08945217728614807\n",
      "Epoch 89 Training loss 0.2928500094382514\n",
      "Epoch 90 iteration 0 loss 0.18542511761188507\n",
      "Epoch 90 iteration 100 loss 0.23398029804229736\n",
      "Epoch 90 iteration 200 loss 0.10339298099279404\n",
      "Epoch 90 Training loss 0.31269962276950264\n",
      "Evaluation loss 7.300746553431444\n",
      "Epoch 91 iteration 0 loss 0.22249242663383484\n",
      "Epoch 91 iteration 100 loss 0.1983257681131363\n",
      "Epoch 91 iteration 200 loss 0.14367157220840454\n",
      "Epoch 91 Training loss 0.31451073907167804\n",
      "Epoch 92 iteration 0 loss 0.20575736463069916\n",
      "Epoch 92 iteration 100 loss 0.21845510601997375\n",
      "Epoch 92 iteration 200 loss 0.13289932906627655\n",
      "Epoch 92 Training loss 0.3144100612819238\n",
      "Epoch 93 iteration 0 loss 0.20823928713798523\n",
      "Epoch 93 iteration 100 loss 0.25356805324554443\n",
      "Epoch 93 iteration 200 loss 0.14853958785533905\n",
      "Epoch 93 Training loss 0.30965428399661515\n",
      "Epoch 94 iteration 0 loss 0.19528456032276154\n",
      "Epoch 94 iteration 100 loss 0.2032356858253479\n",
      "Epoch 94 iteration 200 loss 0.12752598524093628\n",
      "Epoch 94 Training loss 0.308270213419551\n",
      "Epoch 95 iteration 0 loss 0.21103912591934204\n",
      "Epoch 95 iteration 100 loss 0.21166935563087463\n",
      "Epoch 95 iteration 200 loss 0.10291724652051926\n",
      "Epoch 95 Training loss 0.3017262308042381\n",
      "Evaluation loss 7.344049551283934\n",
      "Epoch 96 iteration 0 loss 0.19810457527637482\n",
      "Epoch 96 iteration 100 loss 0.21719016134738922\n",
      "Epoch 96 iteration 200 loss 0.13257728517055511\n",
      "Epoch 96 Training loss 0.3043250852124543\n",
      "Epoch 97 iteration 0 loss 0.24178637564182281\n",
      "Epoch 97 iteration 100 loss 0.21594618260860443\n",
      "Epoch 97 iteration 200 loss 0.1041446253657341\n",
      "Epoch 97 Training loss 0.31042451286930367\n",
      "Epoch 98 iteration 0 loss 0.19964085519313812\n",
      "Epoch 98 iteration 100 loss 0.22162552177906036\n",
      "Epoch 98 iteration 200 loss 0.13572074472904205\n",
      "Epoch 98 Training loss 0.29687368950746623\n",
      "Epoch 99 iteration 0 loss 0.17365123331546783\n",
      "Epoch 99 iteration 100 loss 0.22156928479671478\n",
      "Epoch 99 iteration 200 loss 0.10736539959907532\n",
      "Epoch 99 Training loss 0.29522014809137787\n",
      "Epoch 100 iteration 0 loss 0.20949514210224152\n",
      "Epoch 100 iteration 100 loss 0.19078156352043152\n",
      "Epoch 100 iteration 200 loss 0.1374841034412384\n",
      "Epoch 100 Training loss 0.2951887619565899\n",
      "Evaluation loss 7.391242216625406\n",
      "Epoch 101 iteration 0 loss 0.20736141502857208\n",
      "Epoch 101 iteration 100 loss 0.212983638048172\n",
      "Epoch 101 iteration 200 loss 0.12912851572036743\n",
      "Epoch 101 Training loss 0.29353458996158494\n",
      "Epoch 102 iteration 0 loss 0.1657099574804306\n",
      "Epoch 102 iteration 100 loss 0.20884886384010315\n",
      "Epoch 102 iteration 200 loss 0.11632487177848816\n",
      "Epoch 102 Training loss 0.3008196700475829\n",
      "Epoch 103 iteration 0 loss 0.19082723557949066\n",
      "Epoch 103 iteration 100 loss 0.2448764443397522\n",
      "Epoch 103 iteration 200 loss 0.1255895346403122\n",
      "Epoch 103 Training loss 0.30063699762718216\n",
      "Epoch 104 iteration 0 loss 0.20687322318553925\n",
      "Epoch 104 iteration 100 loss 0.21755404770374298\n",
      "Epoch 104 iteration 200 loss 0.15232054889202118\n",
      "Epoch 104 Training loss 0.3013566128612095\n",
      "Epoch 105 iteration 0 loss 0.2283334881067276\n",
      "Epoch 105 iteration 100 loss 0.20540164411067963\n",
      "Epoch 105 iteration 200 loss 0.1130097284913063\n",
      "Epoch 105 Training loss 0.3031379086337054\n",
      "Evaluation loss 7.398111755835988\n",
      "Epoch 106 iteration 0 loss 0.246419757604599\n",
      "Epoch 106 iteration 100 loss 0.24476507306098938\n",
      "Epoch 106 iteration 200 loss 0.10304506868124008\n",
      "Epoch 106 Training loss 0.3114671863762621\n",
      "Epoch 107 iteration 0 loss 0.20295797288417816\n",
      "Epoch 107 iteration 100 loss 0.23423689603805542\n",
      "Epoch 107 iteration 200 loss 0.1294526755809784\n",
      "Epoch 107 Training loss 0.30979150907358455\n",
      "Epoch 108 iteration 0 loss 0.22719566524028778\n",
      "Epoch 108 iteration 100 loss 0.2071239948272705\n",
      "Epoch 108 iteration 200 loss 0.12972170114517212\n",
      "Epoch 108 Training loss 0.3044636640370001\n",
      "Epoch 109 iteration 0 loss 0.19076642394065857\n",
      "Epoch 109 iteration 100 loss 0.20939505100250244\n",
      "Epoch 109 iteration 200 loss 0.0952245369553566\n",
      "Epoch 109 Training loss 0.30226745730726834\n",
      "Epoch 110 iteration 0 loss 0.237718403339386\n",
      "Epoch 110 iteration 100 loss 0.19752272963523865\n",
      "Epoch 110 iteration 200 loss 0.11258886754512787\n",
      "Epoch 110 Training loss 0.2984537532899264\n",
      "Evaluation loss 7.480597032767428\n",
      "Epoch 111 iteration 0 loss 0.21189694106578827\n",
      "Epoch 111 iteration 100 loss 0.22113126516342163\n",
      "Epoch 111 iteration 200 loss 0.08657634258270264\n",
      "Epoch 111 Training loss 0.30259343715267595\n",
      "Epoch 112 iteration 0 loss 0.18896682560443878\n",
      "Epoch 112 iteration 100 loss 0.20329871773719788\n",
      "Epoch 112 iteration 200 loss 0.1265549659729004\n",
      "Epoch 112 Training loss 0.30573443966755287\n",
      "Epoch 113 iteration 0 loss 0.21277029812335968\n",
      "Epoch 113 iteration 100 loss 0.2241334766149521\n",
      "Epoch 113 iteration 200 loss 0.13046500086784363\n",
      "Epoch 113 Training loss 0.31027935064749324\n",
      "Epoch 114 iteration 0 loss 0.17377987504005432\n",
      "Epoch 114 iteration 100 loss 0.22930483520030975\n",
      "Epoch 114 iteration 200 loss 0.14254337549209595\n",
      "Epoch 114 Training loss 0.3074222319886858\n",
      "Epoch 115 iteration 0 loss 0.2391265332698822\n",
      "Epoch 115 iteration 100 loss 0.2581923007965088\n",
      "Epoch 115 iteration 200 loss 0.12009666860103607\n",
      "Epoch 115 Training loss 0.30539446959962474\n",
      "Evaluation loss 7.509831786234116\n",
      "Epoch 116 iteration 0 loss 0.2188991904258728\n",
      "Epoch 116 iteration 100 loss 0.2398691475391388\n",
      "Epoch 116 iteration 200 loss 0.10616645961999893\n",
      "Epoch 116 Training loss 0.3029801271773759\n",
      "Epoch 117 iteration 0 loss 0.19541995227336884\n",
      "Epoch 117 iteration 100 loss 0.21796487271785736\n",
      "Epoch 117 iteration 200 loss 0.14483757317066193\n",
      "Epoch 117 Training loss 0.30449818311099963\n",
      "Epoch 118 iteration 0 loss 0.20497040450572968\n",
      "Epoch 118 iteration 100 loss 0.22529734671115875\n",
      "Epoch 118 iteration 200 loss 0.10364633798599243\n",
      "Epoch 118 Training loss 0.31187682894731056\n",
      "Epoch 119 iteration 0 loss 0.21045894920825958\n",
      "Epoch 119 iteration 100 loss 0.21745146811008453\n",
      "Epoch 119 iteration 200 loss 0.11625451594591141\n",
      "Epoch 119 Training loss 0.3131846808056403\n",
      "Epoch 120 iteration 0 loss 0.2386026233434677\n",
      "Epoch 120 iteration 100 loss 0.22185423970222473\n",
      "Epoch 120 iteration 200 loss 0.104190893471241\n",
      "Epoch 120 Training loss 0.30499526599504784\n",
      "Evaluation loss 7.5248627639254755\n",
      "Epoch 121 iteration 0 loss 0.2151043713092804\n",
      "Epoch 121 iteration 100 loss 0.20110386610031128\n",
      "Epoch 121 iteration 200 loss 0.10609333962202072\n",
      "Epoch 121 Training loss 0.3085396525377541\n",
      "Epoch 122 iteration 0 loss 0.2068275809288025\n",
      "Epoch 122 iteration 100 loss 0.20504318177700043\n",
      "Epoch 122 iteration 200 loss 0.10646763443946838\n",
      "Epoch 122 Training loss 0.30656410971459985\n",
      "Epoch 123 iteration 0 loss 0.21875318884849548\n",
      "Epoch 123 iteration 100 loss 0.227055624127388\n",
      "Epoch 123 iteration 200 loss 0.11429956555366516\n",
      "Epoch 123 Training loss 0.30688895048893344\n",
      "Epoch 124 iteration 0 loss 0.19617006182670593\n",
      "Epoch 124 iteration 100 loss 0.1953834593296051\n",
      "Epoch 124 iteration 200 loss 0.10447272658348083\n",
      "Epoch 124 Training loss 0.3023018497239041\n",
      "Epoch 125 iteration 0 loss 0.21238088607788086\n",
      "Epoch 125 iteration 100 loss 0.21323710680007935\n",
      "Epoch 125 iteration 200 loss 0.11123160272836685\n",
      "Epoch 125 Training loss 0.3013146319281106\n",
      "Evaluation loss 7.6064478305949255\n",
      "Epoch 126 iteration 0 loss 0.200094074010849\n",
      "Epoch 126 iteration 100 loss 0.2055332511663437\n",
      "Epoch 126 iteration 200 loss 0.14152154326438904\n",
      "Epoch 126 Training loss 0.3010726476523798\n",
      "Epoch 127 iteration 0 loss 0.20883189141750336\n",
      "Epoch 127 iteration 100 loss 0.24286100268363953\n",
      "Epoch 127 iteration 200 loss 0.08716198056936264\n",
      "Epoch 127 Training loss 0.31495187240991945\n",
      "Epoch 128 iteration 0 loss 0.19622819125652313\n",
      "Epoch 128 iteration 100 loss 0.23657411336898804\n",
      "Epoch 128 iteration 200 loss 0.08259556442499161\n",
      "Epoch 128 Training loss 0.3162125717550146\n",
      "Epoch 129 iteration 0 loss 0.25214582681655884\n",
      "Epoch 129 iteration 100 loss 0.22793491184711456\n",
      "Epoch 129 iteration 200 loss 0.12268687784671783\n",
      "Epoch 129 Training loss 0.3165958401478285\n",
      "Epoch 130 iteration 0 loss 0.2187615931034088\n",
      "Epoch 130 iteration 100 loss 0.22671517729759216\n",
      "Epoch 130 iteration 200 loss 0.12407270818948746\n",
      "Epoch 130 Training loss 0.31894411701107084\n",
      "Evaluation loss 7.602874696806653\n",
      "Epoch 131 iteration 0 loss 0.2096213698387146\n",
      "Epoch 131 iteration 100 loss 0.2532489001750946\n",
      "Epoch 131 iteration 200 loss 0.14836783707141876\n",
      "Epoch 131 Training loss 0.31747247606977197\n",
      "Epoch 132 iteration 0 loss 0.2314358502626419\n",
      "Epoch 132 iteration 100 loss 0.21569378674030304\n",
      "Epoch 132 iteration 200 loss 0.08119668811559677\n",
      "Epoch 132 Training loss 0.3202253546136815\n",
      "Epoch 133 iteration 0 loss 0.22844132781028748\n",
      "Epoch 133 iteration 100 loss 0.2135062962770462\n",
      "Epoch 133 iteration 200 loss 0.1013692319393158\n",
      "Epoch 133 Training loss 0.3193234692498724\n",
      "Epoch 134 iteration 0 loss 0.19536320865154266\n",
      "Epoch 134 iteration 100 loss 0.2689196467399597\n",
      "Epoch 134 iteration 200 loss 0.13496991991996765\n",
      "Epoch 134 Training loss 0.31106405526986736\n",
      "Epoch 135 iteration 0 loss 0.2166299819946289\n",
      "Epoch 135 iteration 100 loss 0.25170475244522095\n",
      "Epoch 135 iteration 200 loss 0.11387336254119873\n",
      "Epoch 135 Training loss 0.3181046809027814\n",
      "Evaluation loss 7.650159399235459\n",
      "Epoch 136 iteration 0 loss 0.20964816212654114\n",
      "Epoch 136 iteration 100 loss 0.21914079785346985\n",
      "Epoch 136 iteration 200 loss 0.14904382824897766\n",
      "Epoch 136 Training loss 0.32310889587078234\n",
      "Epoch 137 iteration 0 loss 0.212445929646492\n",
      "Epoch 137 iteration 100 loss 0.23719704151153564\n",
      "Epoch 137 iteration 200 loss 0.1294410526752472\n",
      "Epoch 137 Training loss 0.32066405151926514\n",
      "Epoch 138 iteration 0 loss 0.2297467440366745\n",
      "Epoch 138 iteration 100 loss 0.24568425118923187\n",
      "Epoch 138 iteration 200 loss 0.15152223408222198\n",
      "Epoch 138 Training loss 0.32173719883265856\n",
      "Epoch 139 iteration 0 loss 0.2205308973789215\n",
      "Epoch 139 iteration 100 loss 0.25599679350852966\n",
      "Epoch 139 iteration 200 loss 0.13252924382686615\n",
      "Epoch 139 Training loss 0.3236982314215259\n",
      "Epoch 140 iteration 0 loss 0.2323358654975891\n",
      "Epoch 140 iteration 100 loss 0.26671674847602844\n",
      "Epoch 140 iteration 200 loss 0.12101219594478607\n",
      "Epoch 140 Training loss 0.31851809545137544\n",
      "Evaluation loss 7.67973923396046\n",
      "Epoch 141 iteration 0 loss 0.1762290745973587\n",
      "Epoch 141 iteration 100 loss 0.2253381460905075\n",
      "Epoch 141 iteration 200 loss 0.1371871381998062\n",
      "Epoch 141 Training loss 0.3217574186205853\n",
      "Epoch 142 iteration 0 loss 0.2032983899116516\n",
      "Epoch 142 iteration 100 loss 0.24342657625675201\n",
      "Epoch 142 iteration 200 loss 0.11050399392843246\n",
      "Epoch 142 Training loss 0.3151263621628752\n",
      "Epoch 143 iteration 0 loss 0.19098369777202606\n",
      "Epoch 143 iteration 100 loss 0.2025148570537567\n",
      "Epoch 143 iteration 200 loss 0.1408621221780777\n",
      "Epoch 143 Training loss 0.3188005609375614\n",
      "Epoch 144 iteration 0 loss 0.20730489492416382\n",
      "Epoch 144 iteration 100 loss 0.23349629342556\n",
      "Epoch 144 iteration 200 loss 0.13440068066120148\n",
      "Epoch 144 Training loss 0.314332139436337\n",
      "Epoch 145 iteration 0 loss 0.19867481291294098\n",
      "Epoch 145 iteration 100 loss 0.2602965831756592\n",
      "Epoch 145 iteration 200 loss 0.14017662405967712\n",
      "Epoch 145 Training loss 0.3104298907531099\n",
      "Evaluation loss 7.733792721438021\n",
      "Epoch 146 iteration 0 loss 0.2549060583114624\n",
      "Epoch 146 iteration 100 loss 0.2551969587802887\n",
      "Epoch 146 iteration 200 loss 0.14604103565216064\n",
      "Epoch 146 Training loss 0.3119667656675945\n",
      "Epoch 147 iteration 0 loss 0.22162941098213196\n",
      "Epoch 147 iteration 100 loss 0.21786047518253326\n",
      "Epoch 147 iteration 200 loss 0.14249856770038605\n",
      "Epoch 147 Training loss 0.31534480276383015\n",
      "Epoch 148 iteration 0 loss 0.20454083383083344\n",
      "Epoch 148 iteration 100 loss 0.21581345796585083\n",
      "Epoch 148 iteration 200 loss 0.13065582513809204\n",
      "Epoch 148 Training loss 0.31063398000538717\n",
      "Epoch 149 iteration 0 loss 0.24596144258975983\n",
      "Epoch 149 iteration 100 loss 0.2167765200138092\n",
      "Epoch 149 iteration 200 loss 0.14530713856220245\n",
      "Epoch 149 Training loss 0.3153608913799859\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=150)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.283748876407639, 4.312491681022675, 3.7586550394879232, 3.3292234561675085, 2.958110513023686, 2.629351006837794, 2.333364686448987, 2.069129560853707, 1.8508082363935938, 1.6731592754120406, 1.5153205886651269, 1.365297600640477, 1.238735862429309, 1.148306216781321, 1.0432519795628925, 0.9427176251155633, 0.8559843173821501, 0.7913743819896357, 0.7363029936724436, 0.6885730354349503, 0.6405740675806608, 0.611757647188475, 0.570414481529668, 0.5369238225072219, 0.5258248308385632, 0.505472461957427, 0.483959563317963, 0.4581346452920605, 0.4461134315435096, 0.43835483501883765, 0.44387707513547586, 0.425903582588172, 0.40146114002187605, 0.37931715807282845, 0.370068671055077, 0.36213017953290483, 0.36701595371255735, 0.37246968642266076, 0.35931230440219136, 0.35473104483499757, 0.35262440886186386, 0.3511883217716476, 0.35578762927562746, 0.3471238258081737, 0.33739649675209366, 0.3293054227279562, 0.318648881861258, 0.3139614380856284, 0.31246057039601044, 0.3051490193107287, 0.31049336626945867, 0.3175527770118828, 0.32458426338301427, 0.31723307980019433, 0.31919052731605746, 0.31359392465319436, 0.30812792835511776, 0.3045828775775641, 0.30264560520817363, 0.3061931393757672, 0.3036202522423707, 0.30377445183254514, 0.2994976482128712, 0.30200046968686545, 0.29929755321500084, 0.2953420382184476, 0.30276164713739073, 0.3010284612126597, 0.3000607935088457, 0.29129656448682467, 0.28806866364795325, 0.28817056183184986, 0.2961558504455092, 0.29692035700886443, 0.2968457855548779, 0.29275258387130587, 0.29267207612509494, 0.2977527892911503, 0.299112656372529, 0.29791419300292094, 0.2951741865417551, 0.30351642604455076, 0.300322375606837, 0.2954381679773638, 0.29489049221001995, 0.2983299622874205, 0.3010045841792482, 0.29694128402917663, 0.29975208973108286, 0.2928500094382514, 0.31269962276950264, 0.31451073907167804, 0.3144100612819238, 0.30965428399661515, 0.308270213419551, 0.3017262308042381, 0.3043250852124543, 0.31042451286930367, 0.29687368950746623, 0.29522014809137787, 0.2951887619565899, 0.29353458996158494, 0.3008196700475829, 0.30063699762718216, 0.3013566128612095, 0.3031379086337054, 0.3114671863762621, 0.30979150907358455, 0.3044636640370001, 0.30226745730726834, 0.2984537532899264, 0.30259343715267595, 0.30573443966755287, 0.31027935064749324, 0.3074222319886858, 0.30539446959962474, 0.3029801271773759, 0.30449818311099963, 0.31187682894731056, 0.3131846808056403, 0.30499526599504784, 0.3085396525377541, 0.30656410971459985, 0.30688895048893344, 0.3023018497239041, 0.3013146319281106, 0.3010726476523798, 0.31495187240991945, 0.3162125717550146, 0.3165958401478285, 0.31894411701107084, 0.31747247606977197, 0.3202253546136815, 0.3193234692498724, 0.31106405526986736, 0.3181046809027814, 0.32310889587078234, 0.32066405151926514, 0.32173719883265856, 0.3236982314215259, 0.31851809545137544, 0.3217574186205853, 0.3151263621628752, 0.3188005609375614, 0.314332139436337, 0.3104298907531099, 0.3119667656675945, 0.31534480276383015, 0.31063398000538717, 0.3153608913799859]\n",
      "[4.40329669223758, 3.794731282134689, 4.21728604800039, 4.659370646636954, 5.118617371364818, 5.453535795048929, 5.7137503326527295, 5.980005027926572, 6.198195886679496, 6.3902736928355255, 6.513383021198086, 6.664242043413385, 6.801875169319535, 6.910037557623837, 7.005005309608659, 7.081071822561125, 7.1330069569943495, 7.195144355282009, 7.300746553431444, 7.344049551283934, 7.391242216625406, 7.398111755835988, 7.480597032767428, 7.509831786234116, 7.5248627639254755, 7.6064478305949255, 7.602874696806653, 7.650159399235459, 7.67973923396046, 7.733792721438021]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(eval_losses, label='Evaluation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(train_losses)\n",
    "print(eval_losses)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS well done ! EOS\n",
      "BOS 做 得 好 ！ EOS\n",
      "老嘢！\n",
      "\n",
      "BOS be serious . EOS\n",
      "BOS 咪 玩 啦 。 EOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/xc_yp0q16ps2v4zqf84_bbxc0000gn/T/ipykernel_77389/1649215179.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  attn.data.masked_fill(mask, -1e6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "等騷好。\n",
      "\n",
      "BOS be serious . EOS\n",
      "BOS 認 真 啲 啦 。 EOS\n",
      "因住。\n",
      "\n",
      "BOS it matters . EOS\n",
      "BOS 重 要 。 EOS\n",
      "爛到散吓。\n",
      "\n",
      "BOS no problem . EOS\n",
      "BOS 冇 問 題 。 EOS\n",
      "冇錢。\n",
      "\n",
      "BOS say please . EOS\n",
      "BOS 講 唔 該 吖 。 EOS\n",
      "有人話，唔該。\n",
      "\n",
      "BOS start over . EOS\n",
      "BOS 重 新 嚟 過 啦 。 EOS\n",
      "鉛本樹。\n",
      "\n",
      "BOS who talked ? EOS\n",
      "BOS 邊 個 講 嘢 ？ EOS\n",
      "邊個黐線？\n",
      "\n",
      "BOS who yelled ? EOS\n",
      "BOS 邊 個 嗌 呀 ？ EOS\n",
      "邊個青口\n",
      "\n",
      "BOS please stay . EOS\n",
      "BOS 請 留 步 。 EOS\n",
      "請注晒你落。\n",
      "\n",
      "BOS please stay . EOS\n",
      "BOS 請 你 留 低 啦 。 EOS\n",
      "請等住你個躉。\n",
      "\n",
      "BOS talk slower . EOS\n",
      "BOS 講 慢 少 少 。 EOS\n",
      "聽日講到口效效事。\n",
      "\n",
      "BOS abandon ship ! EOS\n",
      "BOS 棄 船 呀 ！ EOS\n",
      "寶步股\n",
      "\n",
      "BOS drive slowly . EOS\n",
      "BOS 揸 慢 啲 。 EOS\n",
      "電腦植慢。\n",
      "\n",
      "BOS speak slower . EOS\n",
      "BOS 講 慢 少 少 。 EOS\n",
      "阿媽有聲。\n",
      "\n",
      "BOS tom screamed . EOS\n",
      "BOS T o m 尖 叫 。 EOS\n",
      "城紙。\n",
      "\n",
      "BOS say something . EOS\n",
      "BOS 講 啲 嘢 啦 。 EOS\n",
      "自己傾得幾多。\n",
      "\n",
      "BOS drive carefully . EOS\n",
      "BOS 小 心 啲 揸 車 。 EOS\n",
      "一篇煙。\n",
      "\n",
      "BOS UNK christmas ! EOS\n",
      "BOS 聖 誕 節 快 樂 ！ EOS\n",
      "呀！你自己發地啦！\n",
      "\n",
      "BOS i get it . EOS\n",
      "BOS 我 明 。 EOS\n",
      "我凍到。\n",
      "\n",
      "BOS i get it . EOS\n",
      "BOS 我 g e t 到 。 EOS\n",
      "我嗗一聲㿭晒。\n",
      "\n",
      "BOS i 'm here . EOS\n",
      "BOS 我 喺 度 。 EOS\n",
      "我喺度揗雞。\n",
      "\n",
      "BOS do n't cry . EOS\n",
      "BOS 唔 好 喊 啦 。 EOS\n",
      "唔好𢫕咗佢。\n",
      "\n",
      "BOS i 'm angry . EOS\n",
      "BOS 我 好 嬲 。 EOS\n",
      "我嬲到篤。\n",
      "\n",
      "BOS i 'm ready . EOS\n",
      "BOS 我 準 備 好 喇 。 EOS\n",
      "我寫門。\n",
      "\n",
      "BOS i 'm right . EOS\n",
      "BOS 我 係 啱 嘅 。 EOS\n",
      "我係見喎。\n",
      "\n",
      "BOS it 's easy . EOS\n",
      "BOS 好 容 易 噃 。 EOS\n",
      "幾易出嚟。\n",
      "\n",
      "BOS come on in ! EOS\n",
      "BOS 入 嚟 啦 ！ EOS\n",
      "嚟㗎！\n",
      "\n",
      "BOS do n't move . EOS\n",
      "BOS 咪 郁 。 EOS\n",
      "唔好咁舂。\n",
      "\n",
      "BOS how 's work ? EOS\n",
      "BOS 返 工 返 成 點 呀 ？ EOS\n",
      "噉嘅嘢你都幾識㗎？\n",
      "\n",
      "BOS i know him . EOS\n",
      "BOS 我 識 佢 。 EOS\n",
      "我遲到佢。\n",
      "\n",
      "BOS is it safe ? EOS\n",
      "BOS 安 唔 安 全 㗎 ？ EOS\n",
      "係唔係咧？\n",
      "\n",
      "BOS it 's windy . EOS\n",
      "BOS 好 大 風 。 EOS\n",
      "由到行山泥塵。\n",
      "\n",
      "BOS let me die . EOS\n",
      "BOS 俾 我 死 咗 去 算 啦 。 EOS\n",
      "等我放得我啦。\n",
      "\n",
      "BOS take a nap . EOS\n",
      "BOS UNK 一 陣 啦 。 EOS\n",
      "將枝竹竿一岩。\n",
      "\n",
      "BOS time is up . EOS\n",
      "BOS 夠 鐘 喇 。 EOS\n",
      "夠時候喇。\n",
      "\n",
      "BOS we are men . EOS\n",
      "BOS 我 哋 係 男 人 。 EOS\n",
      "我哋好冇着。\n",
      "\n",
      "BOS what is it ? EOS\n",
      "BOS 呢 個 咩 嚟 㗎 ？ EOS\n",
      "究有乜睇啊？\n",
      "\n",
      "BOS who did it ? EOS\n",
      "BOS 邊 個 做 架 ？ EOS\n",
      "邊個信啊？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])  #原来的英文\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])  #原来的中文\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))           #翻译后的中文\n",
    "\n",
    "\n",
    "for i in range(10, 49):\n",
    "    translate_dev(i)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/xc_yp0q16ps2v4zqf84_bbxc0000gn/T/ipykernel_77389/1649215179.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  attn.data.masked_fill(mask, -1e6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS well done ! EOS\n",
      "BOS 做 得 好 ！ EOS\n",
      "喇！\n",
      "\n",
      "BOS be serious . EOS\n",
      "BOS 咪 玩 啦 。 EOS\n",
      "佢好容易㗎。\n",
      "\n",
      "BOS be serious . EOS\n",
      "BOS 認 真 啲 啦 。 EOS\n",
      "嗰隻生會好有電。\n",
      "\n",
      "BOS it matters . EOS\n",
      "BOS 重 要 。 EOS\n",
      "佢做緊嘢。\n",
      "\n",
      "BOS no problem . EOS\n",
      "BOS 冇 問 題 。 EOS\n",
      "你間超級佢。\n",
      "\n",
      "BOS say please . EOS\n",
      "BOS 講 唔 該 吖 。 EOS\n",
      "你話佢哋搭生意。\n",
      "\n",
      "BOS start over . EOS\n",
      "BOS 重 新 嚟 過 啦 。 EOS\n",
      "就去咗佢。\n",
      "\n",
      "BOS who talked ? EOS\n",
      "BOS 邊 個 講 嘢 ？ EOS\n",
      "咪喐？\n",
      "\n",
      "BOS who yelled ? EOS\n",
      "BOS 邊 個 嗌 呀 ？ EOS\n",
      "邊個噉行啊？\n",
      "\n",
      "BOS please stay . EOS\n",
      "BOS 請 留 步 。 EOS\n",
      "唔該你。\n",
      "\n",
      "BOS please stay . EOS\n",
      "BOS 請 你 留 低 啦 。 EOS\n",
      "唔該你。\n",
      "\n",
      "BOS talk slower . EOS\n",
      "BOS 講 慢 少 少 。 EOS\n",
      "佢個女打波。\n",
      "\n",
      "BOS abandon ship ! EOS\n",
      "BOS 棄 船 呀 ！ EOS\n",
      "隻船食水嘅！\n",
      "\n",
      "BOS drive slowly . EOS\n",
      "BOS 揸 慢 啲 。 EOS\n",
      "呢粒事我喺度整緊。\n",
      "\n",
      "BOS speak slower . EOS\n",
      "BOS 講 慢 少 少 。 EOS\n",
      "呢個煲嘅大戲。\n",
      "\n",
      "BOS tom screamed . EOS\n",
      "BOS T o m 尖 叫 。 EOS\n",
      "藥煲衡得。\n",
      "\n",
      "BOS say something . EOS\n",
      "BOS 講 啲 嘢 啦 。 EOS\n",
      "大家都係老細嘅。\n",
      "\n",
      "BOS drive carefully . EOS\n",
      "BOS 小 心 啲 揸 車 。 EOS\n",
      "機嘅功課室係心。\n",
      "\n",
      "BOS UNK christmas ! EOS\n",
      "BOS 聖 誕 節 快 樂 ！ EOS\n",
      "崖門！\n",
      "\n",
      "BOS i get it . EOS\n",
      "BOS 我 明 。 EOS\n",
      "我諗到都係條線。\n",
      "\n",
      "BOS i get it . EOS\n",
      "BOS 我 g e t 到 。 EOS\n",
      "我好鍾意。\n",
      "\n",
      "BOS i 'm here . EOS\n",
      "BOS 我 喺 度 。 EOS\n",
      "我而家喺呢度。\n",
      "\n",
      "BOS do n't cry . EOS\n",
      "BOS 唔 好 喊 啦 。 EOS\n",
      "唔好行，開刀。\n",
      "\n",
      "BOS i 'm angry . EOS\n",
      "BOS 我 好 嬲 。 EOS\n",
      "我高佬踩卜。\n",
      "\n",
      "BOS i 'm ready . EOS\n",
      "BOS 我 準 備 好 喇 。 EOS\n",
      "我好鍾意食晏。\n",
      "\n",
      "BOS i 'm right . EOS\n",
      "BOS 我 係 啱 嘅 。 EOS\n",
      "我講嘢乸手。\n",
      "\n",
      "BOS it 's easy . EOS\n",
      "BOS 好 容 易 噃 。 EOS\n",
      "升咗五咪五鑊啦。\n",
      "\n",
      "BOS come on in ! EOS\n",
      "BOS 入 嚟 啦 ！ EOS\n",
      "咁門都唔熄啦！\n",
      "\n",
      "BOS do n't move . EOS\n",
      "BOS 咪 郁 。 EOS\n",
      "唔好爆我啲嘢。\n",
      "\n",
      "BOS how 's work ? EOS\n",
      "BOS 返 工 返 成 點 呀 ？ EOS\n",
      "邊個工㗎？\n",
      "\n",
      "BOS i know him . EOS\n",
      "BOS 我 識 佢 。 EOS\n",
      "我夠知佢。\n",
      "\n",
      "BOS is it safe ? EOS\n",
      "BOS 安 唔 安 全 㗎 ？ EOS\n",
      "膠蛇幾長？\n",
      "\n",
      "BOS it 's windy . EOS\n",
      "BOS 好 大 風 。 EOS\n",
      "我同吓條濕晒。\n",
      "\n",
      "BOS let me die . EOS\n",
      "BOS 俾 我 死 咗 去 算 啦 。 EOS\n",
      "我同佢着數我。\n",
      "\n",
      "BOS take a nap . EOS\n",
      "BOS UNK 一 陣 啦 。 EOS\n",
      "將幅相睇化電掣度。\n",
      "\n",
      "BOS time is up . EOS\n",
      "BOS 夠 鐘 喇 。 EOS\n",
      "夠時間時都係喼住。\n",
      "\n",
      "BOS we are men . EOS\n",
      "BOS 我 哋 係 男 人 。 EOS\n",
      "我哋好鍾意時。\n",
      "\n",
      "BOS what is it ? EOS\n",
      "BOS 呢 個 咩 嚟 㗎 ？ EOS\n",
      "乜嘢咁有啊？\n",
      "\n",
      "BOS who did it ? EOS\n",
      "BOS 邊 個 做 架 ？ EOS\n",
      "邊個邊個係邊個㗎？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Open the file for appending\n",
    "with open('output.txt', 'a') as f:\n",
    "    # Loop through the numbers and translate them\n",
    "    for i in range(10, 49):\n",
    "        en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])  #原来的英文\n",
    "        f.write(en_sent + '\\n')\n",
    "\n",
    "        cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])  #原来的中文\n",
    "        f.write(cn_sent + '\\n')\n",
    "\n",
    "        mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "        mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "        bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "\n",
    "        translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "        translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "        f.write('\\n')\n",
    "        trans = []\n",
    "        for word in translation:\n",
    "            if word != \"EOS\":\n",
    "                trans.append(word)\n",
    "            else:\n",
    "                break\n",
    "        translated = \"\".join(trans)           #翻译后的中文\n",
    "        f.write(translated + '\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "        # Print the output to the console\n",
    "        print(en_sent)\n",
    "        print(cn_sent)\n",
    "        print(translated)\n",
    "        print()\n",
    "\n",
    "# Close the file\n",
    "f.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
