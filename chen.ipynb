{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./venv/lib/python3.10/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk) (1.2.0)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from nltk) (4.64.1)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.10/site-packages (from nltk) (2022.10.31)\r\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk) (8.1.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    },
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "\n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn\n",
    "\n",
    "train_file = \"data/yue-eng2.txt\"\n",
    "dev_file = \"data/yue-eng.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BOS', 'begin', '.', 'EOS'], ['BOS', 'hello', '!', 'EOS']]\n",
      "[['BOS', '開', '始', '啦', '。', 'EOS'], ['BOS', '哈', '佬', '。', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(dev_en[:2])\n",
    "print(dev_cn[:2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "outputs": [],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1\n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 2\n",
    "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
    "    word_dict[\"UNK\"] = UNK_IDX\n",
    "    word_dict[\"PAD\"] = PAD_IDX\n",
    "    return word_dict, total_words      #total_words所有单词数，最大50002\n",
    "\n",
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}    #英文：索引到单词\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}    #中文：索引到字"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "\n",
    "    # sort sentences by word lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "\n",
    "    return out_en_sentences, out_cn_sentences\n",
    "\n",
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 81, 1749, 3]\n",
      "['BOS', '手', '鏈', 'EOS']\n",
      "['BOS', 'bracelet', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "print(train_cn[2])\n",
    "print([inv_cn_dict[i] for i in train_cn[2]])\n",
    "print([inv_en_dict[i] for i in train_en[2]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=True):  #n是传进来的句子数\n",
    "    idx_list = np.arange(0, n, minibatch_size)   #[0, 1, ..., n-1]按minibatch_size大小分割\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n array([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),\n array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]),\n array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74])]"
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_minibatches(100, 15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "outputs": [],
   "source": [
    "def prepare_data(seqs):   #seqs传入的是minibatches中的一个minibatch对应的batch_size个句子索引（嵌套列表），此处batch_size=64\n",
    "\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)  #batch_size个句子中最长句子长度\n",
    "\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype(\"int32\")\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths\n",
    "\n",
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_ex     #返回内容依次是batch_size个英文句子索引，英文句子长度，中文句子索引，中文句子长度\n",
    "\n",
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)\n",
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):   #把mask的部分忽略掉\n",
    "        # input: (batch_size * seq_len) * vocab_size\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        # target: batch_size * 1\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)          #双向，所以拼接\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
    "\n",
    "        return out, hid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):  # a mask of shape x_len * y_len\n",
    "\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
    "        mask = ( ~(x_mask[:, :, None]) * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
    "\n",
    "        output, attn = self.attention(output_seq, ctx, mask)  #根据原来的output_seq和context来计算\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "\n",
    "        return output, hid, attn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out,\n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, attn\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out,\n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        # input: (batch_size * seq_len) * vocab_size\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        # target: batch_size * 1\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#0.2\n",
    "dropout = 0.2\n",
    "#100\n",
    "embed_size = hidden_size = 500\n",
    "encoder = Encoder(vocab_size=en_total_words, embed_size=embed_size, enc_hidden_size=hidden_size, dec_hidden_size=hidden_size, dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words, embed_size=embed_size, enc_hidden_size=hidden_size, dec_hidden_size=hidden_size, dropout=dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)\n",
    "    eval_losses.append(total_loss/total_num_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# def train(model, train_data, dev_data, num_epochs=20):\n",
    "#     train_losses = []\n",
    "#     dev_losses = []\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         train_num_words = train_loss = 0.\n",
    "#         for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(train_data):\n",
    "#             mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "#             mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "#             mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "#             mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "#             mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "#             mb_y_len[mb_y_len<=0] = 1\n",
    "#\n",
    "#             mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "#\n",
    "#             mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "#             mb_out_mask = mb_out_mask.float()\n",
    "#\n",
    "#             loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "#\n",
    "#             num_words = torch.sum(mb_y_len).item()\n",
    "#             train_loss += loss.item() * num_words\n",
    "#             train_num_words += num_words\n",
    "#\n",
    "#             # 更新模型\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "#             optimizer.step()\n",
    "#\n",
    "#             if it % 100 == 0:\n",
    "#                 print(\"Epoch\", epoch, \"iteration\", it, \"train loss\", loss.item())\n",
    "#\n",
    "#         train_loss /= train_num_words\n",
    "#         train_losses.append(train_loss)\n",
    "#         print(\"Epoch\", epoch, \"Training loss\", train_loss)\n",
    "#\n",
    "#         dev_num_words = dev_loss = 0.\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(dev_data):\n",
    "#                 mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "#                 mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "#                 mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "#                 mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "#                 mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "#                 mb_y_len[mb_y_len<=0] = 1\n",
    "#\n",
    "#                 mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "#\n",
    "#                 mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "#                 mb_out_mask = mb_out_mask.float()\n",
    "#\n",
    "#                 loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "#\n",
    "#                 num_words = torch.sum(mb_y_len).item()\n",
    "#                 dev_loss += loss.item() * num_words\n",
    "#                 dev_num_words += num_words\n",
    "#\n",
    "#             dev_loss /= dev_num_words\n",
    "#             dev_losses.append(dev_loss)\n",
    "#             print(\"Epoch\", epoch, \"Validation loss\", dev_loss)\n",
    "#\n",
    "#         if epoch % 5 == 0:\n",
    "#             evaluate(model, dev_data)\n",
    "#\n",
    "#     # Generate the graph\n",
    "#     plt.plot(range(num_epochs), train_losses, label='Train')\n",
    "#     plt.plot(range(num_epochs), dev_losses, label='Validation')\n",
    "#     plt.title('Training and Validation Loss Over Time')\n",
    "#     plt.xlabel('Epoch')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "def train(model, data, num_epochs=20):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            mb_y_len[mb_y_len<=0] = 1\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "\n",
    "            # 更新模型\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "\n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "\n",
    "        train_loss = total_loss/total_num_words\n",
    "        train_losses.append(train_loss)\n",
    "        print(\"Epoch\", epoch, \"Training loss\", train_loss)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/xc_yp0q16ps2v4zqf84_bbxc0000gn/T/ipykernel_77389/1649215179.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  attn.data.masked_fill(mask, -1e6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.120576858520508\n",
      "Epoch 0 iteration 100 loss 5.178638458251953\n",
      "Epoch 0 iteration 200 loss 5.06793737411499\n",
      "Epoch 0 Training loss 5.372446208750755\n",
      "Evaluation loss 4.498395010436141\n",
      "Epoch 1 iteration 0 loss 4.341202259063721\n",
      "Epoch 1 iteration 100 loss 4.174962043762207\n",
      "Epoch 1 iteration 200 loss 4.370672225952148\n",
      "Epoch 1 Training loss 4.408008573983182\n",
      "Epoch 2 iteration 0 loss 3.6221001148223877\n",
      "Epoch 2 iteration 100 loss 3.6178886890411377\n",
      "Epoch 2 iteration 200 loss 3.963189125061035\n",
      "Epoch 2 Training loss 3.8654470329006205\n",
      "Epoch 3 iteration 0 loss 2.967040538787842\n",
      "Epoch 3 iteration 100 loss 3.2431440353393555\n",
      "Epoch 3 iteration 200 loss 3.6536712646484375\n",
      "Epoch 3 Training loss 3.43820219721083\n",
      "Epoch 4 iteration 0 loss 2.4407033920288086\n",
      "Epoch 4 iteration 100 loss 2.8949341773986816\n",
      "Epoch 4 iteration 200 loss 3.3958632946014404\n",
      "Epoch 4 Training loss 3.0788994657786515\n",
      "Epoch 5 iteration 0 loss 2.032367706298828\n",
      "Epoch 5 iteration 100 loss 2.5974314212799072\n",
      "Epoch 5 iteration 200 loss 3.196399688720703\n",
      "Epoch 5 Training loss 2.776198476256388\n",
      "Evaluation loss 3.7659469382492556\n",
      "Epoch 6 iteration 0 loss 1.7401745319366455\n",
      "Epoch 6 iteration 100 loss 2.2923779487609863\n",
      "Epoch 6 iteration 200 loss 3.0027878284454346\n",
      "Epoch 6 Training loss 2.5085109058576793\n",
      "Epoch 7 iteration 0 loss 1.3671019077301025\n",
      "Epoch 7 iteration 100 loss 2.0908546447753906\n",
      "Epoch 7 iteration 200 loss 2.7517383098602295\n",
      "Epoch 7 Training loss 2.2772815310250762\n",
      "Epoch 8 iteration 0 loss 1.19478440284729\n",
      "Epoch 8 iteration 100 loss 1.858353853225708\n",
      "Epoch 8 iteration 200 loss 2.5804481506347656\n",
      "Epoch 8 Training loss 2.0682286378614863\n",
      "Epoch 9 iteration 0 loss 0.9651767015457153\n",
      "Epoch 9 iteration 100 loss 1.6760978698730469\n",
      "Epoch 9 iteration 200 loss 2.4241390228271484\n",
      "Epoch 9 Training loss 1.891585909966633\n",
      "Epoch 10 iteration 0 loss 0.8146266937255859\n",
      "Epoch 10 iteration 100 loss 1.4685745239257812\n",
      "Epoch 10 iteration 200 loss 2.2599472999572754\n",
      "Epoch 10 Training loss 1.7339235257809904\n",
      "Evaluation loss 4.250152470641717\n",
      "Epoch 11 iteration 0 loss 0.6773723363876343\n",
      "Epoch 11 iteration 100 loss 1.3451297283172607\n",
      "Epoch 11 iteration 200 loss 2.09479022026062\n",
      "Epoch 11 Training loss 1.5984956555866776\n",
      "Epoch 12 iteration 0 loss 0.6034125089645386\n",
      "Epoch 12 iteration 100 loss 1.180268406867981\n",
      "Epoch 12 iteration 200 loss 1.9872746467590332\n",
      "Epoch 12 Training loss 1.4688003106117813\n",
      "Epoch 13 iteration 0 loss 0.46771422028541565\n",
      "Epoch 13 iteration 100 loss 1.06022047996521\n",
      "Epoch 13 iteration 200 loss 1.8460460901260376\n",
      "Epoch 13 Training loss 1.36047718115042\n",
      "Epoch 14 iteration 0 loss 0.4626614451408386\n",
      "Epoch 14 iteration 100 loss 0.9399415254592896\n",
      "Epoch 14 iteration 200 loss 1.7412484884262085\n",
      "Epoch 14 Training loss 1.2573771235789821\n",
      "Epoch 15 iteration 0 loss 0.43585094809532166\n",
      "Epoch 15 iteration 100 loss 0.9584460258483887\n",
      "Epoch 15 iteration 200 loss 1.645298957824707\n",
      "Epoch 15 Training loss 1.2163683124416207\n",
      "Evaluation loss 4.47628599137772\n",
      "Epoch 16 iteration 0 loss 0.3429977297782898\n",
      "Epoch 16 iteration 100 loss 0.8039400577545166\n",
      "Epoch 16 iteration 200 loss 1.5512266159057617\n",
      "Epoch 16 Training loss 1.1240049691797704\n",
      "Epoch 17 iteration 0 loss 0.36598721146583557\n",
      "Epoch 17 iteration 100 loss 0.7251236438751221\n",
      "Epoch 17 iteration 200 loss 1.48198401927948\n",
      "Epoch 17 Training loss 1.0631060737666578\n",
      "Epoch 18 iteration 0 loss 0.33208364248275757\n",
      "Epoch 18 iteration 100 loss 0.697342574596405\n",
      "Epoch 18 iteration 200 loss 1.4062474966049194\n",
      "Epoch 18 Training loss 0.9999874184139865\n",
      "Epoch 19 iteration 0 loss 0.3232923150062561\n",
      "Epoch 19 iteration 100 loss 0.6131271123886108\n",
      "Epoch 19 iteration 200 loss 1.267534613609314\n",
      "Epoch 19 Training loss 0.9536026681136053\n",
      "Epoch 20 iteration 0 loss 0.3174194097518921\n",
      "Epoch 20 iteration 100 loss 0.5630261301994324\n",
      "Epoch 20 iteration 200 loss 1.2322094440460205\n",
      "Epoch 20 Training loss 0.909282773529088\n",
      "Evaluation loss 4.832897515961173\n",
      "Epoch 21 iteration 0 loss 0.33074769377708435\n",
      "Epoch 21 iteration 100 loss 0.5371757745742798\n",
      "Epoch 21 iteration 200 loss 1.2338943481445312\n",
      "Epoch 21 Training loss 0.8782606733385969\n",
      "Epoch 22 iteration 0 loss 0.29130131006240845\n",
      "Epoch 22 iteration 100 loss 0.5406708717346191\n",
      "Epoch 22 iteration 200 loss 1.125665545463562\n",
      "Epoch 22 Training loss 0.8184408648421555\n",
      "Epoch 23 iteration 0 loss 0.28151410818099976\n",
      "Epoch 23 iteration 100 loss 0.48915809392929077\n",
      "Epoch 23 iteration 200 loss 1.0862748622894287\n",
      "Epoch 23 Training loss 0.7917987101085857\n",
      "Epoch 24 iteration 0 loss 0.24812078475952148\n",
      "Epoch 24 iteration 100 loss 0.49398791790008545\n",
      "Epoch 24 iteration 200 loss 1.045032024383545\n",
      "Epoch 24 Training loss 0.7773635974509401\n",
      "Epoch 25 iteration 0 loss 0.256971538066864\n",
      "Epoch 25 iteration 100 loss 0.5115821957588196\n",
      "Epoch 25 iteration 200 loss 1.067573070526123\n",
      "Epoch 25 Training loss 0.761407089598744\n",
      "Evaluation loss 5.104850826461035\n",
      "Epoch 26 iteration 0 loss 0.204384446144104\n",
      "Epoch 26 iteration 100 loss 0.4347887337207794\n",
      "Epoch 26 iteration 200 loss 1.0223912000656128\n",
      "Epoch 26 Training loss 0.7311803426004864\n",
      "Epoch 27 iteration 0 loss 0.25843554735183716\n",
      "Epoch 27 iteration 100 loss 0.4862052798271179\n",
      "Epoch 27 iteration 200 loss 1.0443803071975708\n",
      "Epoch 27 Training loss 0.7341639765724352\n",
      "Epoch 28 iteration 0 loss 0.2559610903263092\n",
      "Epoch 28 iteration 100 loss 0.4512800872325897\n",
      "Epoch 28 iteration 200 loss 0.9537382125854492\n",
      "Epoch 28 Training loss 0.7039198818708409\n",
      "Epoch 29 iteration 0 loss 0.23824505507946014\n",
      "Epoch 29 iteration 100 loss 0.42601603269577026\n",
      "Epoch 29 iteration 200 loss 0.941487729549408\n",
      "Epoch 29 Training loss 0.6766366386802936\n",
      "Epoch 30 iteration 0 loss 0.241165429353714\n",
      "Epoch 30 iteration 100 loss 0.4053146541118622\n",
      "Epoch 30 iteration 200 loss 0.9129576683044434\n",
      "Epoch 30 Training loss 0.6667393478615804\n",
      "Evaluation loss 5.412916458208588\n",
      "Epoch 31 iteration 0 loss 0.22670269012451172\n",
      "Epoch 31 iteration 100 loss 0.3856615722179413\n",
      "Epoch 31 iteration 200 loss 0.8737408518791199\n",
      "Epoch 31 Training loss 0.6479832044574593\n",
      "Epoch 32 iteration 0 loss 0.20612578094005585\n",
      "Epoch 32 iteration 100 loss 0.36442017555236816\n",
      "Epoch 32 iteration 200 loss 0.822533130645752\n",
      "Epoch 32 Training loss 0.6176852072594607\n",
      "Epoch 33 iteration 0 loss 0.16919198632240295\n",
      "Epoch 33 iteration 100 loss 0.39834487438201904\n",
      "Epoch 33 iteration 200 loss 0.8281561136245728\n",
      "Epoch 33 Training loss 0.6002752520591371\n",
      "Epoch 34 iteration 0 loss 0.18871483206748962\n",
      "Epoch 34 iteration 100 loss 0.41676095128059387\n",
      "Epoch 34 iteration 200 loss 0.7616203427314758\n",
      "Epoch 34 Training loss 0.5838595282483724\n",
      "Epoch 35 iteration 0 loss 0.17570137977600098\n",
      "Epoch 35 iteration 100 loss 0.38252928853034973\n",
      "Epoch 35 iteration 200 loss 0.8285199999809265\n",
      "Epoch 35 Training loss 0.5762542013838632\n",
      "Evaluation loss 5.597429816372896\n",
      "Epoch 36 iteration 0 loss 0.20037099719047546\n",
      "Epoch 36 iteration 100 loss 0.34265777468681335\n",
      "Epoch 36 iteration 200 loss 0.7522794008255005\n",
      "Epoch 36 Training loss 0.5694861956902496\n",
      "Epoch 37 iteration 0 loss 0.21276208758354187\n",
      "Epoch 37 iteration 100 loss 0.40678173303604126\n",
      "Epoch 37 iteration 200 loss 0.7483341097831726\n",
      "Epoch 37 Training loss 0.5774377162568864\n",
      "Epoch 38 iteration 0 loss 0.19037726521492004\n",
      "Epoch 38 iteration 100 loss 0.376976877450943\n",
      "Epoch 38 iteration 200 loss 0.7179550528526306\n",
      "Epoch 38 Training loss 0.578052692853075\n",
      "Epoch 39 iteration 0 loss 0.21834871172904968\n",
      "Epoch 39 iteration 100 loss 0.41879627108573914\n",
      "Epoch 39 iteration 200 loss 0.7899665832519531\n",
      "Epoch 39 Training loss 0.5768809969403879\n",
      "Epoch 40 iteration 0 loss 0.24536435306072235\n",
      "Epoch 40 iteration 100 loss 0.4196600914001465\n",
      "Epoch 40 iteration 200 loss 0.7415223717689514\n",
      "Epoch 40 Training loss 0.5753070315577964\n",
      "Evaluation loss 5.804054199319218\n",
      "Epoch 41 iteration 0 loss 0.24948327243328094\n",
      "Epoch 41 iteration 100 loss 0.38165053725242615\n",
      "Epoch 41 iteration 200 loss 0.7465791702270508\n",
      "Epoch 41 Training loss 0.5769955792641382\n",
      "Epoch 42 iteration 0 loss 0.2208985537290573\n",
      "Epoch 42 iteration 100 loss 0.3440868556499481\n",
      "Epoch 42 iteration 200 loss 0.7411649227142334\n",
      "Epoch 42 Training loss 0.5541251030672657\n",
      "Epoch 43 iteration 0 loss 0.18232299387454987\n",
      "Epoch 43 iteration 100 loss 0.32586851716041565\n",
      "Epoch 43 iteration 200 loss 0.7022208571434021\n",
      "Epoch 43 Training loss 0.5313514652899781\n",
      "Epoch 44 iteration 0 loss 0.23202982544898987\n",
      "Epoch 44 iteration 100 loss 0.3421455919742584\n",
      "Epoch 44 iteration 200 loss 0.67044597864151\n",
      "Epoch 44 Training loss 0.5092816777717196\n",
      "Epoch 45 iteration 0 loss 0.19448043406009674\n",
      "Epoch 45 iteration 100 loss 0.30056753754615784\n",
      "Epoch 45 iteration 200 loss 0.7386143803596497\n",
      "Epoch 45 Training loss 0.5236484086412805\n",
      "Evaluation loss 5.908302990952189\n",
      "Epoch 46 iteration 0 loss 0.20257775485515594\n",
      "Epoch 46 iteration 100 loss 0.3335413336753845\n",
      "Epoch 46 iteration 200 loss 0.6986057758331299\n",
      "Epoch 46 Training loss 0.511769317065693\n",
      "Epoch 47 iteration 0 loss 0.1917230486869812\n",
      "Epoch 47 iteration 100 loss 0.3746510148048401\n",
      "Epoch 47 iteration 200 loss 0.7345409989356995\n",
      "Epoch 47 Training loss 0.5257803828217789\n",
      "Epoch 48 iteration 0 loss 0.20478162169456482\n",
      "Epoch 48 iteration 100 loss 0.34223976731300354\n",
      "Epoch 48 iteration 200 loss 0.6715813279151917\n",
      "Epoch 48 Training loss 0.5212664028171504\n",
      "Epoch 49 iteration 0 loss 0.17149263620376587\n",
      "Epoch 49 iteration 100 loss 0.32135531306266785\n",
      "Epoch 49 iteration 200 loss 0.6684899926185608\n",
      "Epoch 49 Training loss 0.5128699593978738\n",
      "Epoch 50 iteration 0 loss 0.204131081700325\n",
      "Epoch 50 iteration 100 loss 0.3408289849758148\n",
      "Epoch 50 iteration 200 loss 0.6726751923561096\n",
      "Epoch 50 Training loss 0.5236081756381594\n",
      "Evaluation loss 6.041571297928105\n",
      "Epoch 51 iteration 0 loss 0.22169776260852814\n",
      "Epoch 51 iteration 100 loss 0.30464011430740356\n",
      "Epoch 51 iteration 200 loss 0.686697244644165\n",
      "Epoch 51 Training loss 0.5303117199114042\n",
      "Epoch 52 iteration 0 loss 0.18870872259140015\n",
      "Epoch 52 iteration 100 loss 0.3460536003112793\n",
      "Epoch 52 iteration 200 loss 0.7101691961288452\n",
      "Epoch 52 Training loss 0.5245172809257136\n",
      "Epoch 53 iteration 0 loss 0.23844297230243683\n",
      "Epoch 53 iteration 100 loss 0.3128132224082947\n",
      "Epoch 53 iteration 200 loss 0.655760645866394\n",
      "Epoch 53 Training loss 0.5043666957391059\n",
      "Epoch 54 iteration 0 loss 0.18125171959400177\n",
      "Epoch 54 iteration 100 loss 0.32392945885658264\n",
      "Epoch 54 iteration 200 loss 0.6353394985198975\n",
      "Epoch 54 Training loss 0.4856149168778086\n",
      "Epoch 55 iteration 0 loss 0.17302227020263672\n",
      "Epoch 55 iteration 100 loss 0.34040096402168274\n",
      "Epoch 55 iteration 200 loss 0.6413488388061523\n",
      "Epoch 55 Training loss 0.48678430915026283\n",
      "Evaluation loss 6.192376567882752\n",
      "Epoch 56 iteration 0 loss 0.16130110621452332\n",
      "Epoch 56 iteration 100 loss 0.29960471391677856\n",
      "Epoch 56 iteration 200 loss 0.585533082485199\n",
      "Epoch 56 Training loss 0.4873531028695023\n",
      "Epoch 57 iteration 0 loss 0.15819138288497925\n",
      "Epoch 57 iteration 100 loss 0.3489934504032135\n",
      "Epoch 57 iteration 200 loss 0.6551075577735901\n",
      "Epoch 57 Training loss 0.4873753928415224\n",
      "Epoch 58 iteration 0 loss 0.20127978920936584\n",
      "Epoch 58 iteration 100 loss 0.3085828125476837\n",
      "Epoch 58 iteration 200 loss 0.5854212641716003\n",
      "Epoch 58 Training loss 0.4875898059372765\n",
      "Epoch 59 iteration 0 loss 0.19256910681724548\n",
      "Epoch 59 iteration 100 loss 0.30801326036453247\n",
      "Epoch 59 iteration 200 loss 0.6100647449493408\n",
      "Epoch 59 Training loss 0.47262753194826623\n",
      "Epoch 60 iteration 0 loss 0.16997450590133667\n",
      "Epoch 60 iteration 100 loss 0.3212824761867523\n",
      "Epoch 60 iteration 200 loss 0.6176508665084839\n",
      "Epoch 60 Training loss 0.4820327222450792\n",
      "Evaluation loss 6.290169808749142\n",
      "Epoch 61 iteration 0 loss 0.18720772862434387\n",
      "Epoch 61 iteration 100 loss 0.33584779500961304\n",
      "Epoch 61 iteration 200 loss 0.6491711139678955\n",
      "Epoch 61 Training loss 0.47357167312749043\n",
      "Epoch 62 iteration 0 loss 0.16428235173225403\n",
      "Epoch 62 iteration 100 loss 0.26199859380722046\n",
      "Epoch 62 iteration 200 loss 0.590267539024353\n",
      "Epoch 62 Training loss 0.4685618579253662\n",
      "Epoch 63 iteration 0 loss 0.19918134808540344\n",
      "Epoch 63 iteration 100 loss 0.33434778451919556\n",
      "Epoch 63 iteration 200 loss 0.6424639821052551\n",
      "Epoch 63 Training loss 0.46552481402424883\n",
      "Epoch 64 iteration 0 loss 0.17254140973091125\n",
      "Epoch 64 iteration 100 loss 0.3201364576816559\n",
      "Epoch 64 iteration 200 loss 0.5792954564094543\n",
      "Epoch 64 Training loss 0.4640259386680448\n",
      "Epoch 65 iteration 0 loss 0.13504426181316376\n",
      "Epoch 65 iteration 100 loss 0.3361058235168457\n",
      "Epoch 65 iteration 200 loss 0.6268072724342346\n",
      "Epoch 65 Training loss 0.4648137205228577\n",
      "Evaluation loss 6.4138568988220594\n",
      "Epoch 66 iteration 0 loss 0.16364717483520508\n",
      "Epoch 66 iteration 100 loss 0.2742522358894348\n",
      "Epoch 66 iteration 200 loss 0.6084960699081421\n",
      "Epoch 66 Training loss 0.4518354139315343\n",
      "Epoch 67 iteration 0 loss 0.13849934935569763\n",
      "Epoch 67 iteration 100 loss 0.29936110973358154\n",
      "Epoch 67 iteration 200 loss 0.5397677421569824\n",
      "Epoch 67 Training loss 0.4427749825979345\n",
      "Epoch 68 iteration 0 loss 0.12429919093847275\n",
      "Epoch 68 iteration 100 loss 0.29149043560028076\n",
      "Epoch 68 iteration 200 loss 0.5712661147117615\n",
      "Epoch 68 Training loss 0.43774099696277446\n",
      "Epoch 69 iteration 0 loss 0.18319612741470337\n",
      "Epoch 69 iteration 100 loss 0.2683151662349701\n",
      "Epoch 69 iteration 200 loss 0.6019859313964844\n",
      "Epoch 69 Training loss 0.44803602961714284\n",
      "Epoch 70 iteration 0 loss 0.17813684046268463\n",
      "Epoch 70 iteration 100 loss 0.2878335416316986\n",
      "Epoch 70 iteration 200 loss 0.555736243724823\n",
      "Epoch 70 Training loss 0.4588301671702019\n",
      "Evaluation loss 6.509017227053468\n",
      "Epoch 71 iteration 0 loss 0.19718880951404572\n",
      "Epoch 71 iteration 100 loss 0.2712242603302002\n",
      "Epoch 71 iteration 200 loss 0.6091521382331848\n",
      "Epoch 71 Training loss 0.46199222755405656\n",
      "Epoch 72 iteration 0 loss 0.20885546505451202\n",
      "Epoch 72 iteration 100 loss 0.2831852436065674\n",
      "Epoch 72 iteration 200 loss 0.5987542867660522\n",
      "Epoch 72 Training loss 0.46038178488583015\n",
      "Epoch 73 iteration 0 loss 0.15470744669437408\n",
      "Epoch 73 iteration 100 loss 0.32125958800315857\n",
      "Epoch 73 iteration 200 loss 0.5705488324165344\n",
      "Epoch 73 Training loss 0.4514900282415223\n",
      "Epoch 74 iteration 0 loss 0.16008058190345764\n",
      "Epoch 74 iteration 100 loss 0.2893470525741577\n",
      "Epoch 74 iteration 200 loss 0.5763019919395447\n",
      "Epoch 74 Training loss 0.45928458219262586\n",
      "Epoch 75 iteration 0 loss 0.17985093593597412\n",
      "Epoch 75 iteration 100 loss 0.28424468636512756\n",
      "Epoch 75 iteration 200 loss 0.5834595561027527\n",
      "Epoch 75 Training loss 0.47144665314158885\n",
      "Evaluation loss 6.580371578375603\n",
      "Epoch 76 iteration 0 loss 0.17011016607284546\n",
      "Epoch 76 iteration 100 loss 0.3121486306190491\n",
      "Epoch 76 iteration 200 loss 0.5617692470550537\n",
      "Epoch 76 Training loss 0.4514248661342246\n",
      "Epoch 77 iteration 0 loss 0.1732812225818634\n",
      "Epoch 77 iteration 100 loss 0.24798929691314697\n",
      "Epoch 77 iteration 200 loss 0.5156496167182922\n",
      "Epoch 77 Training loss 0.4426095651408901\n",
      "Epoch 78 iteration 0 loss 0.18720729649066925\n",
      "Epoch 78 iteration 100 loss 0.2718448340892792\n",
      "Epoch 78 iteration 200 loss 0.5757737159729004\n",
      "Epoch 78 Training loss 0.45120109703904293\n",
      "Epoch 79 iteration 0 loss 0.17648229002952576\n",
      "Epoch 79 iteration 100 loss 0.29037579894065857\n",
      "Epoch 79 iteration 200 loss 0.6054712533950806\n",
      "Epoch 79 Training loss 0.45358615116509293\n",
      "Epoch 80 iteration 0 loss 0.20645304024219513\n",
      "Epoch 80 iteration 100 loss 0.40420717000961304\n",
      "Epoch 80 iteration 200 loss 0.616585373878479\n",
      "Epoch 80 Training loss 0.4666362715376922\n",
      "Evaluation loss 6.640484333339927\n",
      "Epoch 81 iteration 0 loss 0.21823211014270782\n",
      "Epoch 81 iteration 100 loss 0.31170475482940674\n",
      "Epoch 81 iteration 200 loss 0.6143092513084412\n",
      "Epoch 81 Training loss 0.45775156870298583\n",
      "Epoch 82 iteration 0 loss 0.21636667847633362\n",
      "Epoch 82 iteration 100 loss 0.30684009194374084\n",
      "Epoch 82 iteration 200 loss 0.6042844653129578\n",
      "Epoch 82 Training loss 0.4621446206650676\n",
      "Epoch 83 iteration 0 loss 0.22789327800273895\n",
      "Epoch 83 iteration 100 loss 0.3489542305469513\n",
      "Epoch 83 iteration 200 loss 0.5721631646156311\n",
      "Epoch 83 Training loss 0.46284654286843285\n",
      "Epoch 84 iteration 0 loss 0.17572659254074097\n",
      "Epoch 84 iteration 100 loss 0.32547125220298767\n",
      "Epoch 84 iteration 200 loss 0.5944515466690063\n",
      "Epoch 84 Training loss 0.4599045634107207\n",
      "Epoch 85 iteration 0 loss 0.17974761128425598\n",
      "Epoch 85 iteration 100 loss 0.31819581985473633\n",
      "Epoch 85 iteration 200 loss 0.5857357978820801\n",
      "Epoch 85 Training loss 0.45654708370661384\n",
      "Evaluation loss 6.700479935219623\n",
      "Epoch 86 iteration 0 loss 0.1775071918964386\n",
      "Epoch 86 iteration 100 loss 0.3215484917163849\n",
      "Epoch 86 iteration 200 loss 0.6050702333450317\n",
      "Epoch 86 Training loss 0.45539693508589374\n",
      "Epoch 87 iteration 0 loss 0.1694527566432953\n",
      "Epoch 87 iteration 100 loss 0.29350775480270386\n",
      "Epoch 87 iteration 200 loss 0.6335088610649109\n",
      "Epoch 87 Training loss 0.44625651526046417\n",
      "Epoch 88 iteration 0 loss 0.26458531618118286\n",
      "Epoch 88 iteration 100 loss 0.2952027916908264\n",
      "Epoch 88 iteration 200 loss 0.5933467745780945\n",
      "Epoch 88 Training loss 0.45072313366165284\n",
      "Epoch 89 iteration 0 loss 0.17488650977611542\n",
      "Epoch 89 iteration 100 loss 0.2795935869216919\n",
      "Epoch 89 iteration 200 loss 0.6307027339935303\n",
      "Epoch 89 Training loss 0.44537213882107235\n",
      "Epoch 90 iteration 0 loss 0.18995444476604462\n",
      "Epoch 90 iteration 100 loss 0.31102055311203003\n",
      "Epoch 90 iteration 200 loss 0.5809378027915955\n",
      "Epoch 90 Training loss 0.4491438514565481\n",
      "Evaluation loss 6.775078162872881\n",
      "Epoch 91 iteration 0 loss 0.20170852541923523\n",
      "Epoch 91 iteration 100 loss 0.3016679883003235\n",
      "Epoch 91 iteration 200 loss 0.5591342449188232\n",
      "Epoch 91 Training loss 0.45254829629927407\n",
      "Epoch 92 iteration 0 loss 0.1838742196559906\n",
      "Epoch 92 iteration 100 loss 0.2973116636276245\n",
      "Epoch 92 iteration 200 loss 0.5961616635322571\n",
      "Epoch 92 Training loss 0.4492581452402527\n",
      "Epoch 93 iteration 0 loss 0.15634752810001373\n",
      "Epoch 93 iteration 100 loss 0.3261934518814087\n",
      "Epoch 93 iteration 200 loss 0.6760936975479126\n",
      "Epoch 93 Training loss 0.4515379487945101\n",
      "Epoch 94 iteration 0 loss 0.19279161095619202\n",
      "Epoch 94 iteration 100 loss 0.35494059324264526\n",
      "Epoch 94 iteration 200 loss 0.5553209781646729\n",
      "Epoch 94 Training loss 0.4542068502886721\n",
      "Epoch 95 iteration 0 loss 0.17495225369930267\n",
      "Epoch 95 iteration 100 loss 0.30978748202323914\n",
      "Epoch 95 iteration 200 loss 0.6054996252059937\n",
      "Epoch 95 Training loss 0.4655807903504215\n",
      "Evaluation loss 6.770515353924674\n",
      "Epoch 96 iteration 0 loss 0.23739305138587952\n",
      "Epoch 96 iteration 100 loss 0.29988381266593933\n",
      "Epoch 96 iteration 200 loss 0.6072078943252563\n",
      "Epoch 96 Training loss 0.45960821040322475\n",
      "Epoch 97 iteration 0 loss 0.1995212733745575\n",
      "Epoch 97 iteration 100 loss 0.3059087097644806\n",
      "Epoch 97 iteration 200 loss 0.557776927947998\n",
      "Epoch 97 Training loss 0.45378693207629567\n",
      "Epoch 98 iteration 0 loss 0.17876553535461426\n",
      "Epoch 98 iteration 100 loss 0.30546680092811584\n",
      "Epoch 98 iteration 200 loss 0.6528218388557434\n",
      "Epoch 98 Training loss 0.4535934094937461\n",
      "Epoch 99 iteration 0 loss 0.24219293892383575\n",
      "Epoch 99 iteration 100 loss 0.29618459939956665\n",
      "Epoch 99 iteration 200 loss 0.5860558152198792\n",
      "Epoch 99 Training loss 0.464610240860749\n",
      "Epoch 100 iteration 0 loss 0.21290378272533417\n",
      "Epoch 100 iteration 100 loss 0.3172701299190521\n",
      "Epoch 100 iteration 200 loss 0.6235533952713013\n",
      "Epoch 100 Training loss 0.4643012507588988\n",
      "Evaluation loss 6.843364641998531\n",
      "Epoch 101 iteration 0 loss 0.21147263050079346\n",
      "Epoch 101 iteration 100 loss 0.3095668852329254\n",
      "Epoch 101 iteration 200 loss 0.5647726655006409\n",
      "Epoch 101 Training loss 0.4595095694026433\n",
      "Epoch 102 iteration 0 loss 0.2044251263141632\n",
      "Epoch 102 iteration 100 loss 0.35702329874038696\n",
      "Epoch 102 iteration 200 loss 0.6350144147872925\n",
      "Epoch 102 Training loss 0.4568945200202954\n",
      "Epoch 103 iteration 0 loss 0.14076785743236542\n",
      "Epoch 103 iteration 100 loss 0.30459052324295044\n",
      "Epoch 103 iteration 200 loss 0.6495710611343384\n",
      "Epoch 103 Training loss 0.4506726632479495\n",
      "Epoch 104 iteration 0 loss 0.18151509761810303\n",
      "Epoch 104 iteration 100 loss 0.28799745440483093\n",
      "Epoch 104 iteration 200 loss 0.5893384218215942\n",
      "Epoch 104 Training loss 0.44282650586022454\n",
      "Epoch 105 iteration 0 loss 0.16612230241298676\n",
      "Epoch 105 iteration 100 loss 0.28708863258361816\n",
      "Epoch 105 iteration 200 loss 0.5768248438835144\n",
      "Epoch 105 Training loss 0.4364478199216737\n",
      "Evaluation loss 6.881245744172266\n",
      "Epoch 106 iteration 0 loss 0.18952205777168274\n",
      "Epoch 106 iteration 100 loss 0.26747819781303406\n",
      "Epoch 106 iteration 200 loss 0.5854533314704895\n",
      "Epoch 106 Training loss 0.43891074894002385\n",
      "Epoch 107 iteration 0 loss 0.1828741878271103\n",
      "Epoch 107 iteration 100 loss 0.30134132504463196\n",
      "Epoch 107 iteration 200 loss 0.5581721663475037\n",
      "Epoch 107 Training loss 0.43530191535284585\n",
      "Epoch 108 iteration 0 loss 0.2055095136165619\n",
      "Epoch 108 iteration 100 loss 0.26472583413124084\n",
      "Epoch 108 iteration 200 loss 0.6446528434753418\n",
      "Epoch 108 Training loss 0.44172102373456795\n",
      "Epoch 109 iteration 0 loss 0.16712337732315063\n",
      "Epoch 109 iteration 100 loss 0.31515994668006897\n",
      "Epoch 109 iteration 200 loss 0.6484233736991882\n",
      "Epoch 109 Training loss 0.45068693825696693\n",
      "Epoch 110 iteration 0 loss 0.1826387196779251\n",
      "Epoch 110 iteration 100 loss 0.3104436993598938\n",
      "Epoch 110 iteration 200 loss 0.5964580774307251\n",
      "Epoch 110 Training loss 0.4513756805870779\n",
      "Evaluation loss 6.910811623900621\n",
      "Epoch 111 iteration 0 loss 0.1522921323776245\n",
      "Epoch 111 iteration 100 loss 0.28327786922454834\n",
      "Epoch 111 iteration 200 loss 0.5546606183052063\n",
      "Epoch 111 Training loss 0.45935122031239406\n",
      "Epoch 112 iteration 0 loss 0.20898810029029846\n",
      "Epoch 112 iteration 100 loss 0.31925076246261597\n",
      "Epoch 112 iteration 200 loss 0.5988397598266602\n",
      "Epoch 112 Training loss 0.4510269462873241\n",
      "Epoch 113 iteration 0 loss 0.17605182528495789\n",
      "Epoch 113 iteration 100 loss 0.29356199502944946\n",
      "Epoch 113 iteration 200 loss 0.5537263751029968\n",
      "Epoch 113 Training loss 0.44906514666399383\n",
      "Epoch 114 iteration 0 loss 0.18944543600082397\n",
      "Epoch 114 iteration 100 loss 0.2752240002155304\n",
      "Epoch 114 iteration 200 loss 0.5728340148925781\n",
      "Epoch 114 Training loss 0.45973671613549133\n",
      "Epoch 115 iteration 0 loss 0.20296132564544678\n",
      "Epoch 115 iteration 100 loss 0.3084031641483307\n",
      "Epoch 115 iteration 200 loss 0.5892167687416077\n",
      "Epoch 115 Training loss 0.4520562034524336\n",
      "Evaluation loss 6.968227967326593\n",
      "Epoch 116 iteration 0 loss 0.19398383796215057\n",
      "Epoch 116 iteration 100 loss 0.29903146624565125\n",
      "Epoch 116 iteration 200 loss 0.5475133061408997\n",
      "Epoch 116 Training loss 0.4453189684268822\n",
      "Epoch 117 iteration 0 loss 0.20185713469982147\n",
      "Epoch 117 iteration 100 loss 0.2938093841075897\n",
      "Epoch 117 iteration 200 loss 0.5682810544967651\n",
      "Epoch 117 Training loss 0.44161127260203525\n",
      "Epoch 118 iteration 0 loss 0.19452162086963654\n",
      "Epoch 118 iteration 100 loss 0.29041528701782227\n",
      "Epoch 118 iteration 200 loss 0.5489856004714966\n",
      "Epoch 118 Training loss 0.4422466698944122\n",
      "Epoch 119 iteration 0 loss 0.22663207352161407\n",
      "Epoch 119 iteration 100 loss 0.2532036304473877\n",
      "Epoch 119 iteration 200 loss 0.5401341915130615\n",
      "Epoch 119 Training loss 0.4340471253040501\n",
      "Epoch 120 iteration 0 loss 0.21433484554290771\n",
      "Epoch 120 iteration 100 loss 0.3293931484222412\n",
      "Epoch 120 iteration 200 loss 0.537188708782196\n",
      "Epoch 120 Training loss 0.4426076005922886\n",
      "Evaluation loss 7.019253737565894\n",
      "Epoch 121 iteration 0 loss 0.1596413105726242\n",
      "Epoch 121 iteration 100 loss 0.3035848140716553\n",
      "Epoch 121 iteration 200 loss 0.5905467867851257\n",
      "Epoch 121 Training loss 0.4389283808096937\n",
      "Epoch 122 iteration 0 loss 0.1730354279279709\n",
      "Epoch 122 iteration 100 loss 0.26312020421028137\n",
      "Epoch 122 iteration 200 loss 0.5602167248725891\n",
      "Epoch 122 Training loss 0.43786916125015585\n",
      "Epoch 123 iteration 0 loss 0.1589205116033554\n",
      "Epoch 123 iteration 100 loss 0.3550763428211212\n",
      "Epoch 123 iteration 200 loss 0.5944146513938904\n",
      "Epoch 123 Training loss 0.45455394297958124\n",
      "Epoch 124 iteration 0 loss 0.18503370881080627\n",
      "Epoch 124 iteration 100 loss 0.2888524830341339\n",
      "Epoch 124 iteration 200 loss 0.6344983577728271\n",
      "Epoch 124 Training loss 0.44779555427703355\n",
      "Epoch 125 iteration 0 loss 0.2234220802783966\n",
      "Epoch 125 iteration 100 loss 0.30633583664894104\n",
      "Epoch 125 iteration 200 loss 0.5991238355636597\n",
      "Epoch 125 Training loss 0.45058179317221486\n",
      "Evaluation loss 7.044851798624704\n",
      "Epoch 126 iteration 0 loss 0.1858605146408081\n",
      "Epoch 126 iteration 100 loss 0.3105940818786621\n",
      "Epoch 126 iteration 200 loss 0.570041835308075\n",
      "Epoch 126 Training loss 0.44972942298509727\n",
      "Epoch 127 iteration 0 loss 0.1868489682674408\n",
      "Epoch 127 iteration 100 loss 0.31987836956977844\n",
      "Epoch 127 iteration 200 loss 0.5907549262046814\n",
      "Epoch 127 Training loss 0.44385331515468107\n",
      "Epoch 128 iteration 0 loss 0.2172490358352661\n",
      "Epoch 128 iteration 100 loss 0.3082002103328705\n",
      "Epoch 128 iteration 200 loss 0.5490601658821106\n",
      "Epoch 128 Training loss 0.44379889030590325\n",
      "Epoch 129 iteration 0 loss 0.23713378608226776\n",
      "Epoch 129 iteration 100 loss 0.27624058723449707\n",
      "Epoch 129 iteration 200 loss 0.5870956778526306\n",
      "Epoch 129 Training loss 0.4452003822765285\n",
      "Epoch 130 iteration 0 loss 0.19195853173732758\n",
      "Epoch 130 iteration 100 loss 0.3291684091091156\n",
      "Epoch 130 iteration 200 loss 0.5728994607925415\n",
      "Epoch 130 Training loss 0.44435150771206083\n",
      "Evaluation loss 7.077433043233671\n",
      "Epoch 131 iteration 0 loss 0.16740873456001282\n",
      "Epoch 131 iteration 100 loss 0.2885497212409973\n",
      "Epoch 131 iteration 200 loss 0.5838931202888489\n",
      "Epoch 131 Training loss 0.4480465082353124\n",
      "Epoch 132 iteration 0 loss 0.16756609082221985\n",
      "Epoch 132 iteration 100 loss 0.3194563686847687\n",
      "Epoch 132 iteration 200 loss 0.5791863203048706\n",
      "Epoch 132 Training loss 0.4544982100807056\n",
      "Epoch 133 iteration 0 loss 0.2255396842956543\n",
      "Epoch 133 iteration 100 loss 0.2917943596839905\n",
      "Epoch 133 iteration 200 loss 0.5997287631034851\n",
      "Epoch 133 Training loss 0.4444574344161875\n",
      "Epoch 134 iteration 0 loss 0.1597127616405487\n",
      "Epoch 134 iteration 100 loss 0.2978259325027466\n",
      "Epoch 134 iteration 200 loss 0.5510928630828857\n",
      "Epoch 134 Training loss 0.447176600443924\n",
      "Epoch 135 iteration 0 loss 0.15413019061088562\n",
      "Epoch 135 iteration 100 loss 0.3174819052219391\n",
      "Epoch 135 iteration 200 loss 0.6139024496078491\n",
      "Epoch 135 Training loss 0.4491998748817394\n",
      "Evaluation loss 7.110911604895232\n",
      "Epoch 136 iteration 0 loss 0.1609540730714798\n",
      "Epoch 136 iteration 100 loss 0.3026869297027588\n",
      "Epoch 136 iteration 200 loss 0.6364982724189758\n",
      "Epoch 136 Training loss 0.4579876959944751\n",
      "Epoch 137 iteration 0 loss 0.19027844071388245\n",
      "Epoch 137 iteration 100 loss 0.27190688252449036\n",
      "Epoch 137 iteration 200 loss 0.621430516242981\n",
      "Epoch 137 Training loss 0.4472656146712127\n",
      "Epoch 138 iteration 0 loss 0.17184242606163025\n",
      "Epoch 138 iteration 100 loss 0.30699965357780457\n",
      "Epoch 138 iteration 200 loss 0.577532172203064\n",
      "Epoch 138 Training loss 0.44935018615019645\n",
      "Epoch 139 iteration 0 loss 0.17539800703525543\n",
      "Epoch 139 iteration 100 loss 0.3073861002922058\n",
      "Epoch 139 iteration 200 loss 0.5768182277679443\n",
      "Epoch 139 Training loss 0.4524843733696876\n",
      "Epoch 140 iteration 0 loss 0.18813425302505493\n",
      "Epoch 140 iteration 100 loss 0.2774982750415802\n",
      "Epoch 140 iteration 200 loss 0.5970984697341919\n",
      "Epoch 140 Training loss 0.46103092383335553\n",
      "Evaluation loss 7.158783293672764\n",
      "Epoch 141 iteration 0 loss 0.19564704596996307\n",
      "Epoch 141 iteration 100 loss 0.31924694776535034\n",
      "Epoch 141 iteration 200 loss 0.5825937390327454\n",
      "Epoch 141 Training loss 0.4495067004237662\n",
      "Epoch 142 iteration 0 loss 0.14490275084972382\n",
      "Epoch 142 iteration 100 loss 0.31714436411857605\n",
      "Epoch 142 iteration 200 loss 0.59195876121521\n",
      "Epoch 142 Training loss 0.4550787540287286\n",
      "Epoch 143 iteration 0 loss 0.2121523916721344\n",
      "Epoch 143 iteration 100 loss 0.32630643248558044\n",
      "Epoch 143 iteration 200 loss 0.6153189539909363\n",
      "Epoch 143 Training loss 0.4560027682092662\n",
      "Epoch 144 iteration 0 loss 0.20142300426959991\n",
      "Epoch 144 iteration 100 loss 0.2916262447834015\n",
      "Epoch 144 iteration 200 loss 0.5885892510414124\n",
      "Epoch 144 Training loss 0.4577809843603409\n",
      "Epoch 145 iteration 0 loss 0.20886854827404022\n",
      "Epoch 145 iteration 100 loss 0.3699859380722046\n",
      "Epoch 145 iteration 200 loss 0.5786001086235046\n",
      "Epoch 145 Training loss 0.45807589941331733\n",
      "Evaluation loss 7.172874273742578\n",
      "Epoch 146 iteration 0 loss 0.21500177681446075\n",
      "Epoch 146 iteration 100 loss 0.37174081802368164\n",
      "Epoch 146 iteration 200 loss 0.6011666059494019\n",
      "Epoch 146 Training loss 0.4582968813909485\n",
      "Epoch 147 iteration 0 loss 0.18714067339897156\n",
      "Epoch 147 iteration 100 loss 0.3091566562652588\n",
      "Epoch 147 iteration 200 loss 0.6038733720779419\n",
      "Epoch 147 Training loss 0.45414414981926066\n",
      "Epoch 148 iteration 0 loss 0.2137923538684845\n",
      "Epoch 148 iteration 100 loss 0.2934347689151764\n",
      "Epoch 148 iteration 200 loss 0.5887882113456726\n",
      "Epoch 148 Training loss 0.4476326607695704\n",
      "Epoch 149 iteration 0 loss 0.2179533988237381\n",
      "Epoch 149 iteration 100 loss 0.2825671434402466\n",
      "Epoch 149 iteration 200 loss 0.523077666759491\n",
      "Epoch 149 Training loss 0.4450302300800138\n",
      "Epoch 150 iteration 0 loss 0.1837020367383957\n",
      "Epoch 150 iteration 100 loss 0.31590816378593445\n",
      "Epoch 150 iteration 200 loss 0.5686831474304199\n",
      "Epoch 150 Training loss 0.45040927331872765\n",
      "Evaluation loss 7.179501295970385\n",
      "Epoch 151 iteration 0 loss 0.21399147808551788\n",
      "Epoch 151 iteration 100 loss 0.3043009042739868\n",
      "Epoch 151 iteration 200 loss 0.6274454593658447\n",
      "Epoch 151 Training loss 0.4551596752802378\n",
      "Epoch 152 iteration 0 loss 0.18740926682949066\n",
      "Epoch 152 iteration 100 loss 0.35208797454833984\n",
      "Epoch 152 iteration 200 loss 0.6085643768310547\n",
      "Epoch 152 Training loss 0.45518951764791127\n",
      "Epoch 153 iteration 0 loss 0.21816444396972656\n",
      "Epoch 153 iteration 100 loss 0.3024459779262543\n",
      "Epoch 153 iteration 200 loss 0.605219841003418\n",
      "Epoch 153 Training loss 0.4543001179730868\n",
      "Epoch 154 iteration 0 loss 0.22260931134223938\n",
      "Epoch 154 iteration 100 loss 0.3173843026161194\n",
      "Epoch 154 iteration 200 loss 0.5708082914352417\n",
      "Epoch 154 Training loss 0.45479417878479395\n",
      "Epoch 155 iteration 0 loss 0.22727671265602112\n",
      "Epoch 155 iteration 100 loss 0.3010408580303192\n",
      "Epoch 155 iteration 200 loss 0.5832198858261108\n",
      "Epoch 155 Training loss 0.45125112842607645\n",
      "Evaluation loss 7.25464236057704\n",
      "Epoch 156 iteration 0 loss 0.23774321377277374\n",
      "Epoch 156 iteration 100 loss 0.2643645405769348\n",
      "Epoch 156 iteration 200 loss 0.6225882172584534\n",
      "Epoch 156 Training loss 0.4523302823273677\n",
      "Epoch 157 iteration 0 loss 0.1815415918827057\n",
      "Epoch 157 iteration 100 loss 0.2934176027774811\n",
      "Epoch 157 iteration 200 loss 0.61116623878479\n",
      "Epoch 157 Training loss 0.454312152401524\n",
      "Epoch 158 iteration 0 loss 0.19315795600414276\n",
      "Epoch 158 iteration 100 loss 0.2719951272010803\n",
      "Epoch 158 iteration 200 loss 0.6364681720733643\n",
      "Epoch 158 Training loss 0.4577758543869087\n",
      "Epoch 159 iteration 0 loss 0.22274455428123474\n",
      "Epoch 159 iteration 100 loss 0.3193928897380829\n",
      "Epoch 159 iteration 200 loss 0.5710665583610535\n",
      "Epoch 159 Training loss 0.4630497307950239\n",
      "Epoch 160 iteration 0 loss 0.17184960842132568\n",
      "Epoch 160 iteration 100 loss 0.23518338799476624\n",
      "Epoch 160 iteration 200 loss 0.588370680809021\n",
      "Epoch 160 Training loss 0.463139923083983\n",
      "Evaluation loss 7.2284346786049865\n",
      "Epoch 161 iteration 0 loss 0.212814062833786\n",
      "Epoch 161 iteration 100 loss 0.30131587386131287\n",
      "Epoch 161 iteration 200 loss 0.5878434181213379\n",
      "Epoch 161 Training loss 0.4646261321338101\n",
      "Epoch 162 iteration 0 loss 0.16823050379753113\n",
      "Epoch 162 iteration 100 loss 0.35823410749435425\n",
      "Epoch 162 iteration 200 loss 0.652822732925415\n",
      "Epoch 162 Training loss 0.4664786497337195\n",
      "Epoch 163 iteration 0 loss 0.21389003098011017\n",
      "Epoch 163 iteration 100 loss 0.29035717248916626\n",
      "Epoch 163 iteration 200 loss 0.5901021957397461\n",
      "Epoch 163 Training loss 0.4742435281035262\n",
      "Epoch 164 iteration 0 loss 0.20275232195854187\n",
      "Epoch 164 iteration 100 loss 0.3306286931037903\n",
      "Epoch 164 iteration 200 loss 0.6232302188873291\n",
      "Epoch 164 Training loss 0.46692132880239456\n",
      "Epoch 165 iteration 0 loss 0.2308017760515213\n",
      "Epoch 165 iteration 100 loss 0.2901403307914734\n",
      "Epoch 165 iteration 200 loss 0.5962569117546082\n",
      "Epoch 165 Training loss 0.4606196037008112\n",
      "Evaluation loss 7.275708415998909\n",
      "Epoch 166 iteration 0 loss 0.15652136504650116\n",
      "Epoch 166 iteration 100 loss 0.2784889340400696\n",
      "Epoch 166 iteration 200 loss 0.6290608048439026\n",
      "Epoch 166 Training loss 0.4598141950233142\n",
      "Epoch 167 iteration 0 loss 0.1608697772026062\n",
      "Epoch 167 iteration 100 loss 0.312928706407547\n",
      "Epoch 167 iteration 200 loss 0.6168850660324097\n",
      "Epoch 167 Training loss 0.4619272751742548\n",
      "Epoch 168 iteration 0 loss 0.19560661911964417\n",
      "Epoch 168 iteration 100 loss 0.26942697167396545\n",
      "Epoch 168 iteration 200 loss 0.6718623638153076\n",
      "Epoch 168 Training loss 0.46045540023476084\n",
      "Epoch 169 iteration 0 loss 0.24695059657096863\n",
      "Epoch 169 iteration 100 loss 0.3243488073348999\n",
      "Epoch 169 iteration 200 loss 0.5660624504089355\n",
      "Epoch 169 Training loss 0.4640328819910125\n",
      "Epoch 170 iteration 0 loss 0.172259122133255\n",
      "Epoch 170 iteration 100 loss 0.27095329761505127\n",
      "Epoch 170 iteration 200 loss 0.5617855191230774\n",
      "Epoch 170 Training loss 0.46929888204813125\n",
      "Evaluation loss 7.263044722871742\n",
      "Epoch 171 iteration 0 loss 0.23154087364673615\n",
      "Epoch 171 iteration 100 loss 0.3035508692264557\n",
      "Epoch 171 iteration 200 loss 0.6165280938148499\n",
      "Epoch 171 Training loss 0.4702161579428747\n",
      "Epoch 172 iteration 0 loss 0.18546991050243378\n",
      "Epoch 172 iteration 100 loss 0.24457867443561554\n",
      "Epoch 172 iteration 200 loss 0.613932728767395\n",
      "Epoch 172 Training loss 0.46712099702766324\n",
      "Epoch 173 iteration 0 loss 0.2369280755519867\n",
      "Epoch 173 iteration 100 loss 0.26688888669013977\n",
      "Epoch 173 iteration 200 loss 0.6120859980583191\n",
      "Epoch 173 Training loss 0.46642081209762437\n",
      "Epoch 174 iteration 0 loss 0.22599419951438904\n",
      "Epoch 174 iteration 100 loss 0.28008782863616943\n",
      "Epoch 174 iteration 200 loss 0.6137368679046631\n",
      "Epoch 174 Training loss 0.4587757639370359\n",
      "Epoch 175 iteration 0 loss 0.22559313476085663\n",
      "Epoch 175 iteration 100 loss 0.3084332346916199\n",
      "Epoch 175 iteration 200 loss 0.6324008703231812\n",
      "Epoch 175 Training loss 0.46209483975333043\n",
      "Evaluation loss 7.253821561291036\n",
      "Epoch 176 iteration 0 loss 0.17311610281467438\n",
      "Epoch 176 iteration 100 loss 0.32202550768852234\n",
      "Epoch 176 iteration 200 loss 0.5698104500770569\n",
      "Epoch 176 Training loss 0.4640090930264582\n",
      "Epoch 177 iteration 0 loss 0.164302796125412\n",
      "Epoch 177 iteration 100 loss 0.3502030074596405\n",
      "Epoch 177 iteration 200 loss 0.6249803900718689\n",
      "Epoch 177 Training loss 0.46455022038809635\n",
      "Epoch 178 iteration 0 loss 0.16797669231891632\n",
      "Epoch 178 iteration 100 loss 0.2904132008552551\n",
      "Epoch 178 iteration 200 loss 0.6179154515266418\n",
      "Epoch 178 Training loss 0.4684057896248987\n",
      "Epoch 179 iteration 0 loss 0.22563764452934265\n",
      "Epoch 179 iteration 100 loss 0.334717333316803\n",
      "Epoch 179 iteration 200 loss 0.6358407735824585\n",
      "Epoch 179 Training loss 0.4719614810290994\n",
      "Epoch 180 iteration 0 loss 0.21977253258228302\n",
      "Epoch 180 iteration 100 loss 0.3273390233516693\n",
      "Epoch 180 iteration 200 loss 0.6138978600502014\n",
      "Epoch 180 Training loss 0.4760187534368074\n",
      "Evaluation loss 7.2861998231764575\n",
      "Epoch 181 iteration 0 loss 0.17059800028800964\n",
      "Epoch 181 iteration 100 loss 0.2730899751186371\n",
      "Epoch 181 iteration 200 loss 0.6313000917434692\n",
      "Epoch 181 Training loss 0.4715418245683431\n",
      "Epoch 182 iteration 0 loss 0.246012881398201\n",
      "Epoch 182 iteration 100 loss 0.3569108843803406\n",
      "Epoch 182 iteration 200 loss 0.6259079575538635\n",
      "Epoch 182 Training loss 0.474862353112377\n",
      "Epoch 183 iteration 0 loss 0.2119288444519043\n",
      "Epoch 183 iteration 100 loss 0.31863492727279663\n",
      "Epoch 183 iteration 200 loss 0.6032136082649231\n",
      "Epoch 183 Training loss 0.47911668265643154\n",
      "Epoch 184 iteration 0 loss 0.16488167643547058\n",
      "Epoch 184 iteration 100 loss 0.2860851287841797\n",
      "Epoch 184 iteration 200 loss 0.580661416053772\n",
      "Epoch 184 Training loss 0.48376137237620803\n",
      "Epoch 185 iteration 0 loss 0.22755677998065948\n",
      "Epoch 185 iteration 100 loss 0.2849007844924927\n",
      "Epoch 185 iteration 200 loss 0.6180511713027954\n",
      "Epoch 185 Training loss 0.4777712197002518\n",
      "Evaluation loss 7.306662777021142\n",
      "Epoch 186 iteration 0 loss 0.20090647041797638\n",
      "Epoch 186 iteration 100 loss 0.30877330899238586\n",
      "Epoch 186 iteration 200 loss 0.6180567741394043\n",
      "Epoch 186 Training loss 0.47705801175853807\n",
      "Epoch 187 iteration 0 loss 0.16945524513721466\n",
      "Epoch 187 iteration 100 loss 0.34454333782196045\n",
      "Epoch 187 iteration 200 loss 0.6448808312416077\n",
      "Epoch 187 Training loss 0.4827037091782654\n",
      "Epoch 188 iteration 0 loss 0.1916024386882782\n",
      "Epoch 188 iteration 100 loss 0.351849764585495\n",
      "Epoch 188 iteration 200 loss 0.6388714909553528\n",
      "Epoch 188 Training loss 0.4873453800104263\n",
      "Epoch 189 iteration 0 loss 0.1711033582687378\n",
      "Epoch 189 iteration 100 loss 0.320458322763443\n",
      "Epoch 189 iteration 200 loss 0.643039345741272\n",
      "Epoch 189 Training loss 0.4837067455699753\n",
      "Epoch 190 iteration 0 loss 0.18250620365142822\n",
      "Epoch 190 iteration 100 loss 0.2875748872756958\n",
      "Epoch 190 iteration 200 loss 0.6605246067047119\n",
      "Epoch 190 Training loss 0.4829773062104195\n",
      "Evaluation loss 7.315190781925948\n",
      "Epoch 191 iteration 0 loss 0.23574642837047577\n",
      "Epoch 191 iteration 100 loss 0.3089850842952728\n",
      "Epoch 191 iteration 200 loss 0.6739866733551025\n",
      "Epoch 191 Training loss 0.48330970550391844\n",
      "Epoch 192 iteration 0 loss 0.20475706458091736\n",
      "Epoch 192 iteration 100 loss 0.3595772683620453\n",
      "Epoch 192 iteration 200 loss 0.7386519312858582\n",
      "Epoch 192 Training loss 0.4768635807420018\n",
      "Epoch 193 iteration 0 loss 0.19118686020374298\n",
      "Epoch 193 iteration 100 loss 0.29818499088287354\n",
      "Epoch 193 iteration 200 loss 0.6670700311660767\n",
      "Epoch 193 Training loss 0.4799966806169524\n",
      "Epoch 194 iteration 0 loss 0.18684163689613342\n",
      "Epoch 194 iteration 100 loss 0.26690828800201416\n",
      "Epoch 194 iteration 200 loss 0.614698588848114\n",
      "Epoch 194 Training loss 0.4770486019803214\n",
      "Epoch 195 iteration 0 loss 0.1953100860118866\n",
      "Epoch 195 iteration 100 loss 0.25463172793388367\n",
      "Epoch 195 iteration 200 loss 0.6480624675750732\n",
      "Epoch 195 Training loss 0.4783283704915841\n",
      "Evaluation loss 7.328673458543316\n",
      "Epoch 196 iteration 0 loss 0.22851498425006866\n",
      "Epoch 196 iteration 100 loss 0.30468595027923584\n",
      "Epoch 196 iteration 200 loss 0.7585297226905823\n",
      "Epoch 196 Training loss 0.4828736766994252\n",
      "Epoch 197 iteration 0 loss 0.24222052097320557\n",
      "Epoch 197 iteration 100 loss 0.32671645283699036\n",
      "Epoch 197 iteration 200 loss 0.7219445705413818\n",
      "Epoch 197 Training loss 0.4898436629009583\n",
      "Epoch 198 iteration 0 loss 0.20221081376075745\n",
      "Epoch 198 iteration 100 loss 0.30255892872810364\n",
      "Epoch 198 iteration 200 loss 0.7298331260681152\n",
      "Epoch 198 Training loss 0.49254195752693974\n",
      "Epoch 199 iteration 0 loss 0.2110930234193802\n",
      "Epoch 199 iteration 100 loss 0.3054386377334595\n",
      "Epoch 199 iteration 200 loss 0.6821986436843872\n",
      "Epoch 199 Training loss 0.4954076562523619\n",
      "Epoch 200 iteration 0 loss 0.1897408366203308\n",
      "Epoch 200 iteration 100 loss 0.33579397201538086\n",
      "Epoch 200 iteration 200 loss 0.6882576942443848\n",
      "Epoch 200 Training loss 0.4957273098675631\n",
      "Evaluation loss 7.302940813218607\n",
      "Epoch 201 iteration 0 loss 0.23053905367851257\n",
      "Epoch 201 iteration 100 loss 0.30956903100013733\n",
      "Epoch 201 iteration 200 loss 0.6281479001045227\n",
      "Epoch 201 Training loss 0.4945782418095713\n",
      "Epoch 202 iteration 0 loss 0.24871543049812317\n",
      "Epoch 202 iteration 100 loss 0.29780811071395874\n",
      "Epoch 202 iteration 200 loss 0.6416507959365845\n",
      "Epoch 202 Training loss 0.48929221988122706\n",
      "Epoch 203 iteration 0 loss 0.2370329648256302\n",
      "Epoch 203 iteration 100 loss 0.32446300983428955\n",
      "Epoch 203 iteration 200 loss 0.635261058807373\n",
      "Epoch 203 Training loss 0.49602422660009826\n",
      "Epoch 204 iteration 0 loss 0.18126417696475983\n",
      "Epoch 204 iteration 100 loss 0.33066365122795105\n",
      "Epoch 204 iteration 200 loss 0.6466416120529175\n",
      "Epoch 204 Training loss 0.49504947621222234\n",
      "Epoch 205 iteration 0 loss 0.2194232940673828\n",
      "Epoch 205 iteration 100 loss 0.28529292345046997\n",
      "Epoch 205 iteration 200 loss 0.6310566067695618\n",
      "Epoch 205 Training loss 0.4942569631030077\n",
      "Evaluation loss 7.3514691591443775\n",
      "Epoch 206 iteration 0 loss 0.1725870668888092\n",
      "Epoch 206 iteration 100 loss 0.28759151697158813\n",
      "Epoch 206 iteration 200 loss 0.6395577788352966\n",
      "Epoch 206 Training loss 0.4951425532847679\n",
      "Epoch 207 iteration 0 loss 0.18733415007591248\n",
      "Epoch 207 iteration 100 loss 0.3357405960559845\n",
      "Epoch 207 iteration 200 loss 0.6824935674667358\n",
      "Epoch 207 Training loss 0.49102148973281423\n",
      "Epoch 208 iteration 0 loss 0.21146385371685028\n",
      "Epoch 208 iteration 100 loss 0.3401028513908386\n",
      "Epoch 208 iteration 200 loss 0.6393253207206726\n",
      "Epoch 208 Training loss 0.4968869913563459\n",
      "Epoch 209 iteration 0 loss 0.2530759274959564\n",
      "Epoch 209 iteration 100 loss 0.33424124121665955\n",
      "Epoch 209 iteration 200 loss 0.6138391494750977\n",
      "Epoch 209 Training loss 0.4914396119361235\n",
      "Epoch 210 iteration 0 loss 0.21532297134399414\n",
      "Epoch 210 iteration 100 loss 0.31213706731796265\n",
      "Epoch 210 iteration 200 loss 0.608123242855072\n",
      "Epoch 210 Training loss 0.48769537432727056\n",
      "Evaluation loss 7.325565133446812\n",
      "Epoch 211 iteration 0 loss 0.1945575773715973\n",
      "Epoch 211 iteration 100 loss 0.3667687773704529\n",
      "Epoch 211 iteration 200 loss 0.6893391609191895\n",
      "Epoch 211 Training loss 0.49687120108585586\n",
      "Epoch 212 iteration 0 loss 0.21724975109100342\n",
      "Epoch 212 iteration 100 loss 0.33345136046409607\n",
      "Epoch 212 iteration 200 loss 0.6264185309410095\n",
      "Epoch 212 Training loss 0.4951611167058325\n",
      "Epoch 213 iteration 0 loss 0.19327659904956818\n",
      "Epoch 213 iteration 100 loss 0.3513163924217224\n",
      "Epoch 213 iteration 200 loss 0.6568677425384521\n",
      "Epoch 213 Training loss 0.4953627648252023\n",
      "Epoch 214 iteration 0 loss 0.22950875759124756\n",
      "Epoch 214 iteration 100 loss 0.2937697470188141\n",
      "Epoch 214 iteration 200 loss 0.6145520806312561\n",
      "Epoch 214 Training loss 0.4908583697389388\n",
      "Epoch 215 iteration 0 loss 0.2024976760149002\n",
      "Epoch 215 iteration 100 loss 0.3333953022956848\n",
      "Epoch 215 iteration 200 loss 0.687664270401001\n",
      "Epoch 215 Training loss 0.4988455977605808\n",
      "Evaluation loss 7.34299880989118\n",
      "Epoch 216 iteration 0 loss 0.1990741789340973\n",
      "Epoch 216 iteration 100 loss 0.31905102729797363\n",
      "Epoch 216 iteration 200 loss 0.6275556683540344\n",
      "Epoch 216 Training loss 0.4978485166792261\n",
      "Epoch 217 iteration 0 loss 0.2378424108028412\n",
      "Epoch 217 iteration 100 loss 0.3751859962940216\n",
      "Epoch 217 iteration 200 loss 0.7010185122489929\n",
      "Epoch 217 Training loss 0.5018668053950576\n",
      "Epoch 218 iteration 0 loss 0.2290421724319458\n",
      "Epoch 218 iteration 100 loss 0.3360086977481842\n",
      "Epoch 218 iteration 200 loss 0.6881682276725769\n",
      "Epoch 218 Training loss 0.503991275826439\n",
      "Epoch 219 iteration 0 loss 0.2789953947067261\n",
      "Epoch 219 iteration 100 loss 0.2973466217517853\n",
      "Epoch 219 iteration 200 loss 0.6439886093139648\n",
      "Epoch 219 Training loss 0.5033833584950053\n",
      "Epoch 220 iteration 0 loss 0.27136853337287903\n",
      "Epoch 220 iteration 100 loss 0.3288728594779968\n",
      "Epoch 220 iteration 200 loss 0.7083282470703125\n",
      "Epoch 220 Training loss 0.5101527096328056\n",
      "Evaluation loss 7.374947049493147\n",
      "Epoch 221 iteration 0 loss 0.250487744808197\n",
      "Epoch 221 iteration 100 loss 0.31433165073394775\n",
      "Epoch 221 iteration 200 loss 0.6528359055519104\n",
      "Epoch 221 Training loss 0.5067631689846891\n",
      "Epoch 222 iteration 0 loss 0.1810835897922516\n",
      "Epoch 222 iteration 100 loss 0.3426978588104248\n",
      "Epoch 222 iteration 200 loss 0.6626598834991455\n",
      "Epoch 222 Training loss 0.5007555780119158\n",
      "Epoch 223 iteration 0 loss 0.21344098448753357\n",
      "Epoch 223 iteration 100 loss 0.38432803750038147\n",
      "Epoch 223 iteration 200 loss 0.6457144618034363\n",
      "Epoch 223 Training loss 0.5011639775026469\n",
      "Epoch 224 iteration 0 loss 0.1879686415195465\n",
      "Epoch 224 iteration 100 loss 0.3297048509120941\n",
      "Epoch 224 iteration 200 loss 0.7106080055236816\n",
      "Epoch 224 Training loss 0.5068076855780713\n",
      "Epoch 225 iteration 0 loss 0.17161527276039124\n",
      "Epoch 225 iteration 100 loss 0.3416471481323242\n",
      "Epoch 225 iteration 200 loss 0.7978952527046204\n",
      "Epoch 225 Training loss 0.5060675997655215\n",
      "Evaluation loss 7.389660710609979\n",
      "Epoch 226 iteration 0 loss 0.23691332340240479\n",
      "Epoch 226 iteration 100 loss 0.3506868779659271\n",
      "Epoch 226 iteration 200 loss 0.7326908707618713\n",
      "Epoch 226 Training loss 0.5053412736949323\n",
      "Epoch 227 iteration 0 loss 0.19376537203788757\n",
      "Epoch 227 iteration 100 loss 0.33000436425209045\n",
      "Epoch 227 iteration 200 loss 0.671262264251709\n",
      "Epoch 227 Training loss 0.5107752939251284\n",
      "Epoch 228 iteration 0 loss 0.1542276293039322\n",
      "Epoch 228 iteration 100 loss 0.36996084451675415\n",
      "Epoch 228 iteration 200 loss 0.6734092831611633\n",
      "Epoch 228 Training loss 0.5102225241171244\n",
      "Epoch 229 iteration 0 loss 0.2173163890838623\n",
      "Epoch 229 iteration 100 loss 0.37980422377586365\n",
      "Epoch 229 iteration 200 loss 0.6684883236885071\n",
      "Epoch 229 Training loss 0.5132837839465401\n",
      "Epoch 230 iteration 0 loss 0.1995048224925995\n",
      "Epoch 230 iteration 100 loss 0.32354336977005005\n",
      "Epoch 230 iteration 200 loss 0.713105320930481\n",
      "Epoch 230 Training loss 0.5220596294552373\n",
      "Evaluation loss 7.418426593124628\n",
      "Epoch 231 iteration 0 loss 0.20158061385154724\n",
      "Epoch 231 iteration 100 loss 0.3384895622730255\n",
      "Epoch 231 iteration 200 loss 0.6452130079269409\n",
      "Epoch 231 Training loss 0.5127143053355665\n",
      "Epoch 232 iteration 0 loss 0.21551194787025452\n",
      "Epoch 232 iteration 100 loss 0.36134734749794006\n",
      "Epoch 232 iteration 200 loss 0.6541669964790344\n",
      "Epoch 232 Training loss 0.519974759616008\n",
      "Epoch 233 iteration 0 loss 0.18271970748901367\n",
      "Epoch 233 iteration 100 loss 0.32315880060195923\n",
      "Epoch 233 iteration 200 loss 0.7303553819656372\n",
      "Epoch 233 Training loss 0.5156401905327604\n",
      "Epoch 234 iteration 0 loss 0.17255651950836182\n",
      "Epoch 234 iteration 100 loss 0.36239179968833923\n",
      "Epoch 234 iteration 200 loss 0.6958090662956238\n",
      "Epoch 234 Training loss 0.5179030330765793\n",
      "Epoch 235 iteration 0 loss 0.23980021476745605\n",
      "Epoch 235 iteration 100 loss 0.36204153299331665\n",
      "Epoch 235 iteration 200 loss 0.6640137434005737\n",
      "Epoch 235 Training loss 0.5190996370408063\n",
      "Evaluation loss 7.405829116484176\n",
      "Epoch 236 iteration 0 loss 0.2023833692073822\n",
      "Epoch 236 iteration 100 loss 0.33767810463905334\n",
      "Epoch 236 iteration 200 loss 0.6019072532653809\n",
      "Epoch 236 Training loss 0.5207894402996565\n",
      "Epoch 237 iteration 0 loss 0.22928018867969513\n",
      "Epoch 237 iteration 100 loss 0.31794461607933044\n",
      "Epoch 237 iteration 200 loss 0.6815606951713562\n",
      "Epoch 237 Training loss 0.5238120927125663\n",
      "Epoch 238 iteration 0 loss 0.219181627035141\n",
      "Epoch 238 iteration 100 loss 0.2960025668144226\n",
      "Epoch 238 iteration 200 loss 0.6523366570472717\n",
      "Epoch 238 Training loss 0.5259673476944725\n",
      "Epoch 239 iteration 0 loss 0.20280325412750244\n",
      "Epoch 239 iteration 100 loss 0.31057822704315186\n",
      "Epoch 239 iteration 200 loss 0.6144344806671143\n",
      "Epoch 239 Training loss 0.5211719254751714\n",
      "Epoch 240 iteration 0 loss 0.20781345665454865\n",
      "Epoch 240 iteration 100 loss 0.30598852038383484\n",
      "Epoch 240 iteration 200 loss 0.6757335066795349\n",
      "Epoch 240 Training loss 0.5195892772764779\n",
      "Evaluation loss 7.3847416381945\n",
      "Epoch 241 iteration 0 loss 0.22203759849071503\n",
      "Epoch 241 iteration 100 loss 0.3137011229991913\n",
      "Epoch 241 iteration 200 loss 0.6518751382827759\n",
      "Epoch 241 Training loss 0.5209318468140249\n",
      "Epoch 242 iteration 0 loss 0.18038645386695862\n",
      "Epoch 242 iteration 100 loss 0.3342810273170471\n",
      "Epoch 242 iteration 200 loss 0.6055721640586853\n",
      "Epoch 242 Training loss 0.5187244672263608\n",
      "Epoch 243 iteration 0 loss 0.22002427279949188\n",
      "Epoch 243 iteration 100 loss 0.29970359802246094\n",
      "Epoch 243 iteration 200 loss 0.680360734462738\n",
      "Epoch 243 Training loss 0.5202907402027995\n",
      "Epoch 244 iteration 0 loss 0.1973760575056076\n",
      "Epoch 244 iteration 100 loss 0.33117568492889404\n",
      "Epoch 244 iteration 200 loss 0.713854193687439\n",
      "Epoch 244 Training loss 0.5195121193254325\n",
      "Epoch 245 iteration 0 loss 0.17099425196647644\n",
      "Epoch 245 iteration 100 loss 0.30721548199653625\n",
      "Epoch 245 iteration 200 loss 0.764590859413147\n",
      "Epoch 245 Training loss 0.5191822725852507\n",
      "Evaluation loss 7.397254255989004\n",
      "Epoch 246 iteration 0 loss 0.24063274264335632\n",
      "Epoch 246 iteration 100 loss 0.2719399631023407\n",
      "Epoch 246 iteration 200 loss 0.7333999872207642\n",
      "Epoch 246 Training loss 0.5178911794034277\n",
      "Epoch 247 iteration 0 loss 0.23604539036750793\n",
      "Epoch 247 iteration 100 loss 0.27419596910476685\n",
      "Epoch 247 iteration 200 loss 0.7121966481208801\n",
      "Epoch 247 Training loss 0.5238945472559826\n",
      "Epoch 248 iteration 0 loss 0.2617899775505066\n",
      "Epoch 248 iteration 100 loss 0.3199399411678314\n",
      "Epoch 248 iteration 200 loss 0.6457876563072205\n",
      "Epoch 248 Training loss 0.5310493305997487\n",
      "Epoch 249 iteration 0 loss 0.24816687405109406\n",
      "Epoch 249 iteration 100 loss 0.4207741320133209\n",
      "Epoch 249 iteration 200 loss 0.7400047183036804\n",
      "Epoch 249 Training loss 0.535385655911392\n",
      "Epoch 250 iteration 0 loss 0.23204255104064941\n",
      "Epoch 250 iteration 100 loss 0.31789669394493103\n",
      "Epoch 250 iteration 200 loss 0.702294647693634\n",
      "Epoch 250 Training loss 0.5309236071105958\n",
      "Evaluation loss 7.403986248344881\n",
      "Epoch 251 iteration 0 loss 0.23132674396038055\n",
      "Epoch 251 iteration 100 loss 0.3934890627861023\n",
      "Epoch 251 iteration 200 loss 0.6985735893249512\n",
      "Epoch 251 Training loss 0.5373408156098284\n",
      "Epoch 252 iteration 0 loss 0.17409949004650116\n",
      "Epoch 252 iteration 100 loss 0.4516879916191101\n",
      "Epoch 252 iteration 200 loss 0.6665258407592773\n",
      "Epoch 252 Training loss 0.5405340318765348\n",
      "Epoch 253 iteration 0 loss 0.24286389350891113\n",
      "Epoch 253 iteration 100 loss 0.27908164262771606\n",
      "Epoch 253 iteration 200 loss 0.7638956904411316\n",
      "Epoch 253 Training loss 0.5337627813841875\n",
      "Epoch 254 iteration 0 loss 0.22099192440509796\n",
      "Epoch 254 iteration 100 loss 0.33675268292427063\n",
      "Epoch 254 iteration 200 loss 0.7285613417625427\n",
      "Epoch 254 Training loss 0.5345926356944409\n",
      "Epoch 255 iteration 0 loss 0.2277708798646927\n",
      "Epoch 255 iteration 100 loss 0.3508646786212921\n",
      "Epoch 255 iteration 200 loss 0.690920889377594\n",
      "Epoch 255 Training loss 0.540646991287711\n",
      "Evaluation loss 7.409654245814265\n",
      "Epoch 256 iteration 0 loss 0.20177476108074188\n",
      "Epoch 256 iteration 100 loss 0.4353927671909332\n",
      "Epoch 256 iteration 200 loss 0.6681137681007385\n",
      "Epoch 256 Training loss 0.540430424518883\n",
      "Epoch 257 iteration 0 loss 0.28934744000434875\n",
      "Epoch 257 iteration 100 loss 0.35713791847229004\n",
      "Epoch 257 iteration 200 loss 0.6750105023384094\n",
      "Epoch 257 Training loss 0.5351951047975366\n",
      "Epoch 258 iteration 0 loss 0.2966126501560211\n",
      "Epoch 258 iteration 100 loss 0.3291829526424408\n",
      "Epoch 258 iteration 200 loss 0.7322314381599426\n",
      "Epoch 258 Training loss 0.5388624568985582\n",
      "Epoch 259 iteration 0 loss 0.1587141752243042\n",
      "Epoch 259 iteration 100 loss 0.36019036173820496\n",
      "Epoch 259 iteration 200 loss 0.6682891249656677\n",
      "Epoch 259 Training loss 0.5385798258540857\n",
      "Epoch 260 iteration 0 loss 0.16740819811820984\n",
      "Epoch 260 iteration 100 loss 0.346128910779953\n",
      "Epoch 260 iteration 200 loss 0.6651114821434021\n",
      "Epoch 260 Training loss 0.5314882132179627\n",
      "Evaluation loss 7.442028278137444\n",
      "Epoch 261 iteration 0 loss 0.19905352592468262\n",
      "Epoch 261 iteration 100 loss 0.31075865030288696\n",
      "Epoch 261 iteration 200 loss 0.6900796890258789\n",
      "Epoch 261 Training loss 0.5358290319979113\n",
      "Epoch 262 iteration 0 loss 0.2227669656276703\n",
      "Epoch 262 iteration 100 loss 0.446197509765625\n",
      "Epoch 262 iteration 200 loss 0.7248931527137756\n",
      "Epoch 262 Training loss 0.5391181066513012\n",
      "Epoch 263 iteration 0 loss 0.21145415306091309\n",
      "Epoch 263 iteration 100 loss 0.3772156834602356\n",
      "Epoch 263 iteration 200 loss 0.7078181505203247\n",
      "Epoch 263 Training loss 0.537327069882629\n",
      "Epoch 264 iteration 0 loss 0.20702707767486572\n",
      "Epoch 264 iteration 100 loss 0.38927045464515686\n",
      "Epoch 264 iteration 200 loss 0.6899633407592773\n",
      "Epoch 264 Training loss 0.5419280376039998\n",
      "Epoch 265 iteration 0 loss 0.2593457102775574\n",
      "Epoch 265 iteration 100 loss 0.3860691785812378\n",
      "Epoch 265 iteration 200 loss 0.7328400611877441\n",
      "Epoch 265 Training loss 0.537919484096904\n",
      "Evaluation loss 7.411533126655887\n",
      "Epoch 266 iteration 0 loss 0.24962735176086426\n",
      "Epoch 266 iteration 100 loss 0.36664220690727234\n",
      "Epoch 266 iteration 200 loss 0.6823879480361938\n",
      "Epoch 266 Training loss 0.5480203186475424\n",
      "Epoch 267 iteration 0 loss 0.2330840826034546\n",
      "Epoch 267 iteration 100 loss 0.39709237217903137\n",
      "Epoch 267 iteration 200 loss 0.6771153211593628\n",
      "Epoch 267 Training loss 0.550068768196825\n",
      "Epoch 268 iteration 0 loss 0.25781646370887756\n",
      "Epoch 268 iteration 100 loss 0.3186423182487488\n",
      "Epoch 268 iteration 200 loss 0.6990012526512146\n",
      "Epoch 268 Training loss 0.5523899920681022\n",
      "Epoch 269 iteration 0 loss 0.1896030604839325\n",
      "Epoch 269 iteration 100 loss 0.331215500831604\n",
      "Epoch 269 iteration 200 loss 0.7467136979103088\n",
      "Epoch 269 Training loss 0.552788468886996\n",
      "Epoch 270 iteration 0 loss 0.2485339194536209\n",
      "Epoch 270 iteration 100 loss 0.30370190739631653\n",
      "Epoch 270 iteration 200 loss 0.762578547000885\n",
      "Epoch 270 Training loss 0.5556132505665141\n",
      "Evaluation loss 7.401392005157393\n",
      "Epoch 271 iteration 0 loss 0.20948295295238495\n",
      "Epoch 271 iteration 100 loss 0.3995823264122009\n",
      "Epoch 271 iteration 200 loss 0.739494264125824\n",
      "Epoch 271 Training loss 0.5495877756982428\n",
      "Epoch 272 iteration 0 loss 0.25109246373176575\n",
      "Epoch 272 iteration 100 loss 0.33029085397720337\n",
      "Epoch 272 iteration 200 loss 0.7282661199569702\n",
      "Epoch 272 Training loss 0.5545331294459898\n",
      "Epoch 273 iteration 0 loss 0.22868332266807556\n",
      "Epoch 273 iteration 100 loss 0.3605923056602478\n",
      "Epoch 273 iteration 200 loss 0.7449930906295776\n",
      "Epoch 273 Training loss 0.5543742320806345\n",
      "Epoch 274 iteration 0 loss 0.20353655517101288\n",
      "Epoch 274 iteration 100 loss 0.3643152415752411\n",
      "Epoch 274 iteration 200 loss 0.7669838666915894\n",
      "Epoch 274 Training loss 0.5520532059584244\n",
      "Epoch 275 iteration 0 loss 0.20334041118621826\n",
      "Epoch 275 iteration 100 loss 0.34924373030662537\n",
      "Epoch 275 iteration 200 loss 0.7478445172309875\n",
      "Epoch 275 Training loss 0.5540705572441638\n",
      "Evaluation loss 7.422575385019327\n",
      "Epoch 276 iteration 0 loss 0.2318098098039627\n",
      "Epoch 276 iteration 100 loss 0.35983502864837646\n",
      "Epoch 276 iteration 200 loss 0.716253936290741\n",
      "Epoch 276 Training loss 0.5558316700285169\n",
      "Epoch 277 iteration 0 loss 0.22928562760353088\n",
      "Epoch 277 iteration 100 loss 0.3080432713031769\n",
      "Epoch 277 iteration 200 loss 0.7636759877204895\n",
      "Epoch 277 Training loss 0.5563592127751388\n",
      "Epoch 278 iteration 0 loss 0.19171254336833954\n",
      "Epoch 278 iteration 100 loss 0.37008410692214966\n",
      "Epoch 278 iteration 200 loss 0.727006733417511\n",
      "Epoch 278 Training loss 0.5589058474033552\n",
      "Epoch 279 iteration 0 loss 0.2226942479610443\n",
      "Epoch 279 iteration 100 loss 0.392564982175827\n",
      "Epoch 279 iteration 200 loss 0.7538716793060303\n",
      "Epoch 279 Training loss 0.5634510429213961\n",
      "Epoch 280 iteration 0 loss 0.23739150166511536\n",
      "Epoch 280 iteration 100 loss 0.402395099401474\n",
      "Epoch 280 iteration 200 loss 0.6852279901504517\n",
      "Epoch 280 Training loss 0.5553818257846932\n",
      "Evaluation loss 7.453884507482643\n",
      "Epoch 281 iteration 0 loss 0.2443816214799881\n",
      "Epoch 281 iteration 100 loss 0.3514321446418762\n",
      "Epoch 281 iteration 200 loss 0.7342166304588318\n",
      "Epoch 281 Training loss 0.5618919491115235\n",
      "Epoch 282 iteration 0 loss 0.2022915780544281\n",
      "Epoch 282 iteration 100 loss 0.36918774247169495\n",
      "Epoch 282 iteration 200 loss 0.6916894912719727\n",
      "Epoch 282 Training loss 0.5626663421992366\n",
      "Epoch 283 iteration 0 loss 0.25347796082496643\n",
      "Epoch 283 iteration 100 loss 0.35731685161590576\n",
      "Epoch 283 iteration 200 loss 0.7505543828010559\n",
      "Epoch 283 Training loss 0.551758199611517\n",
      "Epoch 284 iteration 0 loss 0.2842811644077301\n",
      "Epoch 284 iteration 100 loss 0.31576308608055115\n",
      "Epoch 284 iteration 200 loss 0.7720886468887329\n",
      "Epoch 284 Training loss 0.5599809646142841\n",
      "Epoch 285 iteration 0 loss 0.22977054119110107\n",
      "Epoch 285 iteration 100 loss 0.37764254212379456\n",
      "Epoch 285 iteration 200 loss 0.798069953918457\n",
      "Epoch 285 Training loss 0.5650124611068897\n",
      "Evaluation loss 7.429361504774456\n",
      "Epoch 286 iteration 0 loss 0.2518308162689209\n",
      "Epoch 286 iteration 100 loss 0.3738526999950409\n",
      "Epoch 286 iteration 200 loss 0.7629968523979187\n",
      "Epoch 286 Training loss 0.561460749923348\n",
      "Epoch 287 iteration 0 loss 0.25117963552474976\n",
      "Epoch 287 iteration 100 loss 0.29813843965530396\n",
      "Epoch 287 iteration 200 loss 0.7189266681671143\n",
      "Epoch 287 Training loss 0.5610295646798039\n",
      "Epoch 288 iteration 0 loss 0.20605480670928955\n",
      "Epoch 288 iteration 100 loss 0.34913673996925354\n",
      "Epoch 288 iteration 200 loss 0.720880389213562\n",
      "Epoch 288 Training loss 0.5682072592282308\n",
      "Epoch 289 iteration 0 loss 0.2705758810043335\n",
      "Epoch 289 iteration 100 loss 0.3861420452594757\n",
      "Epoch 289 iteration 200 loss 0.7278327345848083\n",
      "Epoch 289 Training loss 0.5691165692652459\n",
      "Epoch 290 iteration 0 loss 0.26926982402801514\n",
      "Epoch 290 iteration 100 loss 0.3866214156150818\n",
      "Epoch 290 iteration 200 loss 0.809861958026886\n",
      "Epoch 290 Training loss 0.5705457180686355\n",
      "Evaluation loss 7.426276337163673\n",
      "Epoch 291 iteration 0 loss 0.21241402626037598\n",
      "Epoch 291 iteration 100 loss 0.35459446907043457\n",
      "Epoch 291 iteration 200 loss 0.7530001997947693\n",
      "Epoch 291 Training loss 0.5696906391357992\n",
      "Epoch 292 iteration 0 loss 0.22960786521434784\n",
      "Epoch 292 iteration 100 loss 0.3917669951915741\n",
      "Epoch 292 iteration 200 loss 0.776039183139801\n",
      "Epoch 292 Training loss 0.5699734829740858\n",
      "Epoch 293 iteration 0 loss 0.19372491538524628\n",
      "Epoch 293 iteration 100 loss 0.4025020897388458\n",
      "Epoch 293 iteration 200 loss 0.7130774855613708\n",
      "Epoch 293 Training loss 0.5705123400084537\n",
      "Epoch 294 iteration 0 loss 0.20085668563842773\n",
      "Epoch 294 iteration 100 loss 0.389076828956604\n",
      "Epoch 294 iteration 200 loss 0.7939448356628418\n",
      "Epoch 294 Training loss 0.5685496224138497\n",
      "Epoch 295 iteration 0 loss 0.23102958500385284\n",
      "Epoch 295 iteration 100 loss 0.35384562611579895\n",
      "Epoch 295 iteration 200 loss 0.7728130221366882\n",
      "Epoch 295 Training loss 0.5689538970009069\n",
      "Evaluation loss 7.427306529693032\n",
      "Epoch 296 iteration 0 loss 0.20630061626434326\n",
      "Epoch 296 iteration 100 loss 0.3962360918521881\n",
      "Epoch 296 iteration 200 loss 0.7987095713615417\n",
      "Epoch 296 Training loss 0.5769109864600998\n",
      "Epoch 297 iteration 0 loss 0.23816165328025818\n",
      "Epoch 297 iteration 100 loss 0.3756735920906067\n",
      "Epoch 297 iteration 200 loss 0.8111305236816406\n",
      "Epoch 297 Training loss 0.5734415190542363\n",
      "Epoch 298 iteration 0 loss 0.24889805912971497\n",
      "Epoch 298 iteration 100 loss 0.3327210247516632\n",
      "Epoch 298 iteration 200 loss 0.726690411567688\n",
      "Epoch 298 Training loss 0.5791772148972381\n",
      "Epoch 299 iteration 0 loss 0.2580799460411072\n",
      "Epoch 299 iteration 100 loss 0.4142266511917114\n",
      "Epoch 299 iteration 200 loss 0.7914842367172241\n",
      "Epoch 299 Training loss 0.5789478960768198\n",
      "Epoch 300 iteration 0 loss 0.19443675875663757\n",
      "Epoch 300 iteration 100 loss 0.4491159915924072\n",
      "Epoch 300 iteration 200 loss 0.8060068488121033\n",
      "Epoch 300 Training loss 0.5797102535106923\n",
      "Evaluation loss 7.405463604081867\n",
      "Epoch 301 iteration 0 loss 0.21453151106834412\n",
      "Epoch 301 iteration 100 loss 0.41886651515960693\n",
      "Epoch 301 iteration 200 loss 0.7736693620681763\n",
      "Epoch 301 Training loss 0.5847509165358254\n",
      "Epoch 302 iteration 0 loss 0.22100360691547394\n",
      "Epoch 302 iteration 100 loss 0.4189521372318268\n",
      "Epoch 302 iteration 200 loss 0.7916209697723389\n",
      "Epoch 302 Training loss 0.5875269007065602\n",
      "Epoch 303 iteration 0 loss 0.23246915638446808\n",
      "Epoch 303 iteration 100 loss 0.4229333996772766\n",
      "Epoch 303 iteration 200 loss 0.7583633065223694\n",
      "Epoch 303 Training loss 0.5872643855042177\n",
      "Epoch 304 iteration 0 loss 0.2389732152223587\n",
      "Epoch 304 iteration 100 loss 0.4082563817501068\n",
      "Epoch 304 iteration 200 loss 0.7571390867233276\n",
      "Epoch 304 Training loss 0.5866506258158253\n",
      "Epoch 305 iteration 0 loss 0.220361590385437\n",
      "Epoch 305 iteration 100 loss 0.36817777156829834\n",
      "Epoch 305 iteration 200 loss 0.7700201869010925\n",
      "Epoch 305 Training loss 0.5911002934326982\n",
      "Evaluation loss 7.387063269274248\n",
      "Epoch 306 iteration 0 loss 0.20577804744243622\n",
      "Epoch 306 iteration 100 loss 0.3945786654949188\n",
      "Epoch 306 iteration 200 loss 0.743350088596344\n",
      "Epoch 306 Training loss 0.5915588815210389\n",
      "Epoch 307 iteration 0 loss 0.24013833701610565\n",
      "Epoch 307 iteration 100 loss 0.3881867825984955\n",
      "Epoch 307 iteration 200 loss 0.8603594303131104\n",
      "Epoch 307 Training loss 0.5937877790803477\n",
      "Epoch 308 iteration 0 loss 0.2700539827346802\n",
      "Epoch 308 iteration 100 loss 0.36685553193092346\n",
      "Epoch 308 iteration 200 loss 0.7525203824043274\n",
      "Epoch 308 Training loss 0.5887661991204065\n",
      "Epoch 309 iteration 0 loss 0.23393170535564423\n",
      "Epoch 309 iteration 100 loss 0.41556450724601746\n",
      "Epoch 309 iteration 200 loss 0.7911506295204163\n",
      "Epoch 309 Training loss 0.5980780432338998\n",
      "Epoch 310 iteration 0 loss 0.19172796607017517\n",
      "Epoch 310 iteration 100 loss 0.3966578543186188\n",
      "Epoch 310 iteration 200 loss 0.8000931739807129\n",
      "Epoch 310 Training loss 0.6022374738198057\n",
      "Evaluation loss 7.400240069114142\n",
      "Epoch 311 iteration 0 loss 0.2748311460018158\n",
      "Epoch 311 iteration 100 loss 0.4709317982196808\n",
      "Epoch 311 iteration 200 loss 0.8453623652458191\n",
      "Epoch 311 Training loss 0.597971228496479\n",
      "Epoch 312 iteration 0 loss 0.26530390977859497\n",
      "Epoch 312 iteration 100 loss 0.4336755573749542\n",
      "Epoch 312 iteration 200 loss 0.7825753688812256\n",
      "Epoch 312 Training loss 0.5980787763571388\n",
      "Epoch 313 iteration 0 loss 0.23966829478740692\n",
      "Epoch 313 iteration 100 loss 0.3263522684574127\n",
      "Epoch 313 iteration 200 loss 0.787640392780304\n",
      "Epoch 313 Training loss 0.5987537839461389\n",
      "Epoch 314 iteration 0 loss 0.28927770256996155\n",
      "Epoch 314 iteration 100 loss 0.3828861713409424\n",
      "Epoch 314 iteration 200 loss 0.787376344203949\n",
      "Epoch 314 Training loss 0.6028856244208916\n",
      "Epoch 315 iteration 0 loss 0.2702455222606659\n",
      "Epoch 315 iteration 100 loss 0.41035130620002747\n",
      "Epoch 315 iteration 200 loss 0.7785438895225525\n",
      "Epoch 315 Training loss 0.6012418549991855\n",
      "Evaluation loss 7.413995547580048\n",
      "Epoch 316 iteration 0 loss 0.268490195274353\n",
      "Epoch 316 iteration 100 loss 0.3630610406398773\n",
      "Epoch 316 iteration 200 loss 0.8172144889831543\n",
      "Epoch 316 Training loss 0.6028169203702219\n",
      "Epoch 317 iteration 0 loss 0.24676799774169922\n",
      "Epoch 317 iteration 100 loss 0.37216782569885254\n",
      "Epoch 317 iteration 200 loss 0.7429887652397156\n",
      "Epoch 317 Training loss 0.605540615853972\n",
      "Epoch 318 iteration 0 loss 0.24314658343791962\n",
      "Epoch 318 iteration 100 loss 0.4052722752094269\n",
      "Epoch 318 iteration 200 loss 0.7477560043334961\n",
      "Epoch 318 Training loss 0.6060284773129744\n",
      "Epoch 319 iteration 0 loss 0.2163391411304474\n",
      "Epoch 319 iteration 100 loss 0.36079272627830505\n",
      "Epoch 319 iteration 200 loss 0.7551504373550415\n",
      "Epoch 319 Training loss 0.6066100417276776\n",
      "Epoch 320 iteration 0 loss 0.1854066550731659\n",
      "Epoch 320 iteration 100 loss 0.37090209126472473\n",
      "Epoch 320 iteration 200 loss 0.7470594644546509\n",
      "Epoch 320 Training loss 0.60587332324758\n",
      "Evaluation loss 7.41142853136482\n",
      "Epoch 321 iteration 0 loss 0.2053389549255371\n",
      "Epoch 321 iteration 100 loss 0.3757724463939667\n",
      "Epoch 321 iteration 200 loss 0.8133304715156555\n",
      "Epoch 321 Training loss 0.607981345708629\n",
      "Epoch 322 iteration 0 loss 0.20470552146434784\n",
      "Epoch 322 iteration 100 loss 0.354543000459671\n",
      "Epoch 322 iteration 200 loss 0.7633849382400513\n",
      "Epoch 322 Training loss 0.6097254526086078\n",
      "Epoch 323 iteration 0 loss 0.2092713564634323\n",
      "Epoch 323 iteration 100 loss 0.42224550247192383\n",
      "Epoch 323 iteration 200 loss 0.7496351599693298\n",
      "Epoch 323 Training loss 0.6164818854259405\n",
      "Epoch 324 iteration 0 loss 0.2558120787143707\n",
      "Epoch 324 iteration 100 loss 0.457845002412796\n",
      "Epoch 324 iteration 200 loss 0.7362194657325745\n",
      "Epoch 324 Training loss 0.6211395109156981\n",
      "Epoch 325 iteration 0 loss 0.22182424366474152\n",
      "Epoch 325 iteration 100 loss 0.41454511880874634\n",
      "Epoch 325 iteration 200 loss 0.76688152551651\n",
      "Epoch 325 Training loss 0.6186356029744875\n",
      "Evaluation loss 7.398648186081763\n",
      "Epoch 326 iteration 0 loss 0.2018585205078125\n",
      "Epoch 326 iteration 100 loss 0.44052654504776\n",
      "Epoch 326 iteration 200 loss 0.8053054809570312\n",
      "Epoch 326 Training loss 0.6269084437335187\n",
      "Epoch 327 iteration 0 loss 0.2828591465950012\n",
      "Epoch 327 iteration 100 loss 0.3362692892551422\n",
      "Epoch 327 iteration 200 loss 0.7808456420898438\n",
      "Epoch 327 Training loss 0.6241224356120648\n",
      "Epoch 328 iteration 0 loss 0.3154439926147461\n",
      "Epoch 328 iteration 100 loss 0.43160340189933777\n",
      "Epoch 328 iteration 200 loss 0.8429328799247742\n",
      "Epoch 328 Training loss 0.630098993097279\n",
      "Epoch 329 iteration 0 loss 0.21680226922035217\n",
      "Epoch 329 iteration 100 loss 0.3957253694534302\n",
      "Epoch 329 iteration 200 loss 0.8684750199317932\n",
      "Epoch 329 Training loss 0.635836970262248\n",
      "Epoch 330 iteration 0 loss 0.23709583282470703\n",
      "Epoch 330 iteration 100 loss 0.4090089797973633\n",
      "Epoch 330 iteration 200 loss 0.7930089235305786\n",
      "Epoch 330 Training loss 0.6336388870427627\n",
      "Evaluation loss 7.416170023064656\n",
      "Epoch 331 iteration 0 loss 0.22091510891914368\n",
      "Epoch 331 iteration 100 loss 0.39332759380340576\n",
      "Epoch 331 iteration 200 loss 0.8150612115859985\n",
      "Epoch 331 Training loss 0.6379533761575716\n",
      "Epoch 332 iteration 0 loss 0.2661712169647217\n",
      "Epoch 332 iteration 100 loss 0.38957738876342773\n",
      "Epoch 332 iteration 200 loss 0.8204431533813477\n",
      "Epoch 332 Training loss 0.6410798103156551\n",
      "Epoch 333 iteration 0 loss 0.22196468710899353\n",
      "Epoch 333 iteration 100 loss 0.3925393521785736\n",
      "Epoch 333 iteration 200 loss 0.8296515345573425\n",
      "Epoch 333 Training loss 0.6437086369995378\n",
      "Epoch 334 iteration 0 loss 0.25368577241897583\n",
      "Epoch 334 iteration 100 loss 0.41547074913978577\n",
      "Epoch 334 iteration 200 loss 0.8329609036445618\n",
      "Epoch 334 Training loss 0.6451232572146829\n",
      "Epoch 335 iteration 0 loss 0.2692415714263916\n",
      "Epoch 335 iteration 100 loss 0.4465324282646179\n",
      "Epoch 335 iteration 200 loss 0.756580114364624\n",
      "Epoch 335 Training loss 0.6445940321491759\n",
      "Evaluation loss 7.419486930892335\n",
      "Epoch 336 iteration 0 loss 0.23132236301898956\n",
      "Epoch 336 iteration 100 loss 0.3988179564476013\n",
      "Epoch 336 iteration 200 loss 0.8026684522628784\n",
      "Epoch 336 Training loss 0.6457591098378807\n",
      "Epoch 337 iteration 0 loss 0.26019078493118286\n",
      "Epoch 337 iteration 100 loss 0.41575345396995544\n",
      "Epoch 337 iteration 200 loss 0.7906581163406372\n",
      "Epoch 337 Training loss 0.6552199731966151\n",
      "Epoch 338 iteration 0 loss 0.27891314029693604\n",
      "Epoch 338 iteration 100 loss 0.3742949068546295\n",
      "Epoch 338 iteration 200 loss 0.8758317828178406\n",
      "Epoch 338 Training loss 0.6498914914833706\n",
      "Epoch 339 iteration 0 loss 0.2886764705181122\n",
      "Epoch 339 iteration 100 loss 0.4161135256290436\n",
      "Epoch 339 iteration 200 loss 0.8744052052497864\n",
      "Epoch 339 Training loss 0.6628387583973424\n",
      "Epoch 340 iteration 0 loss 0.2756752073764801\n",
      "Epoch 340 iteration 100 loss 0.3724665343761444\n",
      "Epoch 340 iteration 200 loss 0.8567308187484741\n",
      "Epoch 340 Training loss 0.6569847763895226\n",
      "Evaluation loss 7.389962527153916\n",
      "Epoch 341 iteration 0 loss 0.23565275967121124\n",
      "Epoch 341 iteration 100 loss 0.3782493472099304\n",
      "Epoch 341 iteration 200 loss 0.830142080783844\n",
      "Epoch 341 Training loss 0.6652323617107981\n",
      "Epoch 342 iteration 0 loss 0.25445571541786194\n",
      "Epoch 342 iteration 100 loss 0.3943036198616028\n",
      "Epoch 342 iteration 200 loss 0.8410165905952454\n",
      "Epoch 342 Training loss 0.6620595886934435\n",
      "Epoch 343 iteration 0 loss 0.23103563487529755\n",
      "Epoch 343 iteration 100 loss 0.44959938526153564\n",
      "Epoch 343 iteration 200 loss 0.8517242074012756\n",
      "Epoch 343 Training loss 0.6701659225873471\n",
      "Epoch 344 iteration 0 loss 0.23112504184246063\n",
      "Epoch 344 iteration 100 loss 0.40540072321891785\n",
      "Epoch 344 iteration 200 loss 0.9213586449623108\n",
      "Epoch 344 Training loss 0.6725742046273137\n",
      "Epoch 345 iteration 0 loss 0.28973060846328735\n",
      "Epoch 345 iteration 100 loss 0.43418577313423157\n",
      "Epoch 345 iteration 200 loss 0.9390472769737244\n",
      "Epoch 345 Training loss 0.6741800598128367\n",
      "Evaluation loss 7.4311602921136775\n",
      "Epoch 346 iteration 0 loss 0.28046196699142456\n",
      "Epoch 346 iteration 100 loss 0.4092472493648529\n",
      "Epoch 346 iteration 200 loss 0.9069125056266785\n",
      "Epoch 346 Training loss 0.6704360817372701\n",
      "Epoch 347 iteration 0 loss 0.253708153963089\n",
      "Epoch 347 iteration 100 loss 0.40432363748550415\n",
      "Epoch 347 iteration 200 loss 0.9243804216384888\n",
      "Epoch 347 Training loss 0.6698523543443351\n",
      "Epoch 348 iteration 0 loss 0.21632222831249237\n",
      "Epoch 348 iteration 100 loss 0.41738882660865784\n",
      "Epoch 348 iteration 200 loss 0.9413402080535889\n",
      "Epoch 348 Training loss 0.6749583086831945\n",
      "Epoch 349 iteration 0 loss 0.26628577709198\n",
      "Epoch 349 iteration 100 loss 0.4640622138977051\n",
      "Epoch 349 iteration 200 loss 0.9144889712333679\n",
      "Epoch 349 Training loss 0.6862186447178761\n",
      "Epoch 350 iteration 0 loss 0.2616853415966034\n",
      "Epoch 350 iteration 100 loss 0.4397009313106537\n",
      "Epoch 350 iteration 200 loss 0.8758667707443237\n",
      "Epoch 350 Training loss 0.6829915931754884\n",
      "Evaluation loss 7.427989231106338\n",
      "Epoch 351 iteration 0 loss 0.19746503233909607\n",
      "Epoch 351 iteration 100 loss 0.40234851837158203\n",
      "Epoch 351 iteration 200 loss 0.9474198818206787\n",
      "Epoch 351 Training loss 0.6831398884932837\n",
      "Epoch 352 iteration 0 loss 0.28559139370918274\n",
      "Epoch 352 iteration 100 loss 0.43293723464012146\n",
      "Epoch 352 iteration 200 loss 0.9364929795265198\n",
      "Epoch 352 Training loss 0.6840173161595182\n",
      "Epoch 353 iteration 0 loss 0.2505652606487274\n",
      "Epoch 353 iteration 100 loss 0.4280507564544678\n",
      "Epoch 353 iteration 200 loss 0.9160080552101135\n",
      "Epoch 353 Training loss 0.6845274635934773\n",
      "Epoch 354 iteration 0 loss 0.25318068265914917\n",
      "Epoch 354 iteration 100 loss 0.3416517674922943\n",
      "Epoch 354 iteration 200 loss 0.8694863319396973\n",
      "Epoch 354 Training loss 0.6863245359782504\n",
      "Epoch 355 iteration 0 loss 0.22464247047901154\n",
      "Epoch 355 iteration 100 loss 0.35887810587882996\n",
      "Epoch 355 iteration 200 loss 0.9183094501495361\n",
      "Epoch 355 Training loss 0.6921228485588482\n",
      "Evaluation loss 7.442529866034665\n",
      "Epoch 356 iteration 0 loss 0.24705645442008972\n",
      "Epoch 356 iteration 100 loss 0.3769404888153076\n",
      "Epoch 356 iteration 200 loss 0.9283499121665955\n",
      "Epoch 356 Training loss 0.6996908256142368\n",
      "Epoch 357 iteration 0 loss 0.28335222601890564\n",
      "Epoch 357 iteration 100 loss 0.37960687279701233\n",
      "Epoch 357 iteration 200 loss 0.8946647047996521\n",
      "Epoch 357 Training loss 0.6983140427946116\n",
      "Epoch 358 iteration 0 loss 0.21807105839252472\n",
      "Epoch 358 iteration 100 loss 0.4028959274291992\n",
      "Epoch 358 iteration 200 loss 0.8411679267883301\n",
      "Epoch 358 Training loss 0.6970401547688473\n",
      "Epoch 359 iteration 0 loss 0.29595130681991577\n",
      "Epoch 359 iteration 100 loss 0.41584375500679016\n",
      "Epoch 359 iteration 200 loss 0.843212366104126\n",
      "Epoch 359 Training loss 0.7052565989976262\n",
      "Epoch 360 iteration 0 loss 0.20209531486034393\n",
      "Epoch 360 iteration 100 loss 0.36612528562545776\n",
      "Epoch 360 iteration 200 loss 0.9100587368011475\n",
      "Epoch 360 Training loss 0.7022865122085417\n",
      "Evaluation loss 7.4295622982163545\n",
      "Epoch 361 iteration 0 loss 0.2619011402130127\n",
      "Epoch 361 iteration 100 loss 0.3388732969760895\n",
      "Epoch 361 iteration 200 loss 0.8436185121536255\n",
      "Epoch 361 Training loss 0.7082837749384882\n",
      "Epoch 362 iteration 0 loss 0.23152604699134827\n",
      "Epoch 362 iteration 100 loss 0.3879922032356262\n",
      "Epoch 362 iteration 200 loss 0.8870170712471008\n",
      "Epoch 362 Training loss 0.7097219103436622\n",
      "Epoch 363 iteration 0 loss 0.3105522692203522\n",
      "Epoch 363 iteration 100 loss 0.4229795038700104\n",
      "Epoch 363 iteration 200 loss 0.8363386988639832\n",
      "Epoch 363 Training loss 0.716446788948818\n",
      "Epoch 364 iteration 0 loss 0.32090410590171814\n",
      "Epoch 364 iteration 100 loss 0.37119922041893005\n",
      "Epoch 364 iteration 200 loss 0.8623780608177185\n",
      "Epoch 364 Training loss 0.7132988265987661\n",
      "Epoch 365 iteration 0 loss 0.2603238821029663\n",
      "Epoch 365 iteration 100 loss 0.38608455657958984\n",
      "Epoch 365 iteration 200 loss 0.8868331909179688\n",
      "Epoch 365 Training loss 0.7236022519283177\n",
      "Evaluation loss 7.421239071394674\n",
      "Epoch 366 iteration 0 loss 0.22726857662200928\n",
      "Epoch 366 iteration 100 loss 0.4049031436443329\n",
      "Epoch 366 iteration 200 loss 0.8506839871406555\n",
      "Epoch 366 Training loss 0.7141061917546937\n",
      "Epoch 367 iteration 0 loss 0.2593304514884949\n",
      "Epoch 367 iteration 100 loss 0.4222058355808258\n",
      "Epoch 367 iteration 200 loss 0.9002829194068909\n",
      "Epoch 367 Training loss 0.7200837912501531\n",
      "Epoch 368 iteration 0 loss 0.25035110116004944\n",
      "Epoch 368 iteration 100 loss 0.37841248512268066\n",
      "Epoch 368 iteration 200 loss 0.7931768298149109\n",
      "Epoch 368 Training loss 0.7236352154008514\n",
      "Epoch 369 iteration 0 loss 0.2256670445203781\n",
      "Epoch 369 iteration 100 loss 0.42114731669425964\n",
      "Epoch 369 iteration 200 loss 0.9919662475585938\n",
      "Epoch 369 Training loss 0.7311925416473731\n",
      "Epoch 370 iteration 0 loss 0.22007986903190613\n",
      "Epoch 370 iteration 100 loss 0.42384880781173706\n",
      "Epoch 370 iteration 200 loss 0.8957613706588745\n",
      "Epoch 370 Training loss 0.7290314912938869\n",
      "Evaluation loss 7.420449765076107\n",
      "Epoch 371 iteration 0 loss 0.2794039845466614\n",
      "Epoch 371 iteration 100 loss 0.4087482690811157\n",
      "Epoch 371 iteration 200 loss 0.8987371325492859\n",
      "Epoch 371 Training loss 0.7363924858950798\n",
      "Epoch 372 iteration 0 loss 0.19884145259857178\n",
      "Epoch 372 iteration 100 loss 0.4062833786010742\n",
      "Epoch 372 iteration 200 loss 0.8597664833068848\n",
      "Epoch 372 Training loss 0.7430681007292169\n",
      "Epoch 373 iteration 0 loss 0.2470061331987381\n",
      "Epoch 373 iteration 100 loss 0.4454883337020874\n",
      "Epoch 373 iteration 200 loss 1.0032551288604736\n",
      "Epoch 373 Training loss 0.7441261643215509\n",
      "Epoch 374 iteration 0 loss 0.27593594789505005\n",
      "Epoch 374 iteration 100 loss 0.40086978673934937\n",
      "Epoch 374 iteration 200 loss 0.9040428400039673\n",
      "Epoch 374 Training loss 0.743526669334734\n",
      "Epoch 375 iteration 0 loss 0.2668330669403076\n",
      "Epoch 375 iteration 100 loss 0.4077881872653961\n",
      "Epoch 375 iteration 200 loss 0.8993251323699951\n",
      "Epoch 375 Training loss 0.7511841696937417\n",
      "Evaluation loss 7.418671707630954\n",
      "Epoch 376 iteration 0 loss 0.2807442247867584\n",
      "Epoch 376 iteration 100 loss 0.42067888379096985\n",
      "Epoch 376 iteration 200 loss 0.8586273193359375\n",
      "Epoch 376 Training loss 0.7531711990564596\n",
      "Epoch 377 iteration 0 loss 0.26280105113983154\n",
      "Epoch 377 iteration 100 loss 0.4095653295516968\n",
      "Epoch 377 iteration 200 loss 0.8796354532241821\n",
      "Epoch 377 Training loss 0.7591190461978614\n",
      "Epoch 378 iteration 0 loss 0.3034089505672455\n",
      "Epoch 378 iteration 100 loss 0.36638230085372925\n",
      "Epoch 378 iteration 200 loss 0.8945508003234863\n",
      "Epoch 378 Training loss 0.7600850291653856\n",
      "Epoch 379 iteration 0 loss 0.32146868109703064\n",
      "Epoch 379 iteration 100 loss 0.44844162464141846\n",
      "Epoch 379 iteration 200 loss 0.9463353753089905\n",
      "Epoch 379 Training loss 0.7561675032914512\n",
      "Epoch 380 iteration 0 loss 0.2286073863506317\n",
      "Epoch 380 iteration 100 loss 0.4071010947227478\n",
      "Epoch 380 iteration 200 loss 0.9191896319389343\n",
      "Epoch 380 Training loss 0.767880275794089\n",
      "Evaluation loss 7.389932467062476\n",
      "Epoch 381 iteration 0 loss 0.23648273944854736\n",
      "Epoch 381 iteration 100 loss 0.485985666513443\n",
      "Epoch 381 iteration 200 loss 0.8796824812889099\n",
      "Epoch 381 Training loss 0.768173976460616\n",
      "Epoch 382 iteration 0 loss 0.24675758183002472\n",
      "Epoch 382 iteration 100 loss 0.4027691185474396\n",
      "Epoch 382 iteration 200 loss 0.8941566348075867\n",
      "Epoch 382 Training loss 0.7680626018579648\n",
      "Epoch 383 iteration 0 loss 0.24118638038635254\n",
      "Epoch 383 iteration 100 loss 0.3680521249771118\n",
      "Epoch 383 iteration 200 loss 0.8843101859092712\n",
      "Epoch 383 Training loss 0.7749183369296587\n",
      "Epoch 384 iteration 0 loss 0.2602836489677429\n",
      "Epoch 384 iteration 100 loss 0.38373494148254395\n",
      "Epoch 384 iteration 200 loss 0.882405698299408\n",
      "Epoch 384 Training loss 0.7718200889866434\n",
      "Epoch 385 iteration 0 loss 0.27894696593284607\n",
      "Epoch 385 iteration 100 loss 0.4384981393814087\n",
      "Epoch 385 iteration 200 loss 0.9559818506240845\n",
      "Epoch 385 Training loss 0.7785388666560694\n",
      "Evaluation loss 7.451447174881221\n",
      "Epoch 386 iteration 0 loss 0.31705185770988464\n",
      "Epoch 386 iteration 100 loss 0.3812708854675293\n",
      "Epoch 386 iteration 200 loss 0.9657270312309265\n",
      "Epoch 386 Training loss 0.7844445198724537\n",
      "Epoch 387 iteration 0 loss 0.23102626204490662\n",
      "Epoch 387 iteration 100 loss 0.4304094910621643\n",
      "Epoch 387 iteration 200 loss 0.933062732219696\n",
      "Epoch 387 Training loss 0.7854105041578072\n",
      "Epoch 388 iteration 0 loss 0.25843197107315063\n",
      "Epoch 388 iteration 100 loss 0.44434407353401184\n",
      "Epoch 388 iteration 200 loss 0.9238149523735046\n",
      "Epoch 388 Training loss 0.7898757751040074\n",
      "Epoch 389 iteration 0 loss 0.3036556541919708\n",
      "Epoch 389 iteration 100 loss 0.36519670486450195\n",
      "Epoch 389 iteration 200 loss 1.0120620727539062\n",
      "Epoch 389 Training loss 0.7925759189150624\n",
      "Epoch 390 iteration 0 loss 0.22879643738269806\n",
      "Epoch 390 iteration 100 loss 0.39676013588905334\n",
      "Epoch 390 iteration 200 loss 1.1166038513183594\n",
      "Epoch 390 Training loss 0.7985971292637313\n",
      "Evaluation loss 7.433877808069439\n",
      "Epoch 391 iteration 0 loss 0.24601730704307556\n",
      "Epoch 391 iteration 100 loss 0.44125595688819885\n",
      "Epoch 391 iteration 200 loss 1.0524417161941528\n",
      "Epoch 391 Training loss 0.8061075480115836\n",
      "Epoch 392 iteration 0 loss 0.29251641035079956\n",
      "Epoch 392 iteration 100 loss 0.45784300565719604\n",
      "Epoch 392 iteration 200 loss 0.9757854342460632\n",
      "Epoch 392 Training loss 0.8047580439733255\n",
      "Epoch 393 iteration 0 loss 0.2930717170238495\n",
      "Epoch 393 iteration 100 loss 0.4019194543361664\n",
      "Epoch 393 iteration 200 loss 0.9991739988327026\n",
      "Epoch 393 Training loss 0.8071263060366879\n",
      "Epoch 394 iteration 0 loss 0.23626241087913513\n",
      "Epoch 394 iteration 100 loss 0.4283820390701294\n",
      "Epoch 394 iteration 200 loss 1.0176161527633667\n",
      "Epoch 394 Training loss 0.8151870175603407\n",
      "Epoch 395 iteration 0 loss 0.267459899187088\n",
      "Epoch 395 iteration 100 loss 0.47015276551246643\n",
      "Epoch 395 iteration 200 loss 0.9612156748771667\n",
      "Epoch 395 Training loss 0.8138988757752189\n",
      "Evaluation loss 7.451360913451695\n",
      "Epoch 396 iteration 0 loss 0.2877465784549713\n",
      "Epoch 396 iteration 100 loss 0.3905336260795593\n",
      "Epoch 396 iteration 200 loss 1.0300716161727905\n",
      "Epoch 396 Training loss 0.8220907999556168\n",
      "Epoch 397 iteration 0 loss 0.21879032254219055\n",
      "Epoch 397 iteration 100 loss 0.34313613176345825\n",
      "Epoch 397 iteration 200 loss 1.0150542259216309\n",
      "Epoch 397 Training loss 0.8213464643573353\n",
      "Epoch 398 iteration 0 loss 0.20483922958374023\n",
      "Epoch 398 iteration 100 loss 0.39332661032676697\n",
      "Epoch 398 iteration 200 loss 1.004961371421814\n",
      "Epoch 398 Training loss 0.8313252234795044\n",
      "Epoch 399 iteration 0 loss 0.2809232771396637\n",
      "Epoch 399 iteration 100 loss 0.3564676344394684\n",
      "Epoch 399 iteration 200 loss 1.0437487363815308\n",
      "Epoch 399 Training loss 0.8336726030942142\n",
      "Epoch 400 iteration 0 loss 0.2958207130432129\n",
      "Epoch 400 iteration 100 loss 0.3870980739593506\n",
      "Epoch 400 iteration 200 loss 1.029860496520996\n",
      "Epoch 400 Training loss 0.8373657852423065\n",
      "Evaluation loss 7.427122906466423\n",
      "Epoch 401 iteration 0 loss 0.25979083776474\n",
      "Epoch 401 iteration 100 loss 0.41501060128211975\n",
      "Epoch 401 iteration 200 loss 1.0554808378219604\n",
      "Epoch 401 Training loss 0.8385999870465978\n",
      "Epoch 402 iteration 0 loss 0.22500689327716827\n",
      "Epoch 402 iteration 100 loss 0.436693400144577\n",
      "Epoch 402 iteration 200 loss 0.9597715735435486\n",
      "Epoch 402 Training loss 0.8493380654802035\n",
      "Epoch 403 iteration 0 loss 0.23027470707893372\n",
      "Epoch 403 iteration 100 loss 0.4757457375526428\n",
      "Epoch 403 iteration 200 loss 0.9865633845329285\n",
      "Epoch 403 Training loss 0.8481764937385621\n",
      "Epoch 404 iteration 0 loss 0.33274126052856445\n",
      "Epoch 404 iteration 100 loss 0.38871797919273376\n",
      "Epoch 404 iteration 200 loss 0.9476764798164368\n",
      "Epoch 404 Training loss 0.8495218938447099\n",
      "Epoch 405 iteration 0 loss 0.2948538064956665\n",
      "Epoch 405 iteration 100 loss 0.4405432641506195\n",
      "Epoch 405 iteration 200 loss 1.0430141687393188\n",
      "Epoch 405 Training loss 0.8548149359168301\n",
      "Evaluation loss 7.43612993218931\n",
      "Epoch 406 iteration 0 loss 0.2832719683647156\n",
      "Epoch 406 iteration 100 loss 0.35728833079338074\n",
      "Epoch 406 iteration 200 loss 1.0418680906295776\n",
      "Epoch 406 Training loss 0.8598635895521451\n",
      "Epoch 407 iteration 0 loss 0.2520550787448883\n",
      "Epoch 407 iteration 100 loss 0.37132319808006287\n",
      "Epoch 407 iteration 200 loss 1.038379192352295\n",
      "Epoch 407 Training loss 0.860767087912176\n",
      "Epoch 408 iteration 0 loss 0.23897381126880646\n",
      "Epoch 408 iteration 100 loss 0.40046703815460205\n",
      "Epoch 408 iteration 200 loss 1.1488428115844727\n",
      "Epoch 408 Training loss 0.8615777170526108\n",
      "Epoch 409 iteration 0 loss 0.24298878014087677\n",
      "Epoch 409 iteration 100 loss 0.41228780150413513\n",
      "Epoch 409 iteration 200 loss 1.1269630193710327\n",
      "Epoch 409 Training loss 0.8756985509306942\n",
      "Epoch 410 iteration 0 loss 0.25152868032455444\n",
      "Epoch 410 iteration 100 loss 0.3866060674190521\n",
      "Epoch 410 iteration 200 loss 1.101914405822754\n",
      "Epoch 410 Training loss 0.873127290161931\n",
      "Evaluation loss 7.417278090197928\n",
      "Epoch 411 iteration 0 loss 0.30501025915145874\n",
      "Epoch 411 iteration 100 loss 0.36866316199302673\n",
      "Epoch 411 iteration 200 loss 1.021310567855835\n",
      "Epoch 411 Training loss 0.882539690176573\n",
      "Epoch 412 iteration 0 loss 0.21884648501873016\n",
      "Epoch 412 iteration 100 loss 0.3526495099067688\n",
      "Epoch 412 iteration 200 loss 1.1435874700546265\n",
      "Epoch 412 Training loss 0.889255476923952\n",
      "Epoch 413 iteration 0 loss 0.297690212726593\n",
      "Epoch 413 iteration 100 loss 0.3880109488964081\n",
      "Epoch 413 iteration 200 loss 1.0865172147750854\n",
      "Epoch 413 Training loss 0.8930972196081968\n",
      "Epoch 414 iteration 0 loss 0.2664770483970642\n",
      "Epoch 414 iteration 100 loss 0.3963351845741272\n",
      "Epoch 414 iteration 200 loss 1.0639344453811646\n",
      "Epoch 414 Training loss 0.8845524085753145\n",
      "Epoch 415 iteration 0 loss 0.2623637616634369\n",
      "Epoch 415 iteration 100 loss 0.4200061857700348\n",
      "Epoch 415 iteration 200 loss 1.1194074153900146\n",
      "Epoch 415 Training loss 0.897355699787925\n",
      "Evaluation loss 7.420644348946345\n",
      "Epoch 416 iteration 0 loss 0.24418675899505615\n",
      "Epoch 416 iteration 100 loss 0.38273870944976807\n",
      "Epoch 416 iteration 200 loss 1.185031771659851\n",
      "Epoch 416 Training loss 0.8936887091175971\n",
      "Epoch 417 iteration 0 loss 0.26834264397621155\n",
      "Epoch 417 iteration 100 loss 0.39716824889183044\n",
      "Epoch 417 iteration 200 loss 1.1510727405548096\n",
      "Epoch 417 Training loss 0.9044870603640167\n",
      "Epoch 418 iteration 0 loss 0.31612956523895264\n",
      "Epoch 418 iteration 100 loss 0.37510058283805847\n",
      "Epoch 418 iteration 200 loss 1.0865925550460815\n",
      "Epoch 418 Training loss 0.9032808393083038\n",
      "Epoch 419 iteration 0 loss 0.24990275502204895\n",
      "Epoch 419 iteration 100 loss 0.40360715985298157\n",
      "Epoch 419 iteration 200 loss 1.129757046699524\n",
      "Epoch 419 Training loss 0.9117394912899404\n",
      "Epoch 420 iteration 0 loss 0.2968049645423889\n",
      "Epoch 420 iteration 100 loss 0.4187406003475189\n",
      "Epoch 420 iteration 200 loss 1.1431583166122437\n",
      "Epoch 420 Training loss 0.9182815401934649\n",
      "Evaluation loss 7.395933950127712\n",
      "Epoch 421 iteration 0 loss 0.23404452204704285\n",
      "Epoch 421 iteration 100 loss 0.4149807095527649\n",
      "Epoch 421 iteration 200 loss 1.0982304811477661\n",
      "Epoch 421 Training loss 0.918016989465801\n",
      "Epoch 422 iteration 0 loss 0.28980812430381775\n",
      "Epoch 422 iteration 100 loss 0.44139036536216736\n",
      "Epoch 422 iteration 200 loss 1.1650233268737793\n",
      "Epoch 422 Training loss 0.915618985425773\n",
      "Epoch 423 iteration 0 loss 0.2667461931705475\n",
      "Epoch 423 iteration 100 loss 0.4010431170463562\n",
      "Epoch 423 iteration 200 loss 1.1386659145355225\n",
      "Epoch 423 Training loss 0.9200665936409996\n",
      "Epoch 424 iteration 0 loss 0.27568912506103516\n",
      "Epoch 424 iteration 100 loss 0.4246140122413635\n",
      "Epoch 424 iteration 200 loss 1.1289104223251343\n",
      "Epoch 424 Training loss 0.9266996746498686\n",
      "Epoch 425 iteration 0 loss 0.2560221254825592\n",
      "Epoch 425 iteration 100 loss 0.36982351541519165\n",
      "Epoch 425 iteration 200 loss 1.1424052715301514\n",
      "Epoch 425 Training loss 0.9225342998180005\n",
      "Evaluation loss 7.425866827865618\n",
      "Epoch 426 iteration 0 loss 0.2472715824842453\n",
      "Epoch 426 iteration 100 loss 0.4244694709777832\n",
      "Epoch 426 iteration 200 loss 1.1268017292022705\n",
      "Epoch 426 Training loss 0.9302694765052988\n",
      "Epoch 427 iteration 0 loss 0.26009175181388855\n",
      "Epoch 427 iteration 100 loss 0.44806960225105286\n",
      "Epoch 427 iteration 200 loss 1.1876171827316284\n",
      "Epoch 427 Training loss 0.9382334717802953\n",
      "Epoch 428 iteration 0 loss 0.2140360176563263\n",
      "Epoch 428 iteration 100 loss 0.43966782093048096\n",
      "Epoch 428 iteration 200 loss 1.1724929809570312\n",
      "Epoch 428 Training loss 0.9391798533483023\n",
      "Epoch 429 iteration 0 loss 0.24565982818603516\n",
      "Epoch 429 iteration 100 loss 0.3824649751186371\n",
      "Epoch 429 iteration 200 loss 1.2506321668624878\n",
      "Epoch 429 Training loss 0.9371792933513114\n",
      "Epoch 430 iteration 0 loss 0.23881301283836365\n",
      "Epoch 430 iteration 100 loss 0.41039609909057617\n",
      "Epoch 430 iteration 200 loss 1.2276288270950317\n",
      "Epoch 430 Training loss 0.9480929625873799\n",
      "Evaluation loss 7.4230689167982415\n",
      "Epoch 431 iteration 0 loss 0.24065551161766052\n",
      "Epoch 431 iteration 100 loss 0.44842395186424255\n",
      "Epoch 431 iteration 200 loss 1.1492785215377808\n",
      "Epoch 431 Training loss 0.9475565363396855\n",
      "Epoch 432 iteration 0 loss 0.28027600049972534\n",
      "Epoch 432 iteration 100 loss 0.42188116908073425\n",
      "Epoch 432 iteration 200 loss 1.1857186555862427\n",
      "Epoch 432 Training loss 0.9557953710086176\n",
      "Epoch 433 iteration 0 loss 0.24234285950660706\n",
      "Epoch 433 iteration 100 loss 0.4502912759780884\n",
      "Epoch 433 iteration 200 loss 1.1646257638931274\n",
      "Epoch 433 Training loss 0.949684477867954\n",
      "Epoch 434 iteration 0 loss 0.2483358085155487\n",
      "Epoch 434 iteration 100 loss 0.37476110458374023\n",
      "Epoch 434 iteration 200 loss 1.0891960859298706\n",
      "Epoch 434 Training loss 0.9628081974388872\n",
      "Epoch 435 iteration 0 loss 0.3065561354160309\n",
      "Epoch 435 iteration 100 loss 0.4293701648712158\n",
      "Epoch 435 iteration 200 loss 1.0860737562179565\n",
      "Epoch 435 Training loss 0.9586025929129776\n",
      "Evaluation loss 7.460579340258741\n",
      "Epoch 436 iteration 0 loss 0.2643696963787079\n",
      "Epoch 436 iteration 100 loss 0.3990425169467926\n",
      "Epoch 436 iteration 200 loss 1.1668795347213745\n",
      "Epoch 436 Training loss 0.9671500348865715\n",
      "Epoch 437 iteration 0 loss 0.20126953721046448\n",
      "Epoch 437 iteration 100 loss 0.3871302306652069\n",
      "Epoch 437 iteration 200 loss 1.2393982410430908\n",
      "Epoch 437 Training loss 0.9671577953409125\n",
      "Epoch 438 iteration 0 loss 0.237871453166008\n",
      "Epoch 438 iteration 100 loss 0.44074156880378723\n",
      "Epoch 438 iteration 200 loss 1.298891544342041\n",
      "Epoch 438 Training loss 0.9782756320536379\n",
      "Epoch 439 iteration 0 loss 0.30999404191970825\n",
      "Epoch 439 iteration 100 loss 0.4064108729362488\n",
      "Epoch 439 iteration 200 loss 1.3132927417755127\n",
      "Epoch 439 Training loss 0.9768735319832879\n",
      "Epoch 440 iteration 0 loss 0.23455993831157684\n",
      "Epoch 440 iteration 100 loss 0.5013368725776672\n",
      "Epoch 440 iteration 200 loss 1.2766185998916626\n",
      "Epoch 440 Training loss 0.9914037346506372\n",
      "Evaluation loss 7.422622126497395\n",
      "Epoch 441 iteration 0 loss 0.31021520495414734\n",
      "Epoch 441 iteration 100 loss 0.38516396284103394\n",
      "Epoch 441 iteration 200 loss 1.3518856763839722\n",
      "Epoch 441 Training loss 0.9979356611424824\n",
      "Epoch 442 iteration 0 loss 0.29509255290031433\n",
      "Epoch 442 iteration 100 loss 0.44317808747291565\n",
      "Epoch 442 iteration 200 loss 1.3570975065231323\n",
      "Epoch 442 Training loss 1.0046294017757653\n",
      "Epoch 443 iteration 0 loss 0.3434092402458191\n",
      "Epoch 443 iteration 100 loss 0.4000782370567322\n",
      "Epoch 443 iteration 200 loss 1.3036160469055176\n",
      "Epoch 443 Training loss 1.0102541723818361\n",
      "Epoch 444 iteration 0 loss 0.32148346304893494\n",
      "Epoch 444 iteration 100 loss 0.3820677697658539\n",
      "Epoch 444 iteration 200 loss 1.3011575937271118\n",
      "Epoch 444 Training loss 1.0120418200675732\n",
      "Epoch 445 iteration 0 loss 0.2746901214122772\n",
      "Epoch 445 iteration 100 loss 0.48194849491119385\n",
      "Epoch 445 iteration 200 loss 1.3569246530532837\n",
      "Epoch 445 Training loss 1.0105517932300974\n",
      "Evaluation loss 7.421198721447858\n",
      "Epoch 446 iteration 0 loss 0.2176731377840042\n",
      "Epoch 446 iteration 100 loss 0.43583428859710693\n",
      "Epoch 446 iteration 200 loss 1.389495849609375\n",
      "Epoch 446 Training loss 1.0207568114015542\n",
      "Epoch 447 iteration 0 loss 0.32741624116897583\n",
      "Epoch 447 iteration 100 loss 0.4219188392162323\n",
      "Epoch 447 iteration 200 loss 1.4855952262878418\n",
      "Epoch 447 Training loss 1.0161673184114097\n",
      "Epoch 448 iteration 0 loss 0.26029467582702637\n",
      "Epoch 448 iteration 100 loss 0.4179474115371704\n",
      "Epoch 448 iteration 200 loss 1.418988585472107\n",
      "Epoch 448 Training loss 1.0271280478918796\n",
      "Epoch 449 iteration 0 loss 0.2298644483089447\n",
      "Epoch 449 iteration 100 loss 0.4139656126499176\n",
      "Epoch 449 iteration 200 loss 1.4077370166778564\n",
      "Epoch 449 Training loss 1.0335498003723589\n",
      "Epoch 450 iteration 0 loss 0.249114990234375\n",
      "Epoch 450 iteration 100 loss 0.43248674273490906\n",
      "Epoch 450 iteration 200 loss 1.3987656831741333\n",
      "Epoch 450 Training loss 1.0322271673290606\n",
      "Evaluation loss 7.400096466695346\n",
      "Epoch 451 iteration 0 loss 0.2771007716655731\n",
      "Epoch 451 iteration 100 loss 0.4181763827800751\n",
      "Epoch 451 iteration 200 loss 1.3856161832809448\n",
      "Epoch 451 Training loss 1.0344391456467725\n",
      "Epoch 452 iteration 0 loss 0.26469260454177856\n",
      "Epoch 452 iteration 100 loss 0.42735347151756287\n",
      "Epoch 452 iteration 200 loss 1.4305083751678467\n",
      "Epoch 452 Training loss 1.0457463774866715\n",
      "Epoch 453 iteration 0 loss 0.2951590418815613\n",
      "Epoch 453 iteration 100 loss 0.381995290517807\n",
      "Epoch 453 iteration 200 loss 1.5219552516937256\n",
      "Epoch 453 Training loss 1.0477584163008886\n",
      "Epoch 454 iteration 0 loss 0.24471914768218994\n",
      "Epoch 454 iteration 100 loss 0.3764948546886444\n",
      "Epoch 454 iteration 200 loss 1.4296321868896484\n",
      "Epoch 454 Training loss 1.0535982314970256\n",
      "Epoch 455 iteration 0 loss 0.33851200342178345\n",
      "Epoch 455 iteration 100 loss 0.3570183217525482\n",
      "Epoch 455 iteration 200 loss 1.4605114459991455\n",
      "Epoch 455 Training loss 1.0597032073942292\n",
      "Evaluation loss 7.4144630209670845\n",
      "Epoch 456 iteration 0 loss 0.273567259311676\n",
      "Epoch 456 iteration 100 loss 0.4382006824016571\n",
      "Epoch 456 iteration 200 loss 1.4533056020736694\n",
      "Epoch 456 Training loss 1.0608018686544618\n",
      "Epoch 457 iteration 0 loss 0.258344829082489\n",
      "Epoch 457 iteration 100 loss 0.42086347937583923\n",
      "Epoch 457 iteration 200 loss 1.5512887239456177\n",
      "Epoch 457 Training loss 1.0671170896047186\n",
      "Epoch 458 iteration 0 loss 0.23522254824638367\n",
      "Epoch 458 iteration 100 loss 0.4028102457523346\n",
      "Epoch 458 iteration 200 loss 1.4514384269714355\n",
      "Epoch 458 Training loss 1.0694531079810332\n",
      "Epoch 459 iteration 0 loss 0.267270565032959\n",
      "Epoch 459 iteration 100 loss 0.37627914547920227\n",
      "Epoch 459 iteration 200 loss 1.4655297994613647\n",
      "Epoch 459 Training loss 1.0761439225535532\n",
      "Epoch 460 iteration 0 loss 0.2283555269241333\n",
      "Epoch 460 iteration 100 loss 0.41557517647743225\n",
      "Epoch 460 iteration 200 loss 1.5175957679748535\n",
      "Epoch 460 Training loss 1.07935222572842\n",
      "Evaluation loss 7.433910187925518\n",
      "Epoch 461 iteration 0 loss 0.2729656398296356\n",
      "Epoch 461 iteration 100 loss 0.4341588020324707\n",
      "Epoch 461 iteration 200 loss 1.55948007106781\n",
      "Epoch 461 Training loss 1.0871026834267628\n",
      "Epoch 462 iteration 0 loss 0.29391711950302124\n",
      "Epoch 462 iteration 100 loss 0.39944586157798767\n",
      "Epoch 462 iteration 200 loss 1.5141359567642212\n",
      "Epoch 462 Training loss 1.0873220582123795\n",
      "Epoch 463 iteration 0 loss 0.3075242042541504\n",
      "Epoch 463 iteration 100 loss 0.4085415005683899\n",
      "Epoch 463 iteration 200 loss 1.6224290132522583\n",
      "Epoch 463 Training loss 1.0991383145281968\n",
      "Epoch 464 iteration 0 loss 0.2714817225933075\n",
      "Epoch 464 iteration 100 loss 0.36397281289100647\n",
      "Epoch 464 iteration 200 loss 1.657557725906372\n",
      "Epoch 464 Training loss 1.1013878242602653\n",
      "Epoch 465 iteration 0 loss 0.25847163796424866\n",
      "Epoch 465 iteration 100 loss 0.4524768888950348\n",
      "Epoch 465 iteration 200 loss 1.634755253791809\n",
      "Epoch 465 Training loss 1.1113702371967795\n",
      "Evaluation loss 7.394537622578888\n",
      "Epoch 466 iteration 0 loss 0.24346449971199036\n",
      "Epoch 466 iteration 100 loss 0.3491629660129547\n",
      "Epoch 466 iteration 200 loss 1.5770035982131958\n",
      "Epoch 466 Training loss 1.1075578881534005\n",
      "Epoch 467 iteration 0 loss 0.32282760739326477\n",
      "Epoch 467 iteration 100 loss 0.3817027807235718\n",
      "Epoch 467 iteration 200 loss 1.591315746307373\n",
      "Epoch 467 Training loss 1.1170863341181092\n",
      "Epoch 468 iteration 0 loss 0.2893231213092804\n",
      "Epoch 468 iteration 100 loss 0.4180790185928345\n",
      "Epoch 468 iteration 200 loss 1.6531671285629272\n",
      "Epoch 468 Training loss 1.1170644644531773\n",
      "Epoch 469 iteration 0 loss 0.2881568670272827\n",
      "Epoch 469 iteration 100 loss 0.42179858684539795\n",
      "Epoch 469 iteration 200 loss 1.6124327182769775\n",
      "Epoch 469 Training loss 1.1209956816619764\n",
      "Epoch 470 iteration 0 loss 0.22167804837226868\n",
      "Epoch 470 iteration 100 loss 0.4324767589569092\n",
      "Epoch 470 iteration 200 loss 1.6762967109680176\n",
      "Epoch 470 Training loss 1.1199356470347552\n",
      "Evaluation loss 7.399273347186275\n",
      "Epoch 471 iteration 0 loss 0.261279821395874\n",
      "Epoch 471 iteration 100 loss 0.4180992543697357\n",
      "Epoch 471 iteration 200 loss 1.7034443616867065\n",
      "Epoch 471 Training loss 1.1264833360630615\n",
      "Epoch 472 iteration 0 loss 0.2472795993089676\n",
      "Epoch 472 iteration 100 loss 0.43605560064315796\n",
      "Epoch 472 iteration 200 loss 1.6549594402313232\n",
      "Epoch 472 Training loss 1.1325208736730084\n",
      "Epoch 473 iteration 0 loss 0.22990813851356506\n",
      "Epoch 473 iteration 100 loss 0.3927481770515442\n",
      "Epoch 473 iteration 200 loss 1.640278935432434\n",
      "Epoch 473 Training loss 1.1375682562520841\n",
      "Epoch 474 iteration 0 loss 0.2616235911846161\n",
      "Epoch 474 iteration 100 loss 0.39683660864830017\n",
      "Epoch 474 iteration 200 loss 1.712742567062378\n",
      "Epoch 474 Training loss 1.1408859367656277\n",
      "Epoch 475 iteration 0 loss 0.2586827874183655\n",
      "Epoch 475 iteration 100 loss 0.44971519708633423\n",
      "Epoch 475 iteration 200 loss 1.7002612352371216\n",
      "Epoch 475 Training loss 1.1465617578914633\n",
      "Evaluation loss 7.404064641671764\n",
      "Epoch 476 iteration 0 loss 0.32830971479415894\n",
      "Epoch 476 iteration 100 loss 0.4533956050872803\n",
      "Epoch 476 iteration 200 loss 1.7328109741210938\n",
      "Epoch 476 Training loss 1.1493593193366047\n",
      "Epoch 477 iteration 0 loss 0.2737100124359131\n",
      "Epoch 477 iteration 100 loss 0.39957699179649353\n",
      "Epoch 477 iteration 200 loss 1.6946697235107422\n",
      "Epoch 477 Training loss 1.1554081720045999\n",
      "Epoch 478 iteration 0 loss 0.24957294762134552\n",
      "Epoch 478 iteration 100 loss 0.4142008125782013\n",
      "Epoch 478 iteration 200 loss 1.783672571182251\n",
      "Epoch 478 Training loss 1.1517373401804585\n",
      "Epoch 479 iteration 0 loss 0.28765103220939636\n",
      "Epoch 479 iteration 100 loss 0.41468822956085205\n",
      "Epoch 479 iteration 200 loss 1.761874794960022\n",
      "Epoch 479 Training loss 1.1554842185447678\n",
      "Epoch 480 iteration 0 loss 0.2745639681816101\n",
      "Epoch 480 iteration 100 loss 0.4719651937484741\n",
      "Epoch 480 iteration 200 loss 1.7476788759231567\n",
      "Epoch 480 Training loss 1.1688520560064783\n",
      "Evaluation loss 7.4124257795085535\n",
      "Epoch 481 iteration 0 loss 0.2647968828678131\n",
      "Epoch 481 iteration 100 loss 0.45586448907852173\n",
      "Epoch 481 iteration 200 loss 1.822751760482788\n",
      "Epoch 481 Training loss 1.1706094126322504\n",
      "Epoch 482 iteration 0 loss 0.2649865746498108\n",
      "Epoch 482 iteration 100 loss 0.3903464674949646\n",
      "Epoch 482 iteration 200 loss 1.8251309394836426\n",
      "Epoch 482 Training loss 1.1826581517003585\n",
      "Epoch 483 iteration 0 loss 0.27069681882858276\n",
      "Epoch 483 iteration 100 loss 0.41009822487831116\n",
      "Epoch 483 iteration 200 loss 1.858120083808899\n",
      "Epoch 483 Training loss 1.184591008235458\n",
      "Epoch 484 iteration 0 loss 0.28041043877601624\n",
      "Epoch 484 iteration 100 loss 0.4074944257736206\n",
      "Epoch 484 iteration 200 loss 1.9301751852035522\n",
      "Epoch 484 Training loss 1.1969162971624987\n",
      "Epoch 485 iteration 0 loss 0.24760858714580536\n",
      "Epoch 485 iteration 100 loss 0.43033522367477417\n",
      "Epoch 485 iteration 200 loss 1.865557312965393\n",
      "Epoch 485 Training loss 1.1982110361703553\n",
      "Evaluation loss 7.439183942727435\n",
      "Epoch 486 iteration 0 loss 0.2794092297554016\n",
      "Epoch 486 iteration 100 loss 0.41154178977012634\n",
      "Epoch 486 iteration 200 loss 1.9146722555160522\n",
      "Epoch 486 Training loss 1.1932194163420815\n",
      "Epoch 487 iteration 0 loss 0.22191095352172852\n",
      "Epoch 487 iteration 100 loss 0.4269365668296814\n",
      "Epoch 487 iteration 200 loss 1.8821303844451904\n",
      "Epoch 487 Training loss 1.1972787061641108\n",
      "Epoch 488 iteration 0 loss 0.24458126723766327\n",
      "Epoch 488 iteration 100 loss 0.40607357025146484\n",
      "Epoch 488 iteration 200 loss 1.8916743993759155\n",
      "Epoch 488 Training loss 1.204366759136894\n",
      "Epoch 489 iteration 0 loss 0.22475787997245789\n",
      "Epoch 489 iteration 100 loss 0.43896484375\n",
      "Epoch 489 iteration 200 loss 1.9094905853271484\n",
      "Epoch 489 Training loss 1.2128054928902858\n",
      "Epoch 490 iteration 0 loss 0.26522690057754517\n",
      "Epoch 490 iteration 100 loss 0.395100474357605\n",
      "Epoch 490 iteration 200 loss 1.9581950902938843\n",
      "Epoch 490 Training loss 1.2157551316808202\n",
      "Evaluation loss 7.435919268365416\n",
      "Epoch 491 iteration 0 loss 0.24605922400951385\n",
      "Epoch 491 iteration 100 loss 0.47026753425598145\n",
      "Epoch 491 iteration 200 loss 1.988438606262207\n",
      "Epoch 491 Training loss 1.2280706182352885\n",
      "Epoch 492 iteration 0 loss 0.2986633777618408\n",
      "Epoch 492 iteration 100 loss 0.4304119050502777\n",
      "Epoch 492 iteration 200 loss 1.9675617218017578\n",
      "Epoch 492 Training loss 1.2350135574410988\n",
      "Epoch 493 iteration 0 loss 0.27691107988357544\n",
      "Epoch 493 iteration 100 loss 0.46609237790107727\n",
      "Epoch 493 iteration 200 loss 2.069326877593994\n",
      "Epoch 493 Training loss 1.236054599830343\n",
      "Epoch 494 iteration 0 loss 0.2827974259853363\n",
      "Epoch 494 iteration 100 loss 0.38782742619514465\n",
      "Epoch 494 iteration 200 loss 2.095566749572754\n",
      "Epoch 494 Training loss 1.237137517953604\n",
      "Epoch 495 iteration 0 loss 0.23383131623268127\n",
      "Epoch 495 iteration 100 loss 0.456405371427536\n",
      "Epoch 495 iteration 200 loss 2.1282474994659424\n",
      "Epoch 495 Training loss 1.249669138366895\n",
      "Evaluation loss 7.445286233988645\n",
      "Epoch 496 iteration 0 loss 0.28551825881004333\n",
      "Epoch 496 iteration 100 loss 0.36333322525024414\n",
      "Epoch 496 iteration 200 loss 1.9817019701004028\n",
      "Epoch 496 Training loss 1.254335697377379\n",
      "Epoch 497 iteration 0 loss 0.3119070529937744\n",
      "Epoch 497 iteration 100 loss 0.41125842928886414\n",
      "Epoch 497 iteration 200 loss 2.0816965103149414\n",
      "Epoch 497 Training loss 1.261864701110305\n",
      "Epoch 498 iteration 0 loss 0.30674630403518677\n",
      "Epoch 498 iteration 100 loss 0.43129870295524597\n",
      "Epoch 498 iteration 200 loss 2.1649534702301025\n",
      "Epoch 498 Training loss 1.2693984907388554\n",
      "Epoch 499 iteration 0 loss 0.26921144127845764\n",
      "Epoch 499 iteration 100 loss 0.506008505821228\n",
      "Epoch 499 iteration 200 loss 2.062647581100464\n",
      "Epoch 499 Training loss 1.2737005218721293\n",
      "Epoch 500 iteration 0 loss 0.3446332812309265\n",
      "Epoch 500 iteration 100 loss 0.4252643585205078\n",
      "Epoch 500 iteration 200 loss 2.0666778087615967\n",
      "Epoch 500 Training loss 1.278514267404811\n",
      "Evaluation loss 7.409024222601777\n",
      "Epoch 501 iteration 0 loss 0.24745182693004608\n",
      "Epoch 501 iteration 100 loss 0.3815487027168274\n",
      "Epoch 501 iteration 200 loss 2.060612678527832\n",
      "Epoch 501 Training loss 1.2830728841000671\n",
      "Epoch 502 iteration 0 loss 0.2923033833503723\n",
      "Epoch 502 iteration 100 loss 0.45792749524116516\n",
      "Epoch 502 iteration 200 loss 1.8902310132980347\n",
      "Epoch 502 Training loss 1.2890947384126834\n",
      "Epoch 503 iteration 0 loss 0.32107236981391907\n",
      "Epoch 503 iteration 100 loss 0.43141013383865356\n",
      "Epoch 503 iteration 200 loss 1.9514583349227905\n",
      "Epoch 503 Training loss 1.2897811108510244\n",
      "Epoch 504 iteration 0 loss 0.2087184637784958\n",
      "Epoch 504 iteration 100 loss 0.45160380005836487\n",
      "Epoch 504 iteration 200 loss 1.9557021856307983\n",
      "Epoch 504 Training loss 1.2974167037135775\n",
      "Epoch 505 iteration 0 loss 0.23425456881523132\n",
      "Epoch 505 iteration 100 loss 0.4096660912036896\n",
      "Epoch 505 iteration 200 loss 1.9510208368301392\n",
      "Epoch 505 Training loss 1.2986314151168987\n",
      "Evaluation loss 7.39264520783056\n",
      "Epoch 506 iteration 0 loss 0.27718520164489746\n",
      "Epoch 506 iteration 100 loss 0.4679819643497467\n",
      "Epoch 506 iteration 200 loss 2.030992031097412\n",
      "Epoch 506 Training loss 1.3081493385013427\n",
      "Epoch 507 iteration 0 loss 0.2806445360183716\n",
      "Epoch 507 iteration 100 loss 0.40389785170555115\n",
      "Epoch 507 iteration 200 loss 2.0151207447052\n",
      "Epoch 507 Training loss 1.3053359296463223\n",
      "Epoch 508 iteration 0 loss 0.28460684418678284\n",
      "Epoch 508 iteration 100 loss 0.4725448489189148\n",
      "Epoch 508 iteration 200 loss 2.0977468490600586\n",
      "Epoch 508 Training loss 1.3095523890958491\n",
      "Epoch 509 iteration 0 loss 0.28190475702285767\n",
      "Epoch 509 iteration 100 loss 0.46072688698768616\n",
      "Epoch 509 iteration 200 loss 2.0551228523254395\n",
      "Epoch 509 Training loss 1.313828143790921\n",
      "Epoch 510 iteration 0 loss 0.3419114351272583\n",
      "Epoch 510 iteration 100 loss 0.44697245955467224\n",
      "Epoch 510 iteration 200 loss 2.102740526199341\n",
      "Epoch 510 Training loss 1.3250446143705028\n",
      "Evaluation loss 7.4212213179990725\n",
      "Epoch 511 iteration 0 loss 0.28325051069259644\n",
      "Epoch 511 iteration 100 loss 0.38829413056373596\n",
      "Epoch 511 iteration 200 loss 2.1102821826934814\n",
      "Epoch 511 Training loss 1.3214287716279307\n",
      "Epoch 512 iteration 0 loss 0.267363041639328\n",
      "Epoch 512 iteration 100 loss 0.43400582671165466\n",
      "Epoch 512 iteration 200 loss 2.089534044265747\n",
      "Epoch 512 Training loss 1.329549308131099\n",
      "Epoch 513 iteration 0 loss 0.31688886880874634\n",
      "Epoch 513 iteration 100 loss 0.4534079134464264\n",
      "Epoch 513 iteration 200 loss 2.1768224239349365\n",
      "Epoch 513 Training loss 1.3346106090986205\n",
      "Epoch 514 iteration 0 loss 0.2780541181564331\n",
      "Epoch 514 iteration 100 loss 0.384427547454834\n",
      "Epoch 514 iteration 200 loss 2.1199350357055664\n",
      "Epoch 514 Training loss 1.3403529038587128\n",
      "Epoch 515 iteration 0 loss 0.2534944713115692\n",
      "Epoch 515 iteration 100 loss 0.4755052328109741\n",
      "Epoch 515 iteration 200 loss 2.2367255687713623\n",
      "Epoch 515 Training loss 1.354281115904311\n",
      "Evaluation loss 7.424235431142938\n",
      "Epoch 516 iteration 0 loss 0.29152292013168335\n",
      "Epoch 516 iteration 100 loss 0.4758957028388977\n",
      "Epoch 516 iteration 200 loss 2.1663401126861572\n",
      "Epoch 516 Training loss 1.3571838618044316\n",
      "Epoch 517 iteration 0 loss 0.24994643032550812\n",
      "Epoch 517 iteration 100 loss 0.48211726546287537\n",
      "Epoch 517 iteration 200 loss 2.229757308959961\n",
      "Epoch 517 Training loss 1.3605839675842022\n",
      "Epoch 518 iteration 0 loss 0.2360258549451828\n",
      "Epoch 518 iteration 100 loss 0.468670129776001\n",
      "Epoch 518 iteration 200 loss 2.149003028869629\n",
      "Epoch 518 Training loss 1.3678454316634674\n",
      "Epoch 519 iteration 0 loss 0.2707543969154358\n",
      "Epoch 519 iteration 100 loss 0.4910605847835541\n",
      "Epoch 519 iteration 200 loss 2.0978646278381348\n",
      "Epoch 519 Training loss 1.3732213897498904\n",
      "Epoch 520 iteration 0 loss 0.3284815847873688\n",
      "Epoch 520 iteration 100 loss 0.4581851363182068\n",
      "Epoch 520 iteration 200 loss 2.2122862339019775\n",
      "Epoch 520 Training loss 1.38877331615759\n",
      "Evaluation loss 7.414343055790629\n",
      "Epoch 521 iteration 0 loss 0.3030136227607727\n",
      "Epoch 521 iteration 100 loss 0.42454153299331665\n",
      "Epoch 521 iteration 200 loss 2.2411675453186035\n",
      "Epoch 521 Training loss 1.3812311653509028\n",
      "Epoch 522 iteration 0 loss 0.2738340198993683\n",
      "Epoch 522 iteration 100 loss 0.4335269331932068\n",
      "Epoch 522 iteration 200 loss 2.200639247894287\n",
      "Epoch 522 Training loss 1.3947459228308143\n",
      "Epoch 523 iteration 0 loss 0.2575529217720032\n",
      "Epoch 523 iteration 100 loss 0.4386276304721832\n",
      "Epoch 523 iteration 200 loss 2.3226795196533203\n",
      "Epoch 523 Training loss 1.3972169326156636\n",
      "Epoch 524 iteration 0 loss 0.3123629689216614\n",
      "Epoch 524 iteration 100 loss 0.45475414395332336\n",
      "Epoch 524 iteration 200 loss 2.3121228218078613\n",
      "Epoch 524 Training loss 1.4046439640475767\n",
      "Epoch 525 iteration 0 loss 0.27916383743286133\n",
      "Epoch 525 iteration 100 loss 0.38810092210769653\n",
      "Epoch 525 iteration 200 loss 2.3660826683044434\n",
      "Epoch 525 Training loss 1.4041605681650782\n",
      "Evaluation loss 7.465533800310423\n",
      "Epoch 526 iteration 0 loss 0.2710668444633484\n",
      "Epoch 526 iteration 100 loss 0.39316684007644653\n",
      "Epoch 526 iteration 200 loss 2.337939977645874\n",
      "Epoch 526 Training loss 1.4096753531377721\n",
      "Epoch 527 iteration 0 loss 0.20965807139873505\n",
      "Epoch 527 iteration 100 loss 0.3653043210506439\n",
      "Epoch 527 iteration 200 loss 2.2618680000305176\n",
      "Epoch 527 Training loss 1.4116923526962812\n",
      "Epoch 528 iteration 0 loss 0.29758650064468384\n",
      "Epoch 528 iteration 100 loss 0.4733608365058899\n",
      "Epoch 528 iteration 200 loss 2.394118547439575\n",
      "Epoch 528 Training loss 1.4235437546197234\n",
      "Epoch 529 iteration 0 loss 0.2650674283504486\n",
      "Epoch 529 iteration 100 loss 0.42792877554893494\n",
      "Epoch 529 iteration 200 loss 2.2992591857910156\n",
      "Epoch 529 Training loss 1.4439742656300099\n",
      "Epoch 530 iteration 0 loss 0.2646443247795105\n",
      "Epoch 530 iteration 100 loss 0.4005008935928345\n",
      "Epoch 530 iteration 200 loss 2.3566956520080566\n",
      "Epoch 530 Training loss 1.4346009111289943\n",
      "Evaluation loss 7.4808707122878015\n",
      "Epoch 531 iteration 0 loss 0.2692044675350189\n",
      "Epoch 531 iteration 100 loss 0.41257256269454956\n",
      "Epoch 531 iteration 200 loss 2.3399837017059326\n",
      "Epoch 531 Training loss 1.4382479857853647\n",
      "Epoch 532 iteration 0 loss 0.29024529457092285\n",
      "Epoch 532 iteration 100 loss 0.4659039378166199\n",
      "Epoch 532 iteration 200 loss 2.387458086013794\n",
      "Epoch 532 Training loss 1.4482471045211573\n",
      "Epoch 533 iteration 0 loss 0.25589504837989807\n",
      "Epoch 533 iteration 100 loss 0.39593082666397095\n",
      "Epoch 533 iteration 200 loss 2.421844244003296\n",
      "Epoch 533 Training loss 1.448306385148552\n",
      "Epoch 534 iteration 0 loss 0.25737854838371277\n",
      "Epoch 534 iteration 100 loss 0.3784498870372772\n",
      "Epoch 534 iteration 200 loss 2.3807239532470703\n",
      "Epoch 534 Training loss 1.4569971325075768\n",
      "Epoch 535 iteration 0 loss 0.31071361899375916\n",
      "Epoch 535 iteration 100 loss 0.4634414315223694\n",
      "Epoch 535 iteration 200 loss 2.3084523677825928\n",
      "Epoch 535 Training loss 1.461256000501058\n",
      "Evaluation loss 7.454564218680844\n",
      "Epoch 536 iteration 0 loss 0.2471180260181427\n",
      "Epoch 536 iteration 100 loss 0.4401393234729767\n",
      "Epoch 536 iteration 200 loss 2.45212721824646\n",
      "Epoch 536 Training loss 1.4620661780285003\n",
      "Epoch 537 iteration 0 loss 0.24367289245128632\n",
      "Epoch 537 iteration 100 loss 0.43967103958129883\n",
      "Epoch 537 iteration 200 loss 2.439344644546509\n",
      "Epoch 537 Training loss 1.47387850552241\n",
      "Epoch 538 iteration 0 loss 0.32925403118133545\n",
      "Epoch 538 iteration 100 loss 0.4221241772174835\n",
      "Epoch 538 iteration 200 loss 2.4263641834259033\n",
      "Epoch 538 Training loss 1.4802130458430294\n",
      "Epoch 539 iteration 0 loss 0.19471000134944916\n",
      "Epoch 539 iteration 100 loss 0.4710858166217804\n",
      "Epoch 539 iteration 200 loss 2.402555465698242\n",
      "Epoch 539 Training loss 1.4776453439269388\n",
      "Epoch 540 iteration 0 loss 0.35384446382522583\n",
      "Epoch 540 iteration 100 loss 0.4509179890155792\n",
      "Epoch 540 iteration 200 loss 2.5663554668426514\n",
      "Epoch 540 Training loss 1.4853530723796113\n",
      "Evaluation loss 7.4477726222230665\n",
      "Epoch 541 iteration 0 loss 0.2602933645248413\n",
      "Epoch 541 iteration 100 loss 0.401873916387558\n",
      "Epoch 541 iteration 200 loss 2.529975414276123\n",
      "Epoch 541 Training loss 1.4884118300866767\n",
      "Epoch 542 iteration 0 loss 0.27631908655166626\n",
      "Epoch 542 iteration 100 loss 0.4397384524345398\n",
      "Epoch 542 iteration 200 loss 2.368959426879883\n",
      "Epoch 542 Training loss 1.4952989486198547\n",
      "Epoch 543 iteration 0 loss 0.2801847457885742\n",
      "Epoch 543 iteration 100 loss 0.4715002477169037\n",
      "Epoch 543 iteration 200 loss 2.5426714420318604\n",
      "Epoch 543 Training loss 1.5019443019620085\n",
      "Epoch 544 iteration 0 loss 0.2848970890045166\n",
      "Epoch 544 iteration 100 loss 0.4458308815956116\n",
      "Epoch 544 iteration 200 loss 2.4438374042510986\n",
      "Epoch 544 Training loss 1.5009576539423752\n",
      "Epoch 545 iteration 0 loss 0.3130576014518738\n",
      "Epoch 545 iteration 100 loss 0.48488718271255493\n",
      "Epoch 545 iteration 200 loss 2.593111515045166\n",
      "Epoch 545 Training loss 1.512460113638135\n",
      "Evaluation loss 7.440324139065427\n",
      "Epoch 546 iteration 0 loss 0.24919633567333221\n",
      "Epoch 546 iteration 100 loss 0.4508533179759979\n",
      "Epoch 546 iteration 200 loss 2.4239115715026855\n",
      "Epoch 546 Training loss 1.5138866625405631\n",
      "Epoch 547 iteration 0 loss 0.26642099022865295\n",
      "Epoch 547 iteration 100 loss 0.39226287603378296\n",
      "Epoch 547 iteration 200 loss 2.4774060249328613\n",
      "Epoch 547 Training loss 1.5166665837781044\n",
      "Epoch 548 iteration 0 loss 0.24447914958000183\n",
      "Epoch 548 iteration 100 loss 0.4315034747123718\n",
      "Epoch 548 iteration 200 loss 2.5379364490509033\n",
      "Epoch 548 Training loss 1.5310479116191866\n",
      "Epoch 549 iteration 0 loss 0.32223236560821533\n",
      "Epoch 549 iteration 100 loss 0.43142616748809814\n",
      "Epoch 549 iteration 200 loss 2.5027568340301514\n",
      "Epoch 549 Training loss 1.5358964837293207\n",
      "Epoch 550 iteration 0 loss 0.2541390359401703\n",
      "Epoch 550 iteration 100 loss 0.4084858298301697\n",
      "Epoch 550 iteration 200 loss 2.5362555980682373\n",
      "Epoch 550 Training loss 1.5490192598709487\n",
      "Evaluation loss 7.469506982414791\n",
      "Epoch 551 iteration 0 loss 0.3065026104450226\n",
      "Epoch 551 iteration 100 loss 0.41929757595062256\n",
      "Epoch 551 iteration 200 loss 2.528533697128296\n",
      "Epoch 551 Training loss 1.5512196351191563\n",
      "Epoch 552 iteration 0 loss 0.36632609367370605\n",
      "Epoch 552 iteration 100 loss 0.44079333543777466\n",
      "Epoch 552 iteration 200 loss 2.562171697616577\n",
      "Epoch 552 Training loss 1.5559865839255653\n",
      "Epoch 553 iteration 0 loss 0.3051564395427704\n",
      "Epoch 553 iteration 100 loss 0.41427910327911377\n",
      "Epoch 553 iteration 200 loss 2.515915632247925\n",
      "Epoch 553 Training loss 1.5622510769656075\n",
      "Epoch 554 iteration 0 loss 0.31531277298927307\n",
      "Epoch 554 iteration 100 loss 0.4218130111694336\n",
      "Epoch 554 iteration 200 loss 2.53128981590271\n",
      "Epoch 554 Training loss 1.5590875463509564\n",
      "Epoch 555 iteration 0 loss 0.3324868083000183\n",
      "Epoch 555 iteration 100 loss 0.4100494086742401\n",
      "Epoch 555 iteration 200 loss 2.597757577896118\n",
      "Epoch 555 Training loss 1.5626066900390514\n",
      "Evaluation loss 7.4905833786740175\n",
      "Epoch 556 iteration 0 loss 0.29681509733200073\n",
      "Epoch 556 iteration 100 loss 0.41791167855262756\n",
      "Epoch 556 iteration 200 loss 2.4824976921081543\n",
      "Epoch 556 Training loss 1.569132942197251\n",
      "Epoch 557 iteration 0 loss 0.32318466901779175\n",
      "Epoch 557 iteration 100 loss 0.35003048181533813\n",
      "Epoch 557 iteration 200 loss 2.553575038909912\n",
      "Epoch 557 Training loss 1.581580309751509\n",
      "Epoch 558 iteration 0 loss 0.2653849720954895\n",
      "Epoch 558 iteration 100 loss 0.3878473937511444\n",
      "Epoch 558 iteration 200 loss 2.5001957416534424\n",
      "Epoch 558 Training loss 1.58735643011157\n",
      "Epoch 559 iteration 0 loss 0.22917351126670837\n",
      "Epoch 559 iteration 100 loss 0.39189109206199646\n",
      "Epoch 559 iteration 200 loss 2.63041353225708\n",
      "Epoch 559 Training loss 1.5968961945539504\n",
      "Epoch 560 iteration 0 loss 0.3113882839679718\n",
      "Epoch 560 iteration 100 loss 0.40410706400871277\n",
      "Epoch 560 iteration 200 loss 2.6514947414398193\n",
      "Epoch 560 Training loss 1.603045735527321\n",
      "Evaluation loss 7.487881762244432\n",
      "Epoch 561 iteration 0 loss 0.3002263903617859\n",
      "Epoch 561 iteration 100 loss 0.4187273383140564\n",
      "Epoch 561 iteration 200 loss 2.7561540603637695\n",
      "Epoch 561 Training loss 1.6155446489039211\n",
      "Epoch 562 iteration 0 loss 0.29919469356536865\n",
      "Epoch 562 iteration 100 loss 0.4389777183532715\n",
      "Epoch 562 iteration 200 loss 2.6957616806030273\n",
      "Epoch 562 Training loss 1.6139296359641422\n",
      "Epoch 563 iteration 0 loss 0.28991788625717163\n",
      "Epoch 563 iteration 100 loss 0.38938769698143005\n",
      "Epoch 563 iteration 200 loss 2.6590797901153564\n",
      "Epoch 563 Training loss 1.6188818544480965\n",
      "Epoch 564 iteration 0 loss 0.28103601932525635\n",
      "Epoch 564 iteration 100 loss 0.41002288460731506\n",
      "Epoch 564 iteration 200 loss 2.642636775970459\n",
      "Epoch 564 Training loss 1.622526749999818\n",
      "Epoch 565 iteration 0 loss 0.2607368528842926\n",
      "Epoch 565 iteration 100 loss 0.4452299475669861\n",
      "Epoch 565 iteration 200 loss 2.775012731552124\n",
      "Epoch 565 Training loss 1.626175154227064\n",
      "Evaluation loss 7.436862902898739\n",
      "Epoch 566 iteration 0 loss 0.23865213990211487\n",
      "Epoch 566 iteration 100 loss 0.4746479392051697\n",
      "Epoch 566 iteration 200 loss 2.7808678150177\n",
      "Epoch 566 Training loss 1.642837788372074\n",
      "Epoch 567 iteration 0 loss 0.28347715735435486\n",
      "Epoch 567 iteration 100 loss 0.38562050461769104\n",
      "Epoch 567 iteration 200 loss 2.7909841537475586\n",
      "Epoch 567 Training loss 1.6447531845713914\n",
      "Epoch 568 iteration 0 loss 0.2349081039428711\n",
      "Epoch 568 iteration 100 loss 0.4060135781764984\n",
      "Epoch 568 iteration 200 loss 2.7864370346069336\n",
      "Epoch 568 Training loss 1.6432309969792132\n",
      "Epoch 569 iteration 0 loss 0.255141019821167\n",
      "Epoch 569 iteration 100 loss 0.4397538900375366\n",
      "Epoch 569 iteration 200 loss 2.8887784481048584\n",
      "Epoch 569 Training loss 1.6495421126773189\n",
      "Epoch 570 iteration 0 loss 0.2710857391357422\n",
      "Epoch 570 iteration 100 loss 0.41767245531082153\n",
      "Epoch 570 iteration 200 loss 2.7550976276397705\n",
      "Epoch 570 Training loss 1.6647619355542267\n",
      "Evaluation loss 7.4468949718622035\n",
      "Epoch 571 iteration 0 loss 0.2324765920639038\n",
      "Epoch 571 iteration 100 loss 0.4630471169948578\n",
      "Epoch 571 iteration 200 loss 2.747528076171875\n",
      "Epoch 571 Training loss 1.6586138376729362\n",
      "Epoch 572 iteration 0 loss 0.27919599413871765\n",
      "Epoch 572 iteration 100 loss 0.5015020370483398\n",
      "Epoch 572 iteration 200 loss 2.9011058807373047\n",
      "Epoch 572 Training loss 1.666477681759665\n",
      "Epoch 573 iteration 0 loss 0.3339945673942566\n",
      "Epoch 573 iteration 100 loss 0.42684653401374817\n",
      "Epoch 573 iteration 200 loss 2.9727981090545654\n",
      "Epoch 573 Training loss 1.6738466197139699\n",
      "Epoch 574 iteration 0 loss 0.23905189335346222\n",
      "Epoch 574 iteration 100 loss 0.4157399535179138\n",
      "Epoch 574 iteration 200 loss 2.9034600257873535\n",
      "Epoch 574 Training loss 1.675464638784213\n",
      "Epoch 575 iteration 0 loss 0.25335922837257385\n",
      "Epoch 575 iteration 100 loss 0.3594225347042084\n",
      "Epoch 575 iteration 200 loss 2.9779186248779297\n",
      "Epoch 575 Training loss 1.6820925957033492\n",
      "Evaluation loss 7.455050174387603\n",
      "Epoch 576 iteration 0 loss 0.27144917845726013\n",
      "Epoch 576 iteration 100 loss 0.43290895223617554\n",
      "Epoch 576 iteration 200 loss 2.8770713806152344\n",
      "Epoch 576 Training loss 1.6827231469045596\n",
      "Epoch 577 iteration 0 loss 0.27784624695777893\n",
      "Epoch 577 iteration 100 loss 0.37180694937705994\n",
      "Epoch 577 iteration 200 loss 2.8690083026885986\n",
      "Epoch 577 Training loss 1.6917131633021536\n",
      "Epoch 578 iteration 0 loss 0.26698198914527893\n",
      "Epoch 578 iteration 100 loss 0.45677250623703003\n",
      "Epoch 578 iteration 200 loss 2.7793145179748535\n",
      "Epoch 578 Training loss 1.6942372955936065\n",
      "Epoch 579 iteration 0 loss 0.29305487871170044\n",
      "Epoch 579 iteration 100 loss 0.4317832589149475\n",
      "Epoch 579 iteration 200 loss 2.928473472595215\n",
      "Epoch 579 Training loss 1.6980856384663372\n",
      "Epoch 580 iteration 0 loss 0.34014463424682617\n",
      "Epoch 580 iteration 100 loss 0.47047188878059387\n",
      "Epoch 580 iteration 200 loss 2.822197914123535\n",
      "Epoch 580 Training loss 1.7034251934561737\n",
      "Evaluation loss 7.476818093499306\n",
      "Epoch 581 iteration 0 loss 0.2998308539390564\n",
      "Epoch 581 iteration 100 loss 0.4504186809062958\n",
      "Epoch 581 iteration 200 loss 2.9195683002471924\n",
      "Epoch 581 Training loss 1.7204162740845463\n",
      "Epoch 582 iteration 0 loss 0.26008522510528564\n",
      "Epoch 582 iteration 100 loss 0.38780954480171204\n",
      "Epoch 582 iteration 200 loss 2.8103110790252686\n",
      "Epoch 582 Training loss 1.7142891234563968\n",
      "Epoch 583 iteration 0 loss 0.3370837867259979\n",
      "Epoch 583 iteration 100 loss 0.4030025005340576\n",
      "Epoch 583 iteration 200 loss 2.901326894760132\n",
      "Epoch 583 Training loss 1.7318650855122064\n",
      "Epoch 584 iteration 0 loss 0.2526465952396393\n",
      "Epoch 584 iteration 100 loss 0.4050425887107849\n",
      "Epoch 584 iteration 200 loss 3.053197145462036\n",
      "Epoch 584 Training loss 1.732783822253645\n",
      "Epoch 585 iteration 0 loss 0.2903219759464264\n",
      "Epoch 585 iteration 100 loss 0.4196558892726898\n",
      "Epoch 585 iteration 200 loss 3.0505683422088623\n",
      "Epoch 585 Training loss 1.7361976490372226\n",
      "Evaluation loss 7.494806854166206\n",
      "Epoch 586 iteration 0 loss 0.2762119174003601\n",
      "Epoch 586 iteration 100 loss 0.4122261106967926\n",
      "Epoch 586 iteration 200 loss 3.0663466453552246\n",
      "Epoch 586 Training loss 1.738831675754666\n",
      "Epoch 587 iteration 0 loss 0.2944105863571167\n",
      "Epoch 587 iteration 100 loss 0.4806787669658661\n",
      "Epoch 587 iteration 200 loss 3.0662841796875\n",
      "Epoch 587 Training loss 1.7478441166801506\n",
      "Epoch 588 iteration 0 loss 0.24076656997203827\n",
      "Epoch 588 iteration 100 loss 0.4141627848148346\n",
      "Epoch 588 iteration 200 loss 3.0222301483154297\n",
      "Epoch 588 Training loss 1.7546152410186435\n",
      "Epoch 589 iteration 0 loss 0.27455171942710876\n",
      "Epoch 589 iteration 100 loss 0.45221614837646484\n",
      "Epoch 589 iteration 200 loss 2.987433910369873\n",
      "Epoch 589 Training loss 1.7615228964755028\n",
      "Epoch 590 iteration 0 loss 0.29253286123275757\n",
      "Epoch 590 iteration 100 loss 0.4443175792694092\n",
      "Epoch 590 iteration 200 loss 3.055128574371338\n",
      "Epoch 590 Training loss 1.755291278439406\n",
      "Evaluation loss 7.506469068610222\n",
      "Epoch 591 iteration 0 loss 0.23094935715198517\n",
      "Epoch 591 iteration 100 loss 0.4710099697113037\n",
      "Epoch 591 iteration 200 loss 3.1078972816467285\n",
      "Epoch 591 Training loss 1.7746119966488558\n",
      "Epoch 592 iteration 0 loss 0.22012677788734436\n",
      "Epoch 592 iteration 100 loss 0.4483809173107147\n",
      "Epoch 592 iteration 200 loss 3.048377513885498\n",
      "Epoch 592 Training loss 1.784679546061048\n",
      "Epoch 593 iteration 0 loss 0.26856154203414917\n",
      "Epoch 593 iteration 100 loss 0.3980095386505127\n",
      "Epoch 593 iteration 200 loss 3.1442015171051025\n",
      "Epoch 593 Training loss 1.7938832036933803\n",
      "Epoch 594 iteration 0 loss 0.24474294483661652\n",
      "Epoch 594 iteration 100 loss 0.44681620597839355\n",
      "Epoch 594 iteration 200 loss 3.0286879539489746\n",
      "Epoch 594 Training loss 1.7947184970021046\n",
      "Epoch 595 iteration 0 loss 0.24181702733039856\n",
      "Epoch 595 iteration 100 loss 0.5395865440368652\n",
      "Epoch 595 iteration 200 loss 3.0689220428466797\n",
      "Epoch 595 Training loss 1.7948844445067973\n",
      "Evaluation loss 7.4977936718993865\n",
      "Epoch 596 iteration 0 loss 0.2865463197231293\n",
      "Epoch 596 iteration 100 loss 0.48429620265960693\n",
      "Epoch 596 iteration 200 loss 3.2205307483673096\n",
      "Epoch 596 Training loss 1.807579247383251\n",
      "Epoch 597 iteration 0 loss 0.3007679879665375\n",
      "Epoch 597 iteration 100 loss 0.49751749634742737\n",
      "Epoch 597 iteration 200 loss 2.9302191734313965\n",
      "Epoch 597 Training loss 1.8081357085728196\n",
      "Epoch 598 iteration 0 loss 0.2500799298286438\n",
      "Epoch 598 iteration 100 loss 0.49987319111824036\n",
      "Epoch 598 iteration 200 loss 3.03867506980896\n",
      "Epoch 598 Training loss 1.8203156518613044\n",
      "Epoch 599 iteration 0 loss 0.2824859619140625\n",
      "Epoch 599 iteration 100 loss 0.4673454463481903\n",
      "Epoch 599 iteration 200 loss 3.12064266204834\n",
      "Epoch 599 Training loss 1.828670514338391\n",
      "Epoch 600 iteration 0 loss 0.26339858770370483\n",
      "Epoch 600 iteration 100 loss 0.4641069769859314\n",
      "Epoch 600 iteration 200 loss 3.1046481132507324\n",
      "Epoch 600 Training loss 1.8334555744213008\n",
      "Evaluation loss 7.462061371089343\n",
      "Epoch 601 iteration 0 loss 0.282696396112442\n",
      "Epoch 601 iteration 100 loss 0.4530896842479706\n",
      "Epoch 601 iteration 200 loss 3.139741897583008\n",
      "Epoch 601 Training loss 1.8334929089058742\n",
      "Epoch 602 iteration 0 loss 0.2684553265571594\n",
      "Epoch 602 iteration 100 loss 0.5610998868942261\n",
      "Epoch 602 iteration 200 loss 3.172558307647705\n",
      "Epoch 602 Training loss 1.84162240389068\n",
      "Epoch 603 iteration 0 loss 0.3179793357849121\n",
      "Epoch 603 iteration 100 loss 0.46768850088119507\n",
      "Epoch 603 iteration 200 loss 3.2691242694854736\n",
      "Epoch 603 Training loss 1.8474706071832048\n",
      "Epoch 604 iteration 0 loss 0.29598721861839294\n",
      "Epoch 604 iteration 100 loss 0.574523389339447\n",
      "Epoch 604 iteration 200 loss 3.2117395401000977\n",
      "Epoch 604 Training loss 1.8614931902543177\n",
      "Epoch 605 iteration 0 loss 0.21370545029640198\n",
      "Epoch 605 iteration 100 loss 0.48881709575653076\n",
      "Epoch 605 iteration 200 loss 3.1503608226776123\n",
      "Epoch 605 Training loss 1.8624886040576518\n",
      "Evaluation loss 7.483854455973934\n",
      "Epoch 606 iteration 0 loss 0.2727845311164856\n",
      "Epoch 606 iteration 100 loss 0.47754138708114624\n",
      "Epoch 606 iteration 200 loss 3.2475366592407227\n",
      "Epoch 606 Training loss 1.8690291546207338\n",
      "Epoch 607 iteration 0 loss 0.3129769563674927\n",
      "Epoch 607 iteration 100 loss 0.4613189995288849\n",
      "Epoch 607 iteration 200 loss 3.288078784942627\n",
      "Epoch 607 Training loss 1.8761619282450017\n",
      "Epoch 608 iteration 0 loss 0.2631914019584656\n",
      "Epoch 608 iteration 100 loss 0.4661155045032501\n",
      "Epoch 608 iteration 200 loss 3.296884059906006\n",
      "Epoch 608 Training loss 1.8777916124042267\n",
      "Epoch 609 iteration 0 loss 0.25108981132507324\n",
      "Epoch 609 iteration 100 loss 0.4662773311138153\n",
      "Epoch 609 iteration 200 loss 3.198425531387329\n",
      "Epoch 609 Training loss 1.8823468233775786\n",
      "Epoch 610 iteration 0 loss 0.2205616980791092\n",
      "Epoch 610 iteration 100 loss 0.49557551741600037\n",
      "Epoch 610 iteration 200 loss 3.245638370513916\n",
      "Epoch 610 Training loss 1.888144941988181\n",
      "Evaluation loss 7.488960883253434\n",
      "Epoch 611 iteration 0 loss 0.3127608895301819\n",
      "Epoch 611 iteration 100 loss 0.4768090844154358\n",
      "Epoch 611 iteration 200 loss 3.3475372791290283\n",
      "Epoch 611 Training loss 1.9004596791708182\n",
      "Epoch 612 iteration 0 loss 0.2536473572254181\n",
      "Epoch 612 iteration 100 loss 0.5248382091522217\n",
      "Epoch 612 iteration 200 loss 3.37309193611145\n",
      "Epoch 612 Training loss 1.905338548146902\n",
      "Epoch 613 iteration 0 loss 0.29460078477859497\n",
      "Epoch 613 iteration 100 loss 0.5167490243911743\n",
      "Epoch 613 iteration 200 loss 3.1955859661102295\n",
      "Epoch 613 Training loss 1.9084860980314131\n",
      "Epoch 614 iteration 0 loss 0.2881518602371216\n",
      "Epoch 614 iteration 100 loss 0.5335752964019775\n",
      "Epoch 614 iteration 200 loss 3.2839908599853516\n",
      "Epoch 614 Training loss 1.9186054020630257\n",
      "Epoch 615 iteration 0 loss 0.2824869155883789\n",
      "Epoch 615 iteration 100 loss 0.5225049257278442\n",
      "Epoch 615 iteration 200 loss 3.367440938949585\n",
      "Epoch 615 Training loss 1.929095905084272\n",
      "Evaluation loss 7.480576017743415\n",
      "Epoch 616 iteration 0 loss 0.22707971930503845\n",
      "Epoch 616 iteration 100 loss 0.47750645875930786\n",
      "Epoch 616 iteration 200 loss 3.3004140853881836\n",
      "Epoch 616 Training loss 1.9354406095846424\n",
      "Epoch 617 iteration 0 loss 0.29246917366981506\n",
      "Epoch 617 iteration 100 loss 0.5223615765571594\n",
      "Epoch 617 iteration 200 loss 3.220019578933716\n",
      "Epoch 617 Training loss 1.9347925652153601\n",
      "Epoch 618 iteration 0 loss 0.23903150856494904\n",
      "Epoch 618 iteration 100 loss 0.4984453320503235\n",
      "Epoch 618 iteration 200 loss 3.337289810180664\n",
      "Epoch 618 Training loss 1.9387759515483365\n",
      "Epoch 619 iteration 0 loss 0.2686557173728943\n",
      "Epoch 619 iteration 100 loss 0.49065443873405457\n",
      "Epoch 619 iteration 200 loss 3.244863986968994\n",
      "Epoch 619 Training loss 1.9507780048661285\n",
      "Epoch 620 iteration 0 loss 0.2756254971027374\n",
      "Epoch 620 iteration 100 loss 0.5118626952171326\n",
      "Epoch 620 iteration 200 loss 3.4632153511047363\n",
      "Epoch 620 Training loss 1.951096480576629\n",
      "Evaluation loss 7.478298327667429\n",
      "Epoch 621 iteration 0 loss 0.31514328718185425\n",
      "Epoch 621 iteration 100 loss 0.5124266147613525\n",
      "Epoch 621 iteration 200 loss 3.3070828914642334\n",
      "Epoch 621 Training loss 1.954219107597425\n",
      "Epoch 622 iteration 0 loss 0.2956910729408264\n",
      "Epoch 622 iteration 100 loss 0.5239078402519226\n",
      "Epoch 622 iteration 200 loss 3.453277826309204\n",
      "Epoch 622 Training loss 1.9588715392194802\n",
      "Epoch 623 iteration 0 loss 0.22301463782787323\n",
      "Epoch 623 iteration 100 loss 0.5239174365997314\n",
      "Epoch 623 iteration 200 loss 3.423750877380371\n",
      "Epoch 623 Training loss 1.958695064086964\n",
      "Epoch 624 iteration 0 loss 0.2351086437702179\n",
      "Epoch 624 iteration 100 loss 0.5015955567359924\n",
      "Epoch 624 iteration 200 loss 3.489506244659424\n",
      "Epoch 624 Training loss 1.9668446014267105\n",
      "Epoch 625 iteration 0 loss 0.2578579783439636\n",
      "Epoch 625 iteration 100 loss 0.5268169045448303\n",
      "Epoch 625 iteration 200 loss 3.4512503147125244\n",
      "Epoch 625 Training loss 1.9703387520100746\n",
      "Evaluation loss 7.49068088867594\n",
      "Epoch 626 iteration 0 loss 0.2809145152568817\n",
      "Epoch 626 iteration 100 loss 0.5453059077262878\n",
      "Epoch 626 iteration 200 loss 3.4255926609039307\n",
      "Epoch 626 Training loss 1.9786651874904475\n",
      "Epoch 627 iteration 0 loss 0.2267749309539795\n",
      "Epoch 627 iteration 100 loss 0.49495217204093933\n",
      "Epoch 627 iteration 200 loss 3.554654836654663\n",
      "Epoch 627 Training loss 1.9785836662361358\n",
      "Epoch 628 iteration 0 loss 0.2674182057380676\n",
      "Epoch 628 iteration 100 loss 0.5301567912101746\n",
      "Epoch 628 iteration 200 loss 3.4910833835601807\n",
      "Epoch 628 Training loss 1.9821190510071867\n",
      "Epoch 629 iteration 0 loss 0.21179242432117462\n",
      "Epoch 629 iteration 100 loss 0.5374014377593994\n",
      "Epoch 629 iteration 200 loss 3.4507782459259033\n",
      "Epoch 629 Training loss 1.9857322451934314\n",
      "Epoch 630 iteration 0 loss 0.21552756428718567\n",
      "Epoch 630 iteration 100 loss 0.5478706359863281\n",
      "Epoch 630 iteration 200 loss 3.5153090953826904\n",
      "Epoch 630 Training loss 1.9969553328645748\n",
      "Evaluation loss 7.490560488620521\n",
      "Epoch 631 iteration 0 loss 0.25749990344047546\n",
      "Epoch 631 iteration 100 loss 0.45211753249168396\n",
      "Epoch 631 iteration 200 loss 3.402425527572632\n",
      "Epoch 631 Training loss 2.004193015641764\n",
      "Epoch 632 iteration 0 loss 0.25667625665664673\n",
      "Epoch 632 iteration 100 loss 0.4843003749847412\n",
      "Epoch 632 iteration 200 loss 3.438803195953369\n",
      "Epoch 632 Training loss 2.0041100312859412\n",
      "Epoch 633 iteration 0 loss 0.2531854808330536\n",
      "Epoch 633 iteration 100 loss 0.539929211139679\n",
      "Epoch 633 iteration 200 loss 3.480093479156494\n",
      "Epoch 633 Training loss 1.9998316440971362\n",
      "Epoch 634 iteration 0 loss 0.30747169256210327\n",
      "Epoch 634 iteration 100 loss 0.5225043892860413\n",
      "Epoch 634 iteration 200 loss 3.495899200439453\n",
      "Epoch 634 Training loss 2.0193609939314716\n",
      "Epoch 635 iteration 0 loss 0.3288359045982361\n",
      "Epoch 635 iteration 100 loss 0.5093276500701904\n",
      "Epoch 635 iteration 200 loss 3.479541063308716\n",
      "Epoch 635 Training loss 2.0265070282388224\n",
      "Evaluation loss 7.439453130392557\n",
      "Epoch 636 iteration 0 loss 0.27180156111717224\n",
      "Epoch 636 iteration 100 loss 0.501823902130127\n",
      "Epoch 636 iteration 200 loss 3.5444209575653076\n",
      "Epoch 636 Training loss 2.0156890441191666\n",
      "Epoch 637 iteration 0 loss 0.2938925325870514\n",
      "Epoch 637 iteration 100 loss 0.48918190598487854\n",
      "Epoch 637 iteration 200 loss 3.3104279041290283\n",
      "Epoch 637 Training loss 2.029002527530979\n",
      "Epoch 638 iteration 0 loss 0.309683233499527\n",
      "Epoch 638 iteration 100 loss 0.5561323761940002\n",
      "Epoch 638 iteration 200 loss 3.495237112045288\n",
      "Epoch 638 Training loss 2.023800118352033\n",
      "Epoch 639 iteration 0 loss 0.29176610708236694\n",
      "Epoch 639 iteration 100 loss 0.5512110590934753\n",
      "Epoch 639 iteration 200 loss 3.572972297668457\n",
      "Epoch 639 Training loss 2.048234033455408\n",
      "Epoch 640 iteration 0 loss 0.26456642150878906\n",
      "Epoch 640 iteration 100 loss 0.5506656765937805\n",
      "Epoch 640 iteration 200 loss 3.4496655464172363\n",
      "Epoch 640 Training loss 2.045949629798821\n",
      "Evaluation loss 7.472125423127676\n",
      "Epoch 641 iteration 0 loss 0.2539535462856293\n",
      "Epoch 641 iteration 100 loss 0.4819294512271881\n",
      "Epoch 641 iteration 200 loss 3.445911169052124\n",
      "Epoch 641 Training loss 2.042375577469593\n",
      "Epoch 642 iteration 0 loss 0.3096282184123993\n",
      "Epoch 642 iteration 100 loss 0.4952974319458008\n",
      "Epoch 642 iteration 200 loss 3.553433895111084\n",
      "Epoch 642 Training loss 2.055045459796482\n",
      "Epoch 643 iteration 0 loss 0.2961137592792511\n",
      "Epoch 643 iteration 100 loss 0.47679010033607483\n",
      "Epoch 643 iteration 200 loss 3.626934766769409\n",
      "Epoch 643 Training loss 2.055180463971145\n",
      "Epoch 644 iteration 0 loss 0.24710004031658173\n",
      "Epoch 644 iteration 100 loss 0.618340790271759\n",
      "Epoch 644 iteration 200 loss 3.613818407058716\n",
      "Epoch 644 Training loss 2.058986917936218\n",
      "Epoch 645 iteration 0 loss 0.28807273507118225\n",
      "Epoch 645 iteration 100 loss 0.5752724409103394\n",
      "Epoch 645 iteration 200 loss 3.4951536655426025\n",
      "Epoch 645 Training loss 2.0679067885945357\n",
      "Evaluation loss 7.500708983566141\n",
      "Epoch 646 iteration 0 loss 0.27687782049179077\n",
      "Epoch 646 iteration 100 loss 0.5090688467025757\n",
      "Epoch 646 iteration 200 loss 3.518148183822632\n",
      "Epoch 646 Training loss 2.077893614872056\n",
      "Epoch 647 iteration 0 loss 0.3101278841495514\n",
      "Epoch 647 iteration 100 loss 0.5899609923362732\n",
      "Epoch 647 iteration 200 loss 3.540261745452881\n",
      "Epoch 647 Training loss 2.078168861807426\n",
      "Epoch 648 iteration 0 loss 0.24071387946605682\n",
      "Epoch 648 iteration 100 loss 0.6145654320716858\n",
      "Epoch 648 iteration 200 loss 3.582347869873047\n",
      "Epoch 648 Training loss 2.0869537339671416\n",
      "Epoch 649 iteration 0 loss 0.23268096148967743\n",
      "Epoch 649 iteration 100 loss 0.6032586693763733\n",
      "Epoch 649 iteration 200 loss 3.6089391708374023\n",
      "Epoch 649 Training loss 2.099581407645872\n",
      "Epoch 650 iteration 0 loss 0.31976789236068726\n",
      "Epoch 650 iteration 100 loss 0.5571665167808533\n",
      "Epoch 650 iteration 200 loss 3.5794365406036377\n",
      "Epoch 650 Training loss 2.095721501306665\n",
      "Evaluation loss 7.510835847940027\n",
      "Epoch 651 iteration 0 loss 0.2801380753517151\n",
      "Epoch 651 iteration 100 loss 0.5528589487075806\n",
      "Epoch 651 iteration 200 loss 3.8255298137664795\n",
      "Epoch 651 Training loss 2.101954191343719\n",
      "Epoch 652 iteration 0 loss 0.2022823840379715\n",
      "Epoch 652 iteration 100 loss 0.6268399953842163\n",
      "Epoch 652 iteration 200 loss 3.801591396331787\n",
      "Epoch 652 Training loss 2.1032205707091527\n",
      "Epoch 653 iteration 0 loss 0.22646382451057434\n",
      "Epoch 653 iteration 100 loss 0.5198183655738831\n",
      "Epoch 653 iteration 200 loss 3.6676063537597656\n",
      "Epoch 653 Training loss 2.1154206016031463\n",
      "Epoch 654 iteration 0 loss 0.2218291312456131\n",
      "Epoch 654 iteration 100 loss 0.5031120181083679\n",
      "Epoch 654 iteration 200 loss 3.608366012573242\n",
      "Epoch 654 Training loss 2.1085555315513576\n",
      "Epoch 655 iteration 0 loss 0.22503729164600372\n",
      "Epoch 655 iteration 100 loss 0.5294028520584106\n",
      "Epoch 655 iteration 200 loss 3.7007784843444824\n",
      "Epoch 655 Training loss 2.124241746028508\n",
      "Evaluation loss 7.544305504837156\n",
      "Epoch 656 iteration 0 loss 0.28658032417297363\n",
      "Epoch 656 iteration 100 loss 0.5731790661811829\n",
      "Epoch 656 iteration 200 loss 3.7908012866973877\n",
      "Epoch 656 Training loss 2.1398997209134483\n",
      "Epoch 657 iteration 0 loss 0.2574487626552582\n",
      "Epoch 657 iteration 100 loss 0.5629404783248901\n",
      "Epoch 657 iteration 200 loss 3.7908618450164795\n",
      "Epoch 657 Training loss 2.1459264337428405\n",
      "Epoch 658 iteration 0 loss 0.2564612030982971\n",
      "Epoch 658 iteration 100 loss 0.5518438816070557\n",
      "Epoch 658 iteration 200 loss 3.739680528640747\n",
      "Epoch 658 Training loss 2.1490386915094595\n",
      "Epoch 659 iteration 0 loss 0.2772027850151062\n",
      "Epoch 659 iteration 100 loss 0.5042950510978699\n",
      "Epoch 659 iteration 200 loss 3.818898916244507\n",
      "Epoch 659 Training loss 2.1466089813052673\n",
      "Epoch 660 iteration 0 loss 0.2833940386772156\n",
      "Epoch 660 iteration 100 loss 0.49650338292121887\n",
      "Epoch 660 iteration 200 loss 3.802427053451538\n",
      "Epoch 660 Training loss 2.1539358020183297\n",
      "Evaluation loss 7.510978490314774\n",
      "Epoch 661 iteration 0 loss 0.3019924461841583\n",
      "Epoch 661 iteration 100 loss 0.4510258138179779\n",
      "Epoch 661 iteration 200 loss 3.807140588760376\n",
      "Epoch 661 Training loss 2.169771536960875\n",
      "Epoch 662 iteration 0 loss 0.3092025816440582\n",
      "Epoch 662 iteration 100 loss 0.5108371376991272\n",
      "Epoch 662 iteration 200 loss 3.920048713684082\n",
      "Epoch 662 Training loss 2.168079005617216\n",
      "Epoch 663 iteration 0 loss 0.26835668087005615\n",
      "Epoch 663 iteration 100 loss 0.5332936644554138\n",
      "Epoch 663 iteration 200 loss 3.870892286300659\n",
      "Epoch 663 Training loss 2.163762216346407\n",
      "Epoch 664 iteration 0 loss 0.2896033227443695\n",
      "Epoch 664 iteration 100 loss 0.5320139527320862\n",
      "Epoch 664 iteration 200 loss 3.800572395324707\n",
      "Epoch 664 Training loss 2.1742381268466255\n",
      "Epoch 665 iteration 0 loss 0.28354859352111816\n",
      "Epoch 665 iteration 100 loss 0.5266058444976807\n",
      "Epoch 665 iteration 200 loss 3.859992504119873\n",
      "Epoch 665 Training loss 2.183627719668488\n",
      "Evaluation loss 7.520780803112163\n",
      "Epoch 666 iteration 0 loss 0.2456328123807907\n",
      "Epoch 666 iteration 100 loss 0.5277369022369385\n",
      "Epoch 666 iteration 200 loss 3.7859928607940674\n",
      "Epoch 666 Training loss 2.1887598651728757\n",
      "Epoch 667 iteration 0 loss 0.2530907690525055\n",
      "Epoch 667 iteration 100 loss 0.5166472792625427\n",
      "Epoch 667 iteration 200 loss 3.8657593727111816\n",
      "Epoch 667 Training loss 2.2002350285722363\n",
      "Epoch 668 iteration 0 loss 0.30147120356559753\n",
      "Epoch 668 iteration 100 loss 0.45241978764533997\n",
      "Epoch 668 iteration 200 loss 3.8117549419403076\n",
      "Epoch 668 Training loss 2.200356794649195\n",
      "Epoch 669 iteration 0 loss 0.22595301270484924\n",
      "Epoch 669 iteration 100 loss 0.5679695010185242\n",
      "Epoch 669 iteration 200 loss 3.9225542545318604\n",
      "Epoch 669 Training loss 2.196301247654153\n",
      "Epoch 670 iteration 0 loss 0.26647910475730896\n",
      "Epoch 670 iteration 100 loss 0.5365656614303589\n",
      "Epoch 670 iteration 200 loss 3.8741042613983154\n",
      "Epoch 670 Training loss 2.204509881479378\n",
      "Evaluation loss 7.52467201725165\n",
      "Epoch 671 iteration 0 loss 0.27758926153182983\n",
      "Epoch 671 iteration 100 loss 0.53050696849823\n",
      "Epoch 671 iteration 200 loss 3.7349913120269775\n",
      "Epoch 671 Training loss 2.2014754928695486\n",
      "Epoch 672 iteration 0 loss 0.29268592596054077\n",
      "Epoch 672 iteration 100 loss 0.512519359588623\n",
      "Epoch 672 iteration 200 loss 4.005897521972656\n",
      "Epoch 672 Training loss 2.2209770761922862\n",
      "Epoch 673 iteration 0 loss 0.21241867542266846\n",
      "Epoch 673 iteration 100 loss 0.5242455005645752\n",
      "Epoch 673 iteration 200 loss 3.9825522899627686\n",
      "Epoch 673 Training loss 2.219909569201222\n",
      "Epoch 674 iteration 0 loss 0.263694167137146\n",
      "Epoch 674 iteration 100 loss 0.45885080099105835\n",
      "Epoch 674 iteration 200 loss 3.9057085514068604\n",
      "Epoch 674 Training loss 2.2180790087995335\n",
      "Epoch 675 iteration 0 loss 0.26024359464645386\n",
      "Epoch 675 iteration 100 loss 0.5720750093460083\n",
      "Epoch 675 iteration 200 loss 3.813882827758789\n",
      "Epoch 675 Training loss 2.240350862482619\n",
      "Evaluation loss 7.513476128388301\n",
      "Epoch 676 iteration 0 loss 0.2635098993778229\n",
      "Epoch 676 iteration 100 loss 0.4456893503665924\n",
      "Epoch 676 iteration 200 loss 3.9467337131500244\n",
      "Epoch 676 Training loss 2.240164821664101\n",
      "Epoch 677 iteration 0 loss 0.25993281602859497\n",
      "Epoch 677 iteration 100 loss 0.5334664583206177\n",
      "Epoch 677 iteration 200 loss 3.887899398803711\n",
      "Epoch 677 Training loss 2.2412386804513424\n",
      "Epoch 678 iteration 0 loss 0.2985842823982239\n",
      "Epoch 678 iteration 100 loss 0.5486561059951782\n",
      "Epoch 678 iteration 200 loss 3.7895421981811523\n",
      "Epoch 678 Training loss 2.2512273470043485\n",
      "Epoch 679 iteration 0 loss 0.26402968168258667\n",
      "Epoch 679 iteration 100 loss 0.47719112038612366\n",
      "Epoch 679 iteration 200 loss 3.9874682426452637\n",
      "Epoch 679 Training loss 2.252189758860561\n",
      "Epoch 680 iteration 0 loss 0.2786427438259125\n",
      "Epoch 680 iteration 100 loss 0.5600417256355286\n",
      "Epoch 680 iteration 200 loss 3.895456314086914\n",
      "Epoch 680 Training loss 2.2601138406469183\n",
      "Evaluation loss 7.557011295799326\n",
      "Epoch 681 iteration 0 loss 0.32443714141845703\n",
      "Epoch 681 iteration 100 loss 0.537331759929657\n",
      "Epoch 681 iteration 200 loss 4.082159519195557\n",
      "Epoch 681 Training loss 2.272511415345363\n",
      "Epoch 682 iteration 0 loss 0.26537227630615234\n",
      "Epoch 682 iteration 100 loss 0.5094034671783447\n",
      "Epoch 682 iteration 200 loss 4.007798671722412\n",
      "Epoch 682 Training loss 2.271313071272916\n",
      "Epoch 683 iteration 0 loss 0.30381518602371216\n",
      "Epoch 683 iteration 100 loss 0.5347176790237427\n",
      "Epoch 683 iteration 200 loss 4.022205829620361\n",
      "Epoch 683 Training loss 2.2675234323649773\n",
      "Epoch 684 iteration 0 loss 0.3593329191207886\n",
      "Epoch 684 iteration 100 loss 0.5368819236755371\n",
      "Epoch 684 iteration 200 loss 4.101439952850342\n",
      "Epoch 684 Training loss 2.2789393343385744\n",
      "Epoch 685 iteration 0 loss 0.3219306170940399\n",
      "Epoch 685 iteration 100 loss 0.5327417850494385\n",
      "Epoch 685 iteration 200 loss 4.073446750640869\n",
      "Epoch 685 Training loss 2.289022622600685\n",
      "Evaluation loss 7.530988278553287\n",
      "Epoch 686 iteration 0 loss 0.31003448367118835\n",
      "Epoch 686 iteration 100 loss 0.5886453986167908\n",
      "Epoch 686 iteration 200 loss 4.128881454467773\n",
      "Epoch 686 Training loss 2.2947548669231055\n",
      "Epoch 687 iteration 0 loss 0.34164103865623474\n",
      "Epoch 687 iteration 100 loss 0.6016891598701477\n",
      "Epoch 687 iteration 200 loss 3.8032116889953613\n",
      "Epoch 687 Training loss 2.303568279251382\n",
      "Epoch 688 iteration 0 loss 0.30004605650901794\n",
      "Epoch 688 iteration 100 loss 0.5569236278533936\n",
      "Epoch 688 iteration 200 loss 4.102540493011475\n",
      "Epoch 688 Training loss 2.29958943843413\n",
      "Epoch 689 iteration 0 loss 0.26148757338523865\n",
      "Epoch 689 iteration 100 loss 0.5775319337844849\n",
      "Epoch 689 iteration 200 loss 4.056356906890869\n",
      "Epoch 689 Training loss 2.3067727042347226\n",
      "Epoch 690 iteration 0 loss 0.288350909948349\n",
      "Epoch 690 iteration 100 loss 0.5666846632957458\n",
      "Epoch 690 iteration 200 loss 3.8758418560028076\n",
      "Epoch 690 Training loss 2.305863543386341\n",
      "Evaluation loss 7.519943379984268\n",
      "Epoch 691 iteration 0 loss 0.3315765857696533\n",
      "Epoch 691 iteration 100 loss 0.533331573009491\n",
      "Epoch 691 iteration 200 loss 4.095319747924805\n",
      "Epoch 691 Training loss 2.3077676830526324\n",
      "Epoch 692 iteration 0 loss 0.33921873569488525\n",
      "Epoch 692 iteration 100 loss 0.5515083074569702\n",
      "Epoch 692 iteration 200 loss 4.146217346191406\n",
      "Epoch 692 Training loss 2.316042823893962\n",
      "Epoch 693 iteration 0 loss 0.2503040134906769\n",
      "Epoch 693 iteration 100 loss 0.5077939629554749\n",
      "Epoch 693 iteration 200 loss 4.038349151611328\n",
      "Epoch 693 Training loss 2.3238416478733566\n",
      "Epoch 694 iteration 0 loss 0.24105393886566162\n",
      "Epoch 694 iteration 100 loss 0.578879714012146\n",
      "Epoch 694 iteration 200 loss 4.066729545593262\n",
      "Epoch 694 Training loss 2.3251808832250083\n",
      "Epoch 695 iteration 0 loss 0.22855143249034882\n",
      "Epoch 695 iteration 100 loss 0.5485519170761108\n",
      "Epoch 695 iteration 200 loss 4.17123556137085\n",
      "Epoch 695 Training loss 2.3281290230602933\n",
      "Evaluation loss 7.515235962549491\n",
      "Epoch 696 iteration 0 loss 0.29258617758750916\n",
      "Epoch 696 iteration 100 loss 0.6025993824005127\n",
      "Epoch 696 iteration 200 loss 4.181230545043945\n",
      "Epoch 696 Training loss 2.347108045908867\n",
      "Epoch 697 iteration 0 loss 0.2654503583908081\n",
      "Epoch 697 iteration 100 loss 0.6356214284896851\n",
      "Epoch 697 iteration 200 loss 4.107068061828613\n",
      "Epoch 697 Training loss 2.3551222653403565\n",
      "Epoch 698 iteration 0 loss 0.254232257604599\n",
      "Epoch 698 iteration 100 loss 0.5445216298103333\n",
      "Epoch 698 iteration 200 loss 4.3892011642456055\n",
      "Epoch 698 Training loss 2.365745289383876\n",
      "Epoch 699 iteration 0 loss 0.27480584383010864\n",
      "Epoch 699 iteration 100 loss 0.5252748727798462\n",
      "Epoch 699 iteration 200 loss 4.298849582672119\n",
      "Epoch 699 Training loss 2.362283895013672\n",
      "Epoch 700 iteration 0 loss 0.2977253198623657\n",
      "Epoch 700 iteration 100 loss 0.6283321976661682\n",
      "Epoch 700 iteration 200 loss 4.2855682373046875\n",
      "Epoch 700 Training loss 2.3700997475893026\n",
      "Evaluation loss 7.540252847914717\n",
      "Epoch 701 iteration 0 loss 0.2296338528394699\n",
      "Epoch 701 iteration 100 loss 0.5576698780059814\n",
      "Epoch 701 iteration 200 loss 4.228916645050049\n",
      "Epoch 701 Training loss 2.3730447286341665\n",
      "Epoch 702 iteration 0 loss 0.3276481032371521\n",
      "Epoch 702 iteration 100 loss 0.6311274766921997\n",
      "Epoch 702 iteration 200 loss 4.285050868988037\n",
      "Epoch 702 Training loss 2.376726977603972\n",
      "Epoch 703 iteration 0 loss 0.29069027304649353\n",
      "Epoch 703 iteration 100 loss 0.6551852226257324\n",
      "Epoch 703 iteration 200 loss 4.205433368682861\n",
      "Epoch 703 Training loss 2.3922811737289513\n",
      "Epoch 704 iteration 0 loss 0.22407011687755585\n",
      "Epoch 704 iteration 100 loss 0.683546781539917\n",
      "Epoch 704 iteration 200 loss 4.225346565246582\n",
      "Epoch 704 Training loss 2.398351798862214\n",
      "Epoch 705 iteration 0 loss 0.2262943536043167\n",
      "Epoch 705 iteration 100 loss 0.6106686592102051\n",
      "Epoch 705 iteration 200 loss 4.205885887145996\n",
      "Epoch 705 Training loss 2.394590332574774\n",
      "Evaluation loss 7.542065417170011\n",
      "Epoch 706 iteration 0 loss 0.32885321974754333\n",
      "Epoch 706 iteration 100 loss 0.5264474749565125\n",
      "Epoch 706 iteration 200 loss 4.416414260864258\n",
      "Epoch 706 Training loss 2.38967924154226\n",
      "Epoch 707 iteration 0 loss 0.27237075567245483\n",
      "Epoch 707 iteration 100 loss 0.6717001795768738\n",
      "Epoch 707 iteration 200 loss 4.251588344573975\n",
      "Epoch 707 Training loss 2.408395206586661\n",
      "Epoch 708 iteration 0 loss 0.21991106867790222\n",
      "Epoch 708 iteration 100 loss 0.5886527299880981\n",
      "Epoch 708 iteration 200 loss 4.367260932922363\n",
      "Epoch 708 Training loss 2.40664592496855\n",
      "Epoch 709 iteration 0 loss 0.2171766459941864\n",
      "Epoch 709 iteration 100 loss 0.6158472895622253\n",
      "Epoch 709 iteration 200 loss 4.4225172996521\n",
      "Epoch 709 Training loss 2.4199755878448657\n",
      "Epoch 710 iteration 0 loss 0.2288675755262375\n",
      "Epoch 710 iteration 100 loss 0.5817887783050537\n",
      "Epoch 710 iteration 200 loss 4.389596939086914\n",
      "Epoch 710 Training loss 2.428025318851745\n",
      "Evaluation loss 7.590082104737017\n",
      "Epoch 711 iteration 0 loss 0.2953886389732361\n",
      "Epoch 711 iteration 100 loss 0.5569305419921875\n",
      "Epoch 711 iteration 200 loss 4.372426509857178\n",
      "Epoch 711 Training loss 2.4262005789612067\n",
      "Epoch 712 iteration 0 loss 0.24255943298339844\n",
      "Epoch 712 iteration 100 loss 0.6178646683692932\n",
      "Epoch 712 iteration 200 loss 4.455420017242432\n",
      "Epoch 712 Training loss 2.4334428268900314\n",
      "Epoch 713 iteration 0 loss 0.18009819090366364\n",
      "Epoch 713 iteration 100 loss 0.6171985268592834\n",
      "Epoch 713 iteration 200 loss 4.42131233215332\n",
      "Epoch 713 Training loss 2.444021220675968\n",
      "Epoch 714 iteration 0 loss 0.23630504310131073\n",
      "Epoch 714 iteration 100 loss 0.5716615319252014\n",
      "Epoch 714 iteration 200 loss 4.239689826965332\n",
      "Epoch 714 Training loss 2.4409411232238085\n",
      "Epoch 715 iteration 0 loss 0.2970811724662781\n",
      "Epoch 715 iteration 100 loss 0.5784838199615479\n",
      "Epoch 715 iteration 200 loss 4.437130928039551\n",
      "Epoch 715 Training loss 2.4498109232699585\n",
      "Evaluation loss 7.609908258191718\n",
      "Epoch 716 iteration 0 loss 0.29254674911499023\n",
      "Epoch 716 iteration 100 loss 0.5826376676559448\n",
      "Epoch 716 iteration 200 loss 4.374044895172119\n",
      "Epoch 716 Training loss 2.445573689026452\n",
      "Epoch 717 iteration 0 loss 0.24268440902233124\n",
      "Epoch 717 iteration 100 loss 0.5926348567008972\n",
      "Epoch 717 iteration 200 loss 4.309544563293457\n",
      "Epoch 717 Training loss 2.451840144645355\n",
      "Epoch 718 iteration 0 loss 0.24715451896190643\n",
      "Epoch 718 iteration 100 loss 0.6633234620094299\n",
      "Epoch 718 iteration 200 loss 4.469967365264893\n",
      "Epoch 718 Training loss 2.4630011709977015\n",
      "Epoch 719 iteration 0 loss 0.251905620098114\n",
      "Epoch 719 iteration 100 loss 0.5515103340148926\n",
      "Epoch 719 iteration 200 loss 4.344119071960449\n",
      "Epoch 719 Training loss 2.4730844060824624\n",
      "Epoch 720 iteration 0 loss 0.2715950608253479\n",
      "Epoch 720 iteration 100 loss 0.6570342779159546\n",
      "Epoch 720 iteration 200 loss 4.386755466461182\n",
      "Epoch 720 Training loss 2.475926432165567\n",
      "Evaluation loss 7.587086696853487\n",
      "Epoch 721 iteration 0 loss 0.294027715921402\n",
      "Epoch 721 iteration 100 loss 0.5842446088790894\n",
      "Epoch 721 iteration 200 loss 4.294233322143555\n",
      "Epoch 721 Training loss 2.4855305658196287\n",
      "Epoch 722 iteration 0 loss 0.3207044005393982\n",
      "Epoch 722 iteration 100 loss 0.6316840648651123\n",
      "Epoch 722 iteration 200 loss 4.521910190582275\n",
      "Epoch 722 Training loss 2.489525717073666\n",
      "Epoch 723 iteration 0 loss 0.2871955633163452\n",
      "Epoch 723 iteration 100 loss 0.6309299468994141\n",
      "Epoch 723 iteration 200 loss 4.410051345825195\n",
      "Epoch 723 Training loss 2.494590858777332\n",
      "Epoch 724 iteration 0 loss 0.30626165866851807\n",
      "Epoch 724 iteration 100 loss 0.6368793845176697\n",
      "Epoch 724 iteration 200 loss 4.548508167266846\n",
      "Epoch 724 Training loss 2.496036982379182\n",
      "Epoch 725 iteration 0 loss 0.28145962953567505\n",
      "Epoch 725 iteration 100 loss 0.6084818243980408\n",
      "Epoch 725 iteration 200 loss 4.578347206115723\n",
      "Epoch 725 Training loss 2.5121675819864606\n",
      "Evaluation loss 7.566069948391556\n",
      "Epoch 726 iteration 0 loss 0.22890329360961914\n",
      "Epoch 726 iteration 100 loss 0.5945852994918823\n",
      "Epoch 726 iteration 200 loss 4.568040370941162\n",
      "Epoch 726 Training loss 2.5142360463330538\n",
      "Epoch 727 iteration 0 loss 0.26011720299720764\n",
      "Epoch 727 iteration 100 loss 0.5062012076377869\n",
      "Epoch 727 iteration 200 loss 4.553679943084717\n",
      "Epoch 727 Training loss 2.5191966743948413\n",
      "Epoch 728 iteration 0 loss 0.26006144285202026\n",
      "Epoch 728 iteration 100 loss 0.5587679147720337\n",
      "Epoch 728 iteration 200 loss 4.409496307373047\n",
      "Epoch 728 Training loss 2.5205718790681995\n",
      "Epoch 729 iteration 0 loss 0.30000218749046326\n",
      "Epoch 729 iteration 100 loss 0.600063681602478\n",
      "Epoch 729 iteration 200 loss 4.476822853088379\n",
      "Epoch 729 Training loss 2.525683881658972\n",
      "Epoch 730 iteration 0 loss 0.2829030156135559\n",
      "Epoch 730 iteration 100 loss 0.6242152452468872\n",
      "Epoch 730 iteration 200 loss 4.6731157302856445\n",
      "Epoch 730 Training loss 2.526474848396506\n",
      "Evaluation loss 7.57568248276261\n",
      "Epoch 731 iteration 0 loss 0.26807671785354614\n",
      "Epoch 731 iteration 100 loss 0.5664680004119873\n",
      "Epoch 731 iteration 200 loss 4.499658107757568\n",
      "Epoch 731 Training loss 2.531546005092985\n",
      "Epoch 732 iteration 0 loss 0.21297259628772736\n",
      "Epoch 732 iteration 100 loss 0.6350346803665161\n",
      "Epoch 732 iteration 200 loss 4.630332946777344\n",
      "Epoch 732 Training loss 2.5360557766144995\n",
      "Epoch 733 iteration 0 loss 0.3031272292137146\n",
      "Epoch 733 iteration 100 loss 0.6651577949523926\n",
      "Epoch 733 iteration 200 loss 4.580186367034912\n",
      "Epoch 733 Training loss 2.5471596184505527\n",
      "Epoch 734 iteration 0 loss 0.3043162226676941\n",
      "Epoch 734 iteration 100 loss 0.6440590620040894\n",
      "Epoch 734 iteration 200 loss 4.539237976074219\n",
      "Epoch 734 Training loss 2.556385783356385\n",
      "Epoch 735 iteration 0 loss 0.28086280822753906\n",
      "Epoch 735 iteration 100 loss 0.6323967576026917\n",
      "Epoch 735 iteration 200 loss 4.6289143562316895\n",
      "Epoch 735 Training loss 2.5570476405523554\n",
      "Evaluation loss 7.591203363271207\n",
      "Epoch 736 iteration 0 loss 0.33088603615760803\n",
      "Epoch 736 iteration 100 loss 0.6094256639480591\n",
      "Epoch 736 iteration 200 loss 4.632437705993652\n",
      "Epoch 736 Training loss 2.5595108727790135\n",
      "Epoch 737 iteration 0 loss 0.2786869406700134\n",
      "Epoch 737 iteration 100 loss 0.6346736550331116\n",
      "Epoch 737 iteration 200 loss 4.658950328826904\n",
      "Epoch 737 Training loss 2.558497022495268\n",
      "Epoch 738 iteration 0 loss 0.2834417223930359\n",
      "Epoch 738 iteration 100 loss 0.5649126172065735\n",
      "Epoch 738 iteration 200 loss 4.540637493133545\n",
      "Epoch 738 Training loss 2.5628770786209225\n",
      "Epoch 739 iteration 0 loss 0.3378225564956665\n",
      "Epoch 739 iteration 100 loss 0.640762448310852\n",
      "Epoch 739 iteration 200 loss 4.582254409790039\n",
      "Epoch 739 Training loss 2.573931668056394\n",
      "Epoch 740 iteration 0 loss 0.3242560029029846\n",
      "Epoch 740 iteration 100 loss 0.5729029774665833\n",
      "Epoch 740 iteration 200 loss 4.557905197143555\n",
      "Epoch 740 Training loss 2.581214901434828\n",
      "Evaluation loss 7.574144199093193\n",
      "Epoch 741 iteration 0 loss 0.25826507806777954\n",
      "Epoch 741 iteration 100 loss 0.5557627081871033\n",
      "Epoch 741 iteration 200 loss 4.56921911239624\n",
      "Epoch 741 Training loss 2.579568369764122\n",
      "Epoch 742 iteration 0 loss 0.2595922350883484\n",
      "Epoch 742 iteration 100 loss 0.5001286268234253\n",
      "Epoch 742 iteration 200 loss 4.555254936218262\n",
      "Epoch 742 Training loss 2.5795712199619825\n",
      "Epoch 743 iteration 0 loss 0.19882598519325256\n",
      "Epoch 743 iteration 100 loss 0.6163306832313538\n",
      "Epoch 743 iteration 200 loss 4.708399295806885\n",
      "Epoch 743 Training loss 2.590165098964944\n",
      "Epoch 744 iteration 0 loss 0.26936089992523193\n",
      "Epoch 744 iteration 100 loss 0.6261577606201172\n",
      "Epoch 744 iteration 200 loss 4.698515892028809\n",
      "Epoch 744 Training loss 2.6027891931840466\n",
      "Epoch 745 iteration 0 loss 0.31789857149124146\n",
      "Epoch 745 iteration 100 loss 0.6198564767837524\n",
      "Epoch 745 iteration 200 loss 4.630145072937012\n",
      "Epoch 745 Training loss 2.5969039748216987\n",
      "Evaluation loss 7.557936696528978\n",
      "Epoch 746 iteration 0 loss 0.21261043846607208\n",
      "Epoch 746 iteration 100 loss 0.567812442779541\n",
      "Epoch 746 iteration 200 loss 4.562655448913574\n",
      "Epoch 746 Training loss 2.6106583316550678\n",
      "Epoch 747 iteration 0 loss 0.2866353690624237\n",
      "Epoch 747 iteration 100 loss 0.6221657991409302\n",
      "Epoch 747 iteration 200 loss 4.737433433532715\n",
      "Epoch 747 Training loss 2.6160345623945873\n",
      "Epoch 748 iteration 0 loss 0.2676483988761902\n",
      "Epoch 748 iteration 100 loss 0.6196640729904175\n",
      "Epoch 748 iteration 200 loss 4.787189483642578\n",
      "Epoch 748 Training loss 2.617733504288807\n",
      "Epoch 749 iteration 0 loss 0.2486458569765091\n",
      "Epoch 749 iteration 100 loss 0.665744960308075\n",
      "Epoch 749 iteration 200 loss 4.845848560333252\n",
      "Epoch 749 Training loss 2.6218545270700093\n",
      "Epoch 750 iteration 0 loss 0.26356562972068787\n",
      "Epoch 750 iteration 100 loss 0.6206371784210205\n",
      "Epoch 750 iteration 200 loss 4.782543659210205\n",
      "Epoch 750 Training loss 2.6386627232007043\n",
      "Evaluation loss 7.608449590753244\n",
      "Epoch 751 iteration 0 loss 0.2817530035972595\n",
      "Epoch 751 iteration 100 loss 0.6877918243408203\n",
      "Epoch 751 iteration 200 loss 4.786863803863525\n",
      "Epoch 751 Training loss 2.627846272494918\n",
      "Epoch 752 iteration 0 loss 0.22019270062446594\n",
      "Epoch 752 iteration 100 loss 0.596365749835968\n",
      "Epoch 752 iteration 200 loss 4.6882710456848145\n",
      "Epoch 752 Training loss 2.6305097891373976\n",
      "Epoch 753 iteration 0 loss 0.2800500690937042\n",
      "Epoch 753 iteration 100 loss 0.6534057259559631\n",
      "Epoch 753 iteration 200 loss 4.80232048034668\n",
      "Epoch 753 Training loss 2.6322026044121314\n",
      "Epoch 754 iteration 0 loss 0.27567988634109497\n",
      "Epoch 754 iteration 100 loss 0.706821084022522\n",
      "Epoch 754 iteration 200 loss 4.808190822601318\n",
      "Epoch 754 Training loss 2.644619569427598\n",
      "Epoch 755 iteration 0 loss 0.25729626417160034\n",
      "Epoch 755 iteration 100 loss 0.641990065574646\n",
      "Epoch 755 iteration 200 loss 4.921718597412109\n",
      "Epoch 755 Training loss 2.6549156757277985\n",
      "Evaluation loss 7.644528981000192\n",
      "Epoch 756 iteration 0 loss 0.25799939036369324\n",
      "Epoch 756 iteration 100 loss 0.6477230191230774\n",
      "Epoch 756 iteration 200 loss 4.8597636222839355\n",
      "Epoch 756 Training loss 2.6509281617283835\n",
      "Epoch 757 iteration 0 loss 0.2269049435853958\n",
      "Epoch 757 iteration 100 loss 0.7051424384117126\n",
      "Epoch 757 iteration 200 loss 4.827248573303223\n",
      "Epoch 757 Training loss 2.6648765666539456\n",
      "Epoch 758 iteration 0 loss 0.27662020921707153\n",
      "Epoch 758 iteration 100 loss 0.6927003860473633\n",
      "Epoch 758 iteration 200 loss 4.86649227142334\n",
      "Epoch 758 Training loss 2.668358083502818\n",
      "Epoch 759 iteration 0 loss 0.3107464909553528\n",
      "Epoch 759 iteration 100 loss 0.6350727677345276\n",
      "Epoch 759 iteration 200 loss 4.772289276123047\n",
      "Epoch 759 Training loss 2.663998360172563\n",
      "Epoch 760 iteration 0 loss 0.2780309319496155\n",
      "Epoch 760 iteration 100 loss 0.6441774368286133\n",
      "Epoch 760 iteration 200 loss 4.920126438140869\n",
      "Epoch 760 Training loss 2.6694753324951135\n",
      "Evaluation loss 7.621958392934206\n",
      "Epoch 761 iteration 0 loss 0.27416524291038513\n",
      "Epoch 761 iteration 100 loss 0.6018317341804504\n",
      "Epoch 761 iteration 200 loss 4.877551078796387\n",
      "Epoch 761 Training loss 2.6849059055152025\n",
      "Epoch 762 iteration 0 loss 0.2214900553226471\n",
      "Epoch 762 iteration 100 loss 0.5677619576454163\n",
      "Epoch 762 iteration 200 loss 4.934371471405029\n",
      "Epoch 762 Training loss 2.6857572489073727\n",
      "Epoch 763 iteration 0 loss 0.2603663504123688\n",
      "Epoch 763 iteration 100 loss 0.6592315435409546\n",
      "Epoch 763 iteration 200 loss 5.01602029800415\n",
      "Epoch 763 Training loss 2.6910017173340277\n",
      "Epoch 764 iteration 0 loss 0.2654356360435486\n",
      "Epoch 764 iteration 100 loss 0.7087864279747009\n",
      "Epoch 764 iteration 200 loss 4.814625263214111\n",
      "Epoch 764 Training loss 2.696014805749281\n",
      "Epoch 765 iteration 0 loss 0.2307252585887909\n",
      "Epoch 765 iteration 100 loss 0.7073812484741211\n",
      "Epoch 765 iteration 200 loss 4.918304443359375\n",
      "Epoch 765 Training loss 2.703712653798552\n",
      "Evaluation loss 7.661508432219025\n",
      "Epoch 766 iteration 0 loss 0.227575421333313\n",
      "Epoch 766 iteration 100 loss 0.6568071246147156\n",
      "Epoch 766 iteration 200 loss 4.957302093505859\n",
      "Epoch 766 Training loss 2.7012659116369506\n",
      "Epoch 767 iteration 0 loss 0.20069453120231628\n",
      "Epoch 767 iteration 100 loss 0.662391722202301\n",
      "Epoch 767 iteration 200 loss 4.947536468505859\n",
      "Epoch 767 Training loss 2.7050094243104383\n",
      "Epoch 768 iteration 0 loss 0.2662767469882965\n",
      "Epoch 768 iteration 100 loss 0.7515646815299988\n",
      "Epoch 768 iteration 200 loss 5.060115814208984\n",
      "Epoch 768 Training loss 2.7214252688161853\n",
      "Epoch 769 iteration 0 loss 0.20487825572490692\n",
      "Epoch 769 iteration 100 loss 0.6829825639724731\n",
      "Epoch 769 iteration 200 loss 4.876423358917236\n",
      "Epoch 769 Training loss 2.716556868385203\n",
      "Epoch 770 iteration 0 loss 0.21071834862232208\n",
      "Epoch 770 iteration 100 loss 0.7388511896133423\n",
      "Epoch 770 iteration 200 loss 4.993120193481445\n",
      "Epoch 770 Training loss 2.710803035152063\n",
      "Evaluation loss 7.6499707777840475\n",
      "Epoch 771 iteration 0 loss 0.25530239939689636\n",
      "Epoch 771 iteration 100 loss 0.7128778100013733\n",
      "Epoch 771 iteration 200 loss 4.854604244232178\n",
      "Epoch 771 Training loss 2.7314362622410715\n",
      "Epoch 772 iteration 0 loss 0.2367365062236786\n",
      "Epoch 772 iteration 100 loss 0.7371383309364319\n",
      "Epoch 772 iteration 200 loss 4.981693267822266\n",
      "Epoch 772 Training loss 2.717399812882697\n",
      "Epoch 773 iteration 0 loss 0.23929253220558167\n",
      "Epoch 773 iteration 100 loss 0.6919379234313965\n",
      "Epoch 773 iteration 200 loss 4.999384880065918\n",
      "Epoch 773 Training loss 2.732107557765475\n",
      "Epoch 774 iteration 0 loss 0.2581295669078827\n",
      "Epoch 774 iteration 100 loss 0.7853905558586121\n",
      "Epoch 774 iteration 200 loss 5.004222393035889\n",
      "Epoch 774 Training loss 2.7488203511951936\n",
      "Epoch 775 iteration 0 loss 0.2099548876285553\n",
      "Epoch 775 iteration 100 loss 0.7222992181777954\n",
      "Epoch 775 iteration 200 loss 4.935898780822754\n",
      "Epoch 775 Training loss 2.7407436801035168\n",
      "Evaluation loss 7.616092898684861\n",
      "Epoch 776 iteration 0 loss 0.24752935767173767\n",
      "Epoch 776 iteration 100 loss 0.686417281627655\n",
      "Epoch 776 iteration 200 loss 4.979382514953613\n",
      "Epoch 776 Training loss 2.7634364482523286\n",
      "Epoch 777 iteration 0 loss 0.24489560723304749\n",
      "Epoch 777 iteration 100 loss 0.7486658096313477\n",
      "Epoch 777 iteration 200 loss 4.98960542678833\n",
      "Epoch 777 Training loss 2.765055789418294\n",
      "Epoch 778 iteration 0 loss 0.26423484086990356\n",
      "Epoch 778 iteration 100 loss 0.6980341076850891\n",
      "Epoch 778 iteration 200 loss 4.93475341796875\n",
      "Epoch 778 Training loss 2.776168018280336\n",
      "Epoch 779 iteration 0 loss 0.2457384169101715\n",
      "Epoch 779 iteration 100 loss 0.7194252610206604\n",
      "Epoch 779 iteration 200 loss 5.009822845458984\n",
      "Epoch 779 Training loss 2.770241889104043\n",
      "Epoch 780 iteration 0 loss 0.24183550477027893\n",
      "Epoch 780 iteration 100 loss 0.7082946300506592\n",
      "Epoch 780 iteration 200 loss 5.065445423126221\n",
      "Epoch 780 Training loss 2.784003953621342\n",
      "Evaluation loss 7.626279928734541\n",
      "Epoch 781 iteration 0 loss 0.28254443407058716\n",
      "Epoch 781 iteration 100 loss 0.7238443493843079\n",
      "Epoch 781 iteration 200 loss 5.153875827789307\n",
      "Epoch 781 Training loss 2.791605344557396\n",
      "Epoch 782 iteration 0 loss 0.263712614774704\n",
      "Epoch 782 iteration 100 loss 0.7115591168403625\n",
      "Epoch 782 iteration 200 loss 5.08356237411499\n",
      "Epoch 782 Training loss 2.795440759891087\n",
      "Epoch 783 iteration 0 loss 0.25366267561912537\n",
      "Epoch 783 iteration 100 loss 0.7133073806762695\n",
      "Epoch 783 iteration 200 loss 5.12180757522583\n",
      "Epoch 783 Training loss 2.801795063873028\n",
      "Epoch 784 iteration 0 loss 0.1935800164937973\n",
      "Epoch 784 iteration 100 loss 0.6854643225669861\n",
      "Epoch 784 iteration 200 loss 5.084273815155029\n",
      "Epoch 784 Training loss 2.8094009956352046\n",
      "Epoch 785 iteration 0 loss 0.2380208522081375\n",
      "Epoch 785 iteration 100 loss 0.7304887771606445\n",
      "Epoch 785 iteration 200 loss 5.234053134918213\n",
      "Epoch 785 Training loss 2.8084731843746886\n",
      "Evaluation loss 7.626274063852232\n",
      "Epoch 786 iteration 0 loss 0.25149205327033997\n",
      "Epoch 786 iteration 100 loss 0.7346593737602234\n",
      "Epoch 786 iteration 200 loss 5.176737308502197\n",
      "Epoch 786 Training loss 2.813557105412563\n",
      "Epoch 787 iteration 0 loss 0.2784213721752167\n",
      "Epoch 787 iteration 100 loss 0.6366855502128601\n",
      "Epoch 787 iteration 200 loss 5.128273963928223\n",
      "Epoch 787 Training loss 2.809242378014898\n",
      "Epoch 788 iteration 0 loss 0.2174731194972992\n",
      "Epoch 788 iteration 100 loss 0.7337485551834106\n",
      "Epoch 788 iteration 200 loss 4.99416446685791\n",
      "Epoch 788 Training loss 2.8119148073621667\n",
      "Epoch 789 iteration 0 loss 0.22962184250354767\n",
      "Epoch 789 iteration 100 loss 0.7290804386138916\n",
      "Epoch 789 iteration 200 loss 5.063942909240723\n",
      "Epoch 789 Training loss 2.8317662412048388\n",
      "Epoch 790 iteration 0 loss 0.27705448865890503\n",
      "Epoch 790 iteration 100 loss 0.754502534866333\n",
      "Epoch 790 iteration 200 loss 5.06820011138916\n",
      "Epoch 790 Training loss 2.8396406366221023\n",
      "Evaluation loss 7.6293962885721465\n",
      "Epoch 791 iteration 0 loss 0.22553327679634094\n",
      "Epoch 791 iteration 100 loss 0.751327395439148\n",
      "Epoch 791 iteration 200 loss 4.960508346557617\n",
      "Epoch 791 Training loss 2.81939489631788\n",
      "Epoch 792 iteration 0 loss 0.2868344783782959\n",
      "Epoch 792 iteration 100 loss 0.7739387154579163\n",
      "Epoch 792 iteration 200 loss 5.106172561645508\n",
      "Epoch 792 Training loss 2.8375706061630748\n",
      "Epoch 793 iteration 0 loss 0.23380827903747559\n",
      "Epoch 793 iteration 100 loss 0.7503805160522461\n",
      "Epoch 793 iteration 200 loss 5.00376033782959\n",
      "Epoch 793 Training loss 2.8368535605323353\n",
      "Epoch 794 iteration 0 loss 0.28006500005722046\n",
      "Epoch 794 iteration 100 loss 0.7416096925735474\n",
      "Epoch 794 iteration 200 loss 5.043656826019287\n",
      "Epoch 794 Training loss 2.843844533172958\n",
      "Epoch 795 iteration 0 loss 0.2609288990497589\n",
      "Epoch 795 iteration 100 loss 0.8051043152809143\n",
      "Epoch 795 iteration 200 loss 5.141073703765869\n",
      "Epoch 795 Training loss 2.8550212081247928\n",
      "Evaluation loss 7.6379408398445685\n",
      "Epoch 796 iteration 0 loss 0.22627265751361847\n",
      "Epoch 796 iteration 100 loss 0.7525469064712524\n",
      "Epoch 796 iteration 200 loss 5.173609256744385\n",
      "Epoch 796 Training loss 2.859479327280351\n",
      "Epoch 797 iteration 0 loss 0.31597191095352173\n",
      "Epoch 797 iteration 100 loss 0.7908456325531006\n",
      "Epoch 797 iteration 200 loss 5.106034755706787\n",
      "Epoch 797 Training loss 2.878458876758031\n",
      "Epoch 798 iteration 0 loss 0.2286570519208908\n",
      "Epoch 798 iteration 100 loss 0.8543639779090881\n",
      "Epoch 798 iteration 200 loss 5.233546257019043\n",
      "Epoch 798 Training loss 2.878221684312956\n",
      "Epoch 799 iteration 0 loss 0.26741737127304077\n",
      "Epoch 799 iteration 100 loss 0.8469838500022888\n",
      "Epoch 799 iteration 200 loss 5.073049545288086\n",
      "Epoch 799 Training loss 2.880096737874369\n",
      "Epoch 800 iteration 0 loss 0.27160751819610596\n",
      "Epoch 800 iteration 100 loss 0.8096326589584351\n",
      "Epoch 800 iteration 200 loss 5.194526672363281\n",
      "Epoch 800 Training loss 2.895835106114683\n",
      "Evaluation loss 7.629395078721211\n",
      "Epoch 801 iteration 0 loss 0.26619067788124084\n",
      "Epoch 801 iteration 100 loss 0.8471637964248657\n",
      "Epoch 801 iteration 200 loss 5.302732467651367\n",
      "Epoch 801 Training loss 2.8970033422306423\n",
      "Epoch 802 iteration 0 loss 0.2655893862247467\n",
      "Epoch 802 iteration 100 loss 0.8717464804649353\n",
      "Epoch 802 iteration 200 loss 5.206263065338135\n",
      "Epoch 802 Training loss 2.8970841496504316\n",
      "Epoch 803 iteration 0 loss 0.23421676456928253\n",
      "Epoch 803 iteration 100 loss 0.8007111549377441\n",
      "Epoch 803 iteration 200 loss 5.296757221221924\n",
      "Epoch 803 Training loss 2.9103337458635\n",
      "Epoch 804 iteration 0 loss 0.279254287481308\n",
      "Epoch 804 iteration 100 loss 0.7577155232429504\n",
      "Epoch 804 iteration 200 loss 5.284146308898926\n",
      "Epoch 804 Training loss 2.9078113690368896\n",
      "Epoch 805 iteration 0 loss 0.22225336730480194\n",
      "Epoch 805 iteration 100 loss 0.834877073764801\n",
      "Epoch 805 iteration 200 loss 5.185978889465332\n",
      "Epoch 805 Training loss 2.9167996420380384\n",
      "Evaluation loss 7.692084036501701\n",
      "Epoch 806 iteration 0 loss 0.24005484580993652\n",
      "Epoch 806 iteration 100 loss 0.8766074776649475\n",
      "Epoch 806 iteration 200 loss 5.099081516265869\n",
      "Epoch 806 Training loss 2.928552000774329\n",
      "Epoch 807 iteration 0 loss 0.3727734088897705\n",
      "Epoch 807 iteration 100 loss 0.8440607786178589\n",
      "Epoch 807 iteration 200 loss 5.146968364715576\n",
      "Epoch 807 Training loss 2.927884999760191\n",
      "Epoch 808 iteration 0 loss 0.267437219619751\n",
      "Epoch 808 iteration 100 loss 0.7959685921669006\n",
      "Epoch 808 iteration 200 loss 5.168850898742676\n",
      "Epoch 808 Training loss 2.943009052250322\n",
      "Epoch 809 iteration 0 loss 0.2620992362499237\n",
      "Epoch 809 iteration 100 loss 0.8394760489463806\n",
      "Epoch 809 iteration 200 loss 5.21071720123291\n",
      "Epoch 809 Training loss 2.9322581713217826\n",
      "Epoch 810 iteration 0 loss 0.23148170113563538\n",
      "Epoch 810 iteration 100 loss 0.8538015484809875\n",
      "Epoch 810 iteration 200 loss 5.244497299194336\n",
      "Epoch 810 Training loss 2.9452510593169094\n",
      "Evaluation loss 7.658878347177963\n",
      "Epoch 811 iteration 0 loss 0.297069787979126\n",
      "Epoch 811 iteration 100 loss 0.8555575609207153\n",
      "Epoch 811 iteration 200 loss 5.216811656951904\n",
      "Epoch 811 Training loss 2.952059330631677\n",
      "Epoch 812 iteration 0 loss 0.310119092464447\n",
      "Epoch 812 iteration 100 loss 0.8112034201622009\n",
      "Epoch 812 iteration 200 loss 5.155776500701904\n",
      "Epoch 812 Training loss 2.9585239454115726\n",
      "Epoch 813 iteration 0 loss 0.21955589950084686\n",
      "Epoch 813 iteration 100 loss 0.8713725209236145\n",
      "Epoch 813 iteration 200 loss 5.241645336151123\n",
      "Epoch 813 Training loss 2.9526593797729745\n",
      "Epoch 814 iteration 0 loss 0.2350979596376419\n",
      "Epoch 814 iteration 100 loss 0.7411673069000244\n",
      "Epoch 814 iteration 200 loss 5.372350215911865\n",
      "Epoch 814 Training loss 2.9609006842274495\n",
      "Epoch 815 iteration 0 loss 0.2787197232246399\n",
      "Epoch 815 iteration 100 loss 0.736172616481781\n",
      "Epoch 815 iteration 200 loss 5.32546854019165\n",
      "Epoch 815 Training loss 2.9750307428439338\n",
      "Evaluation loss 7.6848740958313355\n",
      "Epoch 816 iteration 0 loss 0.2518983483314514\n",
      "Epoch 816 iteration 100 loss 0.8483348488807678\n",
      "Epoch 816 iteration 200 loss 5.251884937286377\n",
      "Epoch 816 Training loss 2.9713968856304778\n",
      "Epoch 817 iteration 0 loss 0.23769652843475342\n",
      "Epoch 817 iteration 100 loss 0.8134443163871765\n",
      "Epoch 817 iteration 200 loss 5.320455074310303\n",
      "Epoch 817 Training loss 2.9814290044053777\n",
      "Epoch 818 iteration 0 loss 0.2095176875591278\n",
      "Epoch 818 iteration 100 loss 0.891723096370697\n",
      "Epoch 818 iteration 200 loss 5.313722610473633\n",
      "Epoch 818 Training loss 2.976314469162682\n",
      "Epoch 819 iteration 0 loss 0.27847492694854736\n",
      "Epoch 819 iteration 100 loss 0.8136756420135498\n",
      "Epoch 819 iteration 200 loss 5.38315486907959\n",
      "Epoch 819 Training loss 2.984144505148191\n",
      "Epoch 820 iteration 0 loss 0.2525901198387146\n",
      "Epoch 820 iteration 100 loss 0.779278039932251\n",
      "Epoch 820 iteration 200 loss 5.346853256225586\n",
      "Epoch 820 Training loss 2.9920208674507154\n",
      "Evaluation loss 7.727985992910818\n",
      "Epoch 821 iteration 0 loss 0.2920723557472229\n",
      "Epoch 821 iteration 100 loss 0.7497931718826294\n",
      "Epoch 821 iteration 200 loss 5.36329984664917\n",
      "Epoch 821 Training loss 3.0005700584495867\n",
      "Epoch 822 iteration 0 loss 0.31504201889038086\n",
      "Epoch 822 iteration 100 loss 0.8290351629257202\n",
      "Epoch 822 iteration 200 loss 5.34731912612915\n",
      "Epoch 822 Training loss 3.005574279456537\n",
      "Epoch 823 iteration 0 loss 0.3859056830406189\n",
      "Epoch 823 iteration 100 loss 0.8225294947624207\n",
      "Epoch 823 iteration 200 loss 5.328482627868652\n",
      "Epoch 823 Training loss 3.003123575765354\n",
      "Epoch 824 iteration 0 loss 0.3082764744758606\n",
      "Epoch 824 iteration 100 loss 0.9693710803985596\n",
      "Epoch 824 iteration 200 loss 5.311587333679199\n",
      "Epoch 824 Training loss 3.0128108123347626\n",
      "Epoch 825 iteration 0 loss 0.25042375922203064\n",
      "Epoch 825 iteration 100 loss 0.8580971956253052\n",
      "Epoch 825 iteration 200 loss 5.223452091217041\n",
      "Epoch 825 Training loss 3.008669845918313\n",
      "Evaluation loss 7.698047438631993\n",
      "Epoch 826 iteration 0 loss 0.31092557311058044\n",
      "Epoch 826 iteration 100 loss 0.9453939199447632\n",
      "Epoch 826 iteration 200 loss 5.293385028839111\n",
      "Epoch 826 Training loss 3.020448200070162\n",
      "Epoch 827 iteration 0 loss 0.29527121782302856\n",
      "Epoch 827 iteration 100 loss 0.9117093682289124\n",
      "Epoch 827 iteration 200 loss 5.390761852264404\n",
      "Epoch 827 Training loss 3.0356810821610862\n",
      "Epoch 828 iteration 0 loss 0.25070515275001526\n",
      "Epoch 828 iteration 100 loss 0.9461098909378052\n",
      "Epoch 828 iteration 200 loss 5.415475368499756\n",
      "Epoch 828 Training loss 3.0367294270228338\n",
      "Epoch 829 iteration 0 loss 0.2748599648475647\n",
      "Epoch 829 iteration 100 loss 0.8323588371276855\n",
      "Epoch 829 iteration 200 loss 5.338901996612549\n",
      "Epoch 829 Training loss 3.047538596852562\n",
      "Epoch 830 iteration 0 loss 0.2927708625793457\n",
      "Epoch 830 iteration 100 loss 0.8347230553627014\n",
      "Epoch 830 iteration 200 loss 5.228503227233887\n",
      "Epoch 830 Training loss 3.048243838414019\n",
      "Evaluation loss 7.727408298082645\n",
      "Epoch 831 iteration 0 loss 0.28074365854263306\n",
      "Epoch 831 iteration 100 loss 0.929833710193634\n",
      "Epoch 831 iteration 200 loss 5.407651901245117\n",
      "Epoch 831 Training loss 3.06327942723971\n",
      "Epoch 832 iteration 0 loss 0.24561412632465363\n",
      "Epoch 832 iteration 100 loss 1.002498745918274\n",
      "Epoch 832 iteration 200 loss 5.531054973602295\n",
      "Epoch 832 Training loss 3.0700140057720775\n",
      "Epoch 833 iteration 0 loss 0.23854045569896698\n",
      "Epoch 833 iteration 100 loss 0.9874306917190552\n",
      "Epoch 833 iteration 200 loss 5.445632457733154\n",
      "Epoch 833 Training loss 3.0678087619746526\n",
      "Epoch 834 iteration 0 loss 0.2489747703075409\n",
      "Epoch 834 iteration 100 loss 0.9939056038856506\n",
      "Epoch 834 iteration 200 loss 5.483868598937988\n",
      "Epoch 834 Training loss 3.087808362167176\n",
      "Epoch 835 iteration 0 loss 0.2454380989074707\n",
      "Epoch 835 iteration 100 loss 0.9291674494743347\n",
      "Epoch 835 iteration 200 loss 5.348501682281494\n",
      "Epoch 835 Training loss 3.093397752455264\n",
      "Evaluation loss 7.73367741623913\n",
      "Epoch 836 iteration 0 loss 0.21910460293293\n",
      "Epoch 836 iteration 100 loss 0.939532458782196\n",
      "Epoch 836 iteration 200 loss 5.430922508239746\n",
      "Epoch 836 Training loss 3.0882246563588316\n",
      "Epoch 837 iteration 0 loss 0.24696466326713562\n",
      "Epoch 837 iteration 100 loss 0.9523407816886902\n",
      "Epoch 837 iteration 200 loss 5.53192138671875\n",
      "Epoch 837 Training loss 3.0918360091453767\n",
      "Epoch 838 iteration 0 loss 0.25940650701522827\n",
      "Epoch 838 iteration 100 loss 0.9751560688018799\n",
      "Epoch 838 iteration 200 loss 5.557218551635742\n",
      "Epoch 838 Training loss 3.103227899277216\n",
      "Epoch 839 iteration 0 loss 0.32883182168006897\n",
      "Epoch 839 iteration 100 loss 0.919662356376648\n",
      "Epoch 839 iteration 200 loss 5.564216136932373\n",
      "Epoch 839 Training loss 3.1014921030990483\n",
      "Epoch 840 iteration 0 loss 0.24197335541248322\n",
      "Epoch 840 iteration 100 loss 1.0157157182693481\n",
      "Epoch 840 iteration 200 loss 5.507119655609131\n",
      "Epoch 840 Training loss 3.110170100404015\n",
      "Evaluation loss 7.764772297670947\n",
      "Epoch 841 iteration 0 loss 0.23776468634605408\n",
      "Epoch 841 iteration 100 loss 0.9571014642715454\n",
      "Epoch 841 iteration 200 loss 5.575018882751465\n",
      "Epoch 841 Training loss 3.117397279777392\n",
      "Epoch 842 iteration 0 loss 0.23261693120002747\n",
      "Epoch 842 iteration 100 loss 1.103978157043457\n",
      "Epoch 842 iteration 200 loss 5.588928699493408\n",
      "Epoch 842 Training loss 3.1284102785580856\n",
      "Epoch 843 iteration 0 loss 0.22481107711791992\n",
      "Epoch 843 iteration 100 loss 0.9767171740531921\n",
      "Epoch 843 iteration 200 loss 5.493811130523682\n",
      "Epoch 843 Training loss 3.1255524919412045\n",
      "Epoch 844 iteration 0 loss 0.21079377830028534\n",
      "Epoch 844 iteration 100 loss 0.923675537109375\n",
      "Epoch 844 iteration 200 loss 5.746683597564697\n",
      "Epoch 844 Training loss 3.136692337215501\n",
      "Epoch 845 iteration 0 loss 0.2903933823108673\n",
      "Epoch 845 iteration 100 loss 0.9953169822692871\n",
      "Epoch 845 iteration 200 loss 5.462260723114014\n",
      "Epoch 845 Training loss 3.137858879088665\n",
      "Evaluation loss 7.730874977535017\n",
      "Epoch 846 iteration 0 loss 0.22571638226509094\n",
      "Epoch 846 iteration 100 loss 0.9446935057640076\n",
      "Epoch 846 iteration 200 loss 5.644501209259033\n",
      "Epoch 846 Training loss 3.1429266460865177\n",
      "Epoch 847 iteration 0 loss 0.221582293510437\n",
      "Epoch 847 iteration 100 loss 1.017319679260254\n",
      "Epoch 847 iteration 200 loss 5.542546272277832\n",
      "Epoch 847 Training loss 3.141761239165419\n",
      "Epoch 848 iteration 0 loss 0.18945704400539398\n",
      "Epoch 848 iteration 100 loss 1.065529227256775\n",
      "Epoch 848 iteration 200 loss 5.555074214935303\n",
      "Epoch 848 Training loss 3.1560459389822304\n",
      "Epoch 849 iteration 0 loss 0.22829791903495789\n",
      "Epoch 849 iteration 100 loss 0.874565601348877\n",
      "Epoch 849 iteration 200 loss 5.578056812286377\n",
      "Epoch 849 Training loss 3.1639413290700986\n",
      "Epoch 850 iteration 0 loss 0.2505415081977844\n",
      "Epoch 850 iteration 100 loss 0.9271336793899536\n",
      "Epoch 850 iteration 200 loss 5.675865173339844\n",
      "Epoch 850 Training loss 3.1781595132487173\n",
      "Evaluation loss 7.756870253742486\n",
      "Epoch 851 iteration 0 loss 0.2574584186077118\n",
      "Epoch 851 iteration 100 loss 0.9568670392036438\n",
      "Epoch 851 iteration 200 loss 5.46103048324585\n",
      "Epoch 851 Training loss 3.1728182272194627\n",
      "Epoch 852 iteration 0 loss 0.23715460300445557\n",
      "Epoch 852 iteration 100 loss 0.9638193845748901\n",
      "Epoch 852 iteration 200 loss 5.78352689743042\n",
      "Epoch 852 Training loss 3.189626419949275\n",
      "Epoch 853 iteration 0 loss 0.2514323592185974\n",
      "Epoch 853 iteration 100 loss 0.8705022931098938\n",
      "Epoch 853 iteration 200 loss 5.6363205909729\n",
      "Epoch 853 Training loss 3.198762381593444\n",
      "Epoch 854 iteration 0 loss 0.31788721680641174\n",
      "Epoch 854 iteration 100 loss 0.8843816518783569\n",
      "Epoch 854 iteration 200 loss 5.638010501861572\n",
      "Epoch 854 Training loss 3.198693274922956\n",
      "Epoch 855 iteration 0 loss 0.2378043383359909\n",
      "Epoch 855 iteration 100 loss 0.9054737687110901\n",
      "Epoch 855 iteration 200 loss 5.597628593444824\n",
      "Epoch 855 Training loss 3.1893173756026405\n",
      "Evaluation loss 7.77285877748557\n",
      "Epoch 856 iteration 0 loss 0.31225523352622986\n",
      "Epoch 856 iteration 100 loss 0.9595938324928284\n",
      "Epoch 856 iteration 200 loss 5.649798393249512\n",
      "Epoch 856 Training loss 3.208785388093722\n",
      "Epoch 857 iteration 0 loss 0.25669652223587036\n",
      "Epoch 857 iteration 100 loss 0.951413094997406\n",
      "Epoch 857 iteration 200 loss 5.656007289886475\n",
      "Epoch 857 Training loss 3.212923490330395\n",
      "Epoch 858 iteration 0 loss 0.23562535643577576\n",
      "Epoch 858 iteration 100 loss 0.9573119878768921\n",
      "Epoch 858 iteration 200 loss 5.653498649597168\n",
      "Epoch 858 Training loss 3.21605959961858\n",
      "Epoch 859 iteration 0 loss 0.22950708866119385\n",
      "Epoch 859 iteration 100 loss 0.9767244458198547\n",
      "Epoch 859 iteration 200 loss 5.596655368804932\n",
      "Epoch 859 Training loss 3.2157981806852076\n",
      "Epoch 860 iteration 0 loss 0.2727019786834717\n",
      "Epoch 860 iteration 100 loss 0.919830858707428\n",
      "Epoch 860 iteration 200 loss 5.660027980804443\n",
      "Epoch 860 Training loss 3.2250236950463664\n",
      "Evaluation loss 7.791460715040881\n",
      "Epoch 861 iteration 0 loss 0.25628241896629333\n",
      "Epoch 861 iteration 100 loss 0.9799132347106934\n",
      "Epoch 861 iteration 200 loss 5.676719665527344\n",
      "Epoch 861 Training loss 3.217599326744307\n",
      "Epoch 862 iteration 0 loss 0.21653668582439423\n",
      "Epoch 862 iteration 100 loss 0.9110784530639648\n",
      "Epoch 862 iteration 200 loss 5.712594985961914\n",
      "Epoch 862 Training loss 3.225204034035761\n",
      "Epoch 863 iteration 0 loss 0.2584070563316345\n",
      "Epoch 863 iteration 100 loss 0.9686934351921082\n",
      "Epoch 863 iteration 200 loss 5.614730358123779\n",
      "Epoch 863 Training loss 3.235843240929284\n",
      "Epoch 864 iteration 0 loss 0.2851503789424896\n",
      "Epoch 864 iteration 100 loss 1.0315823554992676\n",
      "Epoch 864 iteration 200 loss 5.763624668121338\n",
      "Epoch 864 Training loss 3.2377626480303485\n",
      "Epoch 865 iteration 0 loss 0.3298487365245819\n",
      "Epoch 865 iteration 100 loss 0.9998993873596191\n",
      "Epoch 865 iteration 200 loss 5.770078182220459\n",
      "Epoch 865 Training loss 3.251409658430252\n",
      "Evaluation loss 7.790923045628926\n",
      "Epoch 866 iteration 0 loss 0.23788876831531525\n",
      "Epoch 866 iteration 100 loss 0.8928626179695129\n",
      "Epoch 866 iteration 200 loss 5.806331634521484\n",
      "Epoch 866 Training loss 3.2605509945293973\n",
      "Epoch 867 iteration 0 loss 0.32807457447052\n",
      "Epoch 867 iteration 100 loss 0.9706238508224487\n",
      "Epoch 867 iteration 200 loss 5.854722023010254\n",
      "Epoch 867 Training loss 3.2869022240351016\n",
      "Epoch 868 iteration 0 loss 0.24584007263183594\n",
      "Epoch 868 iteration 100 loss 0.9982374310493469\n",
      "Epoch 868 iteration 200 loss 5.701308250427246\n",
      "Epoch 868 Training loss 3.270616256806012\n",
      "Epoch 869 iteration 0 loss 0.28007394075393677\n",
      "Epoch 869 iteration 100 loss 1.0295326709747314\n",
      "Epoch 869 iteration 200 loss 5.768187046051025\n",
      "Epoch 869 Training loss 3.286100744108416\n",
      "Epoch 870 iteration 0 loss 0.32290756702423096\n",
      "Epoch 870 iteration 100 loss 1.086267352104187\n",
      "Epoch 870 iteration 200 loss 5.628513813018799\n",
      "Epoch 870 Training loss 3.292986762185761\n",
      "Evaluation loss 7.754376567122437\n",
      "Epoch 871 iteration 0 loss 0.2283822000026703\n",
      "Epoch 871 iteration 100 loss 1.0610097646713257\n",
      "Epoch 871 iteration 200 loss 6.019200325012207\n",
      "Epoch 871 Training loss 3.316377885520605\n",
      "Epoch 872 iteration 0 loss 0.2852499485015869\n",
      "Epoch 872 iteration 100 loss 1.0737534761428833\n",
      "Epoch 872 iteration 200 loss 5.884795665740967\n",
      "Epoch 872 Training loss 3.3160621083820887\n",
      "Epoch 873 iteration 0 loss 0.2816952168941498\n",
      "Epoch 873 iteration 100 loss 1.0498048067092896\n",
      "Epoch 873 iteration 200 loss 5.736349105834961\n",
      "Epoch 873 Training loss 3.3034553583882493\n",
      "Epoch 874 iteration 0 loss 0.21925799548625946\n",
      "Epoch 874 iteration 100 loss 1.0515921115875244\n",
      "Epoch 874 iteration 200 loss 5.825847625732422\n",
      "Epoch 874 Training loss 3.3143318456856026\n",
      "Epoch 875 iteration 0 loss 0.2653850018978119\n",
      "Epoch 875 iteration 100 loss 1.0654834508895874\n",
      "Epoch 875 iteration 200 loss 5.799175262451172\n",
      "Epoch 875 Training loss 3.3233471245245574\n",
      "Evaluation loss 7.792549948300728\n",
      "Epoch 876 iteration 0 loss 0.29619431495666504\n",
      "Epoch 876 iteration 100 loss 1.191006064414978\n",
      "Epoch 876 iteration 200 loss 5.960819721221924\n",
      "Epoch 876 Training loss 3.322895587164543\n",
      "Epoch 877 iteration 0 loss 0.2547706663608551\n",
      "Epoch 877 iteration 100 loss 1.1316965818405151\n",
      "Epoch 877 iteration 200 loss 6.012422561645508\n",
      "Epoch 877 Training loss 3.3217154591228497\n",
      "Epoch 878 iteration 0 loss 0.21729159355163574\n",
      "Epoch 878 iteration 100 loss 1.1961554288864136\n",
      "Epoch 878 iteration 200 loss 5.965329170227051\n",
      "Epoch 878 Training loss 3.3265172858500107\n",
      "Epoch 879 iteration 0 loss 0.28075864911079407\n",
      "Epoch 879 iteration 100 loss 1.0783679485321045\n",
      "Epoch 879 iteration 200 loss 5.887331008911133\n",
      "Epoch 879 Training loss 3.3193710483777723\n",
      "Epoch 880 iteration 0 loss 0.2952139973640442\n",
      "Epoch 880 iteration 100 loss 1.211259126663208\n",
      "Epoch 880 iteration 200 loss 6.002067565917969\n",
      "Epoch 880 Training loss 3.3305731214185825\n",
      "Evaluation loss 7.770595061841151\n",
      "Epoch 881 iteration 0 loss 0.2151133120059967\n",
      "Epoch 881 iteration 100 loss 1.1020971536636353\n",
      "Epoch 881 iteration 200 loss 5.901590824127197\n",
      "Epoch 881 Training loss 3.337858151061098\n",
      "Epoch 882 iteration 0 loss 0.22865895926952362\n",
      "Epoch 882 iteration 100 loss 1.0583981275558472\n",
      "Epoch 882 iteration 200 loss 5.958072662353516\n",
      "Epoch 882 Training loss 3.3457008638917523\n",
      "Epoch 883 iteration 0 loss 0.2621219754219055\n",
      "Epoch 883 iteration 100 loss 1.0561399459838867\n",
      "Epoch 883 iteration 200 loss 5.850785255432129\n",
      "Epoch 883 Training loss 3.34796756241401\n",
      "Epoch 884 iteration 0 loss 0.33053335547447205\n",
      "Epoch 884 iteration 100 loss 1.0173362493515015\n",
      "Epoch 884 iteration 200 loss 5.847365856170654\n",
      "Epoch 884 Training loss 3.3554961633542746\n",
      "Epoch 885 iteration 0 loss 0.2917446494102478\n",
      "Epoch 885 iteration 100 loss 1.0516282320022583\n",
      "Epoch 885 iteration 200 loss 5.765481472015381\n",
      "Epoch 885 Training loss 3.3449172878870326\n",
      "Evaluation loss 7.786561019277184\n",
      "Epoch 886 iteration 0 loss 0.34646719694137573\n",
      "Epoch 886 iteration 100 loss 1.1563068628311157\n",
      "Epoch 886 iteration 200 loss 5.919579982757568\n",
      "Epoch 886 Training loss 3.3636783842399964\n",
      "Epoch 887 iteration 0 loss 0.31221240758895874\n",
      "Epoch 887 iteration 100 loss 1.142384648323059\n",
      "Epoch 887 iteration 200 loss 5.776442050933838\n",
      "Epoch 887 Training loss 3.3748298176559013\n",
      "Epoch 888 iteration 0 loss 0.31167054176330566\n",
      "Epoch 888 iteration 100 loss 1.1455907821655273\n",
      "Epoch 888 iteration 200 loss 5.949645519256592\n",
      "Epoch 888 Training loss 3.372262745380922\n",
      "Epoch 889 iteration 0 loss 0.2610955834388733\n",
      "Epoch 889 iteration 100 loss 1.079755425453186\n",
      "Epoch 889 iteration 200 loss 5.946485996246338\n",
      "Epoch 889 Training loss 3.375796913667841\n",
      "Epoch 890 iteration 0 loss 0.3200663924217224\n",
      "Epoch 890 iteration 100 loss 1.1090322732925415\n",
      "Epoch 890 iteration 200 loss 5.875941276550293\n",
      "Epoch 890 Training loss 3.3821252392931886\n",
      "Evaluation loss 7.786614617916312\n",
      "Epoch 891 iteration 0 loss 0.28900596499443054\n",
      "Epoch 891 iteration 100 loss 1.0660425424575806\n",
      "Epoch 891 iteration 200 loss 5.919306755065918\n",
      "Epoch 891 Training loss 3.395495139007624\n",
      "Epoch 892 iteration 0 loss 0.247426837682724\n",
      "Epoch 892 iteration 100 loss 1.160757303237915\n",
      "Epoch 892 iteration 200 loss 5.934150218963623\n",
      "Epoch 892 Training loss 3.3890327308206962\n",
      "Epoch 893 iteration 0 loss 0.25332847237586975\n",
      "Epoch 893 iteration 100 loss 1.1090253591537476\n",
      "Epoch 893 iteration 200 loss 5.875131607055664\n",
      "Epoch 893 Training loss 3.3989974604741606\n",
      "Epoch 894 iteration 0 loss 0.2307819426059723\n",
      "Epoch 894 iteration 100 loss 0.9629777073860168\n",
      "Epoch 894 iteration 200 loss 5.941951751708984\n",
      "Epoch 894 Training loss 3.3998909953593963\n",
      "Epoch 895 iteration 0 loss 0.31108358502388\n",
      "Epoch 895 iteration 100 loss 1.1086723804473877\n",
      "Epoch 895 iteration 200 loss 5.902232646942139\n",
      "Epoch 895 Training loss 3.419126517873265\n",
      "Evaluation loss 7.800240072880487\n",
      "Epoch 896 iteration 0 loss 0.27813154458999634\n",
      "Epoch 896 iteration 100 loss 1.185742735862732\n",
      "Epoch 896 iteration 200 loss 5.990691661834717\n",
      "Epoch 896 Training loss 3.419410509747158\n",
      "Epoch 897 iteration 0 loss 0.2232740819454193\n",
      "Epoch 897 iteration 100 loss 1.157220482826233\n",
      "Epoch 897 iteration 200 loss 5.99907112121582\n",
      "Epoch 897 Training loss 3.4105085491392\n",
      "Epoch 898 iteration 0 loss 0.2361866980791092\n",
      "Epoch 898 iteration 100 loss 1.2130687236785889\n",
      "Epoch 898 iteration 200 loss 6.0732421875\n",
      "Epoch 898 Training loss 3.4310221749198098\n",
      "Epoch 899 iteration 0 loss 0.21027517318725586\n",
      "Epoch 899 iteration 100 loss 1.0827243328094482\n",
      "Epoch 899 iteration 200 loss 5.984271049499512\n",
      "Epoch 899 Training loss 3.4293800582660605\n",
      "Epoch 900 iteration 0 loss 0.3306693434715271\n",
      "Epoch 900 iteration 100 loss 1.1497684717178345\n",
      "Epoch 900 iteration 200 loss 6.010409355163574\n",
      "Epoch 900 Training loss 3.436589725806728\n",
      "Evaluation loss 7.781946254380951\n",
      "Epoch 901 iteration 0 loss 0.3507063090801239\n",
      "Epoch 901 iteration 100 loss 1.0931577682495117\n",
      "Epoch 901 iteration 200 loss 6.019762992858887\n",
      "Epoch 901 Training loss 3.4437153560062153\n",
      "Epoch 902 iteration 0 loss 0.26993727684020996\n",
      "Epoch 902 iteration 100 loss 1.3178989887237549\n",
      "Epoch 902 iteration 200 loss 6.152397632598877\n",
      "Epoch 902 Training loss 3.452606182943549\n",
      "Epoch 903 iteration 0 loss 0.2613076865673065\n",
      "Epoch 903 iteration 100 loss 1.249287486076355\n",
      "Epoch 903 iteration 200 loss 5.96854305267334\n",
      "Epoch 903 Training loss 3.4505351905396853\n",
      "Epoch 904 iteration 0 loss 0.22903737425804138\n",
      "Epoch 904 iteration 100 loss 1.3453716039657593\n",
      "Epoch 904 iteration 200 loss 6.006861686706543\n",
      "Epoch 904 Training loss 3.452051634847953\n",
      "Epoch 905 iteration 0 loss 0.2785637378692627\n",
      "Epoch 905 iteration 100 loss 1.2433663606643677\n",
      "Epoch 905 iteration 200 loss 6.101590156555176\n",
      "Epoch 905 Training loss 3.4609967748047334\n",
      "Evaluation loss 7.832152525000815\n",
      "Epoch 906 iteration 0 loss 0.2637164294719696\n",
      "Epoch 906 iteration 100 loss 1.1601825952529907\n",
      "Epoch 906 iteration 200 loss 5.984706878662109\n",
      "Epoch 906 Training loss 3.4770289329670105\n",
      "Epoch 907 iteration 0 loss 0.2811301350593567\n",
      "Epoch 907 iteration 100 loss 1.2072609663009644\n",
      "Epoch 907 iteration 200 loss 6.069623947143555\n",
      "Epoch 907 Training loss 3.4610747294768105\n",
      "Epoch 908 iteration 0 loss 0.2798028886318207\n",
      "Epoch 908 iteration 100 loss 1.2779135704040527\n",
      "Epoch 908 iteration 200 loss 6.109538555145264\n",
      "Epoch 908 Training loss 3.475812383997531\n",
      "Epoch 909 iteration 0 loss 0.24106240272521973\n",
      "Epoch 909 iteration 100 loss 1.3386214971542358\n",
      "Epoch 909 iteration 200 loss 5.991215229034424\n",
      "Epoch 909 Training loss 3.4744318065303212\n",
      "Epoch 910 iteration 0 loss 0.24812479317188263\n",
      "Epoch 910 iteration 100 loss 1.2954117059707642\n",
      "Epoch 910 iteration 200 loss 6.174459934234619\n",
      "Epoch 910 Training loss 3.472346617924748\n",
      "Evaluation loss 7.849212086430531\n",
      "Epoch 911 iteration 0 loss 0.3546350598335266\n",
      "Epoch 911 iteration 100 loss 1.314347267150879\n",
      "Epoch 911 iteration 200 loss 6.10563325881958\n",
      "Epoch 911 Training loss 3.489452443232166\n",
      "Epoch 912 iteration 0 loss 0.28445178270339966\n",
      "Epoch 912 iteration 100 loss 1.2360360622406006\n",
      "Epoch 912 iteration 200 loss 6.168084144592285\n",
      "Epoch 912 Training loss 3.492870414798204\n",
      "Epoch 913 iteration 0 loss 0.24063780903816223\n",
      "Epoch 913 iteration 100 loss 1.405287504196167\n",
      "Epoch 913 iteration 200 loss 6.172361373901367\n",
      "Epoch 913 Training loss 3.507451168939562\n",
      "Epoch 914 iteration 0 loss 0.29347604513168335\n",
      "Epoch 914 iteration 100 loss 1.2602181434631348\n",
      "Epoch 914 iteration 200 loss 6.121079444885254\n",
      "Epoch 914 Training loss 3.5000191729973578\n",
      "Epoch 915 iteration 0 loss 0.264728844165802\n",
      "Epoch 915 iteration 100 loss 1.2569177150726318\n",
      "Epoch 915 iteration 200 loss 6.074066162109375\n",
      "Epoch 915 Training loss 3.5103168642569593\n",
      "Evaluation loss 7.843228331571503\n",
      "Epoch 916 iteration 0 loss 0.2665998339653015\n",
      "Epoch 916 iteration 100 loss 1.3581292629241943\n",
      "Epoch 916 iteration 200 loss 6.135214328765869\n",
      "Epoch 916 Training loss 3.5001108196710207\n",
      "Epoch 917 iteration 0 loss 0.27022016048431396\n",
      "Epoch 917 iteration 100 loss 1.3561855554580688\n",
      "Epoch 917 iteration 200 loss 6.048802852630615\n",
      "Epoch 917 Training loss 3.515158524061056\n",
      "Epoch 918 iteration 0 loss 0.3131083846092224\n",
      "Epoch 918 iteration 100 loss 1.3604825735092163\n",
      "Epoch 918 iteration 200 loss 6.151387691497803\n",
      "Epoch 918 Training loss 3.5141649381527382\n",
      "Epoch 919 iteration 0 loss 0.30128297209739685\n",
      "Epoch 919 iteration 100 loss 1.268980622291565\n",
      "Epoch 919 iteration 200 loss 6.114238262176514\n",
      "Epoch 919 Training loss 3.509559510647202\n",
      "Epoch 920 iteration 0 loss 0.2713450789451599\n",
      "Epoch 920 iteration 100 loss 1.2520313262939453\n",
      "Epoch 920 iteration 200 loss 6.14626407623291\n",
      "Epoch 920 Training loss 3.5287460673277304\n",
      "Evaluation loss 7.7952842452666875\n",
      "Epoch 921 iteration 0 loss 0.31842243671417236\n",
      "Epoch 921 iteration 100 loss 1.243208885192871\n",
      "Epoch 921 iteration 200 loss 6.18541955947876\n",
      "Epoch 921 Training loss 3.535631789588014\n",
      "Epoch 922 iteration 0 loss 0.3053080439567566\n",
      "Epoch 922 iteration 100 loss 1.3159689903259277\n",
      "Epoch 922 iteration 200 loss 6.221656322479248\n",
      "Epoch 922 Training loss 3.5489672462357578\n",
      "Epoch 923 iteration 0 loss 0.30503708124160767\n",
      "Epoch 923 iteration 100 loss 1.255678415298462\n",
      "Epoch 923 iteration 200 loss 6.105188846588135\n",
      "Epoch 923 Training loss 3.549109922041607\n",
      "Epoch 924 iteration 0 loss 0.28954270482063293\n",
      "Epoch 924 iteration 100 loss 1.4399527311325073\n",
      "Epoch 924 iteration 200 loss 6.190775394439697\n",
      "Epoch 924 Training loss 3.5459986039158844\n",
      "Epoch 925 iteration 0 loss 0.28759557008743286\n",
      "Epoch 925 iteration 100 loss 1.3709256649017334\n",
      "Epoch 925 iteration 200 loss 6.310752868652344\n",
      "Epoch 925 Training loss 3.5456840373964424\n",
      "Evaluation loss 7.833235246717873\n",
      "Epoch 926 iteration 0 loss 0.32588809728622437\n",
      "Epoch 926 iteration 100 loss 1.2957403659820557\n",
      "Epoch 926 iteration 200 loss 6.132765769958496\n",
      "Epoch 926 Training loss 3.554385473733763\n",
      "Epoch 927 iteration 0 loss 0.2670239508152008\n",
      "Epoch 927 iteration 100 loss 1.374521255493164\n",
      "Epoch 927 iteration 200 loss 6.202563285827637\n",
      "Epoch 927 Training loss 3.575452050312494\n",
      "Epoch 928 iteration 0 loss 0.25338655710220337\n",
      "Epoch 928 iteration 100 loss 1.2974400520324707\n",
      "Epoch 928 iteration 200 loss 6.140842914581299\n",
      "Epoch 928 Training loss 3.5796325660216346\n",
      "Epoch 929 iteration 0 loss 0.3775179088115692\n",
      "Epoch 929 iteration 100 loss 1.3386774063110352\n",
      "Epoch 929 iteration 200 loss 6.341127872467041\n",
      "Epoch 929 Training loss 3.5775633597431757\n",
      "Epoch 930 iteration 0 loss 0.2841555178165436\n",
      "Epoch 930 iteration 100 loss 1.3021740913391113\n",
      "Epoch 930 iteration 200 loss 6.188286304473877\n",
      "Epoch 930 Training loss 3.5861684011414985\n",
      "Evaluation loss 7.808654899111717\n",
      "Epoch 931 iteration 0 loss 0.2749682366847992\n",
      "Epoch 931 iteration 100 loss 1.242118000984192\n",
      "Epoch 931 iteration 200 loss 6.154987812042236\n",
      "Epoch 931 Training loss 3.593043649652594\n",
      "Epoch 932 iteration 0 loss 0.29049044847488403\n",
      "Epoch 932 iteration 100 loss 1.283141016960144\n",
      "Epoch 932 iteration 200 loss 6.09710693359375\n",
      "Epoch 932 Training loss 3.599287008522119\n",
      "Epoch 933 iteration 0 loss 0.3008004128932953\n",
      "Epoch 933 iteration 100 loss 1.241941213607788\n",
      "Epoch 933 iteration 200 loss 6.276880741119385\n",
      "Epoch 933 Training loss 3.6141336366297026\n",
      "Epoch 934 iteration 0 loss 0.26874756813049316\n",
      "Epoch 934 iteration 100 loss 1.2209213972091675\n",
      "Epoch 934 iteration 200 loss 6.370150566101074\n",
      "Epoch 934 Training loss 3.606163219828017\n",
      "Epoch 935 iteration 0 loss 0.326540470123291\n",
      "Epoch 935 iteration 100 loss 1.3475499153137207\n",
      "Epoch 935 iteration 200 loss 6.204109191894531\n",
      "Epoch 935 Training loss 3.610671590623801\n",
      "Evaluation loss 7.856387031625436\n",
      "Epoch 936 iteration 0 loss 0.3604338765144348\n",
      "Epoch 936 iteration 100 loss 1.3895288705825806\n",
      "Epoch 936 iteration 200 loss 6.311610698699951\n",
      "Epoch 936 Training loss 3.6191420744112226\n",
      "Epoch 937 iteration 0 loss 0.38333994150161743\n",
      "Epoch 937 iteration 100 loss 1.2923290729522705\n",
      "Epoch 937 iteration 200 loss 6.20702600479126\n",
      "Epoch 937 Training loss 3.6246984621945213\n",
      "Epoch 938 iteration 0 loss 0.3002660274505615\n",
      "Epoch 938 iteration 100 loss 1.399157166481018\n",
      "Epoch 938 iteration 200 loss 6.322967529296875\n",
      "Epoch 938 Training loss 3.6395179508391906\n",
      "Epoch 939 iteration 0 loss 0.29161280393600464\n",
      "Epoch 939 iteration 100 loss 1.4627834558486938\n",
      "Epoch 939 iteration 200 loss 6.30471658706665\n",
      "Epoch 939 Training loss 3.65253952854847\n",
      "Epoch 940 iteration 0 loss 0.32220447063446045\n",
      "Epoch 940 iteration 100 loss 1.3795965909957886\n",
      "Epoch 940 iteration 200 loss 6.519446849822998\n",
      "Epoch 940 Training loss 3.6433710082193325\n",
      "Evaluation loss 7.813326967442343\n",
      "Epoch 941 iteration 0 loss 0.3202276825904846\n",
      "Epoch 941 iteration 100 loss 1.406476616859436\n",
      "Epoch 941 iteration 200 loss 6.266691207885742\n",
      "Epoch 941 Training loss 3.649972809517834\n",
      "Epoch 942 iteration 0 loss 0.30076321959495544\n",
      "Epoch 942 iteration 100 loss 1.3943661451339722\n",
      "Epoch 942 iteration 200 loss 6.269742488861084\n",
      "Epoch 942 Training loss 3.6729623417460715\n",
      "Epoch 943 iteration 0 loss 0.2522067427635193\n",
      "Epoch 943 iteration 100 loss 1.4246702194213867\n",
      "Epoch 943 iteration 200 loss 6.238650321960449\n",
      "Epoch 943 Training loss 3.673069274906999\n",
      "Epoch 944 iteration 0 loss 0.2958112061023712\n",
      "Epoch 944 iteration 100 loss 1.3898261785507202\n",
      "Epoch 944 iteration 200 loss 6.395278453826904\n",
      "Epoch 944 Training loss 3.6756712176210242\n",
      "Epoch 945 iteration 0 loss 0.32655391097068787\n",
      "Epoch 945 iteration 100 loss 1.2017103433609009\n",
      "Epoch 945 iteration 200 loss 6.422680854797363\n",
      "Epoch 945 Training loss 3.681281771114778\n",
      "Evaluation loss 7.860825274122489\n",
      "Epoch 946 iteration 0 loss 0.29461735486984253\n",
      "Epoch 946 iteration 100 loss 1.3189952373504639\n",
      "Epoch 946 iteration 200 loss 6.41142463684082\n",
      "Epoch 946 Training loss 3.6756486933571333\n",
      "Epoch 947 iteration 0 loss 0.36987870931625366\n",
      "Epoch 947 iteration 100 loss 1.3781861066818237\n",
      "Epoch 947 iteration 200 loss 6.36060094833374\n",
      "Epoch 947 Training loss 3.685328001056283\n",
      "Epoch 948 iteration 0 loss 0.3635813891887665\n",
      "Epoch 948 iteration 100 loss 1.3676645755767822\n",
      "Epoch 948 iteration 200 loss 6.389153003692627\n",
      "Epoch 948 Training loss 3.6823733643076735\n",
      "Epoch 949 iteration 0 loss 0.28333795070648193\n",
      "Epoch 949 iteration 100 loss 1.3402680158615112\n",
      "Epoch 949 iteration 200 loss 6.376843452453613\n",
      "Epoch 949 Training loss 3.684489010955383\n",
      "Epoch 950 iteration 0 loss 0.3313864767551422\n",
      "Epoch 950 iteration 100 loss 1.3549985885620117\n",
      "Epoch 950 iteration 200 loss 6.393624782562256\n",
      "Epoch 950 Training loss 3.705036692676686\n",
      "Evaluation loss 7.804194191294402\n",
      "Epoch 951 iteration 0 loss 0.43296557664871216\n",
      "Epoch 951 iteration 100 loss 1.327006220817566\n",
      "Epoch 951 iteration 200 loss 6.369124412536621\n",
      "Epoch 951 Training loss 3.6917579221672074\n",
      "Epoch 952 iteration 0 loss 0.40993666648864746\n",
      "Epoch 952 iteration 100 loss 1.3469154834747314\n",
      "Epoch 952 iteration 200 loss 6.524555683135986\n",
      "Epoch 952 Training loss 3.7064517930339242\n",
      "Epoch 953 iteration 0 loss 0.3099632263183594\n",
      "Epoch 953 iteration 100 loss 1.2650575637817383\n",
      "Epoch 953 iteration 200 loss 6.380863666534424\n",
      "Epoch 953 Training loss 3.704513615520181\n",
      "Epoch 954 iteration 0 loss 0.3232514262199402\n",
      "Epoch 954 iteration 100 loss 1.320009708404541\n",
      "Epoch 954 iteration 200 loss 6.321005344390869\n",
      "Epoch 954 Training loss 3.714644759451831\n",
      "Epoch 955 iteration 0 loss 0.35031822323799133\n",
      "Epoch 955 iteration 100 loss 1.3875761032104492\n",
      "Epoch 955 iteration 200 loss 6.384636878967285\n",
      "Epoch 955 Training loss 3.704682709279297\n",
      "Evaluation loss 7.818399994647051\n",
      "Epoch 956 iteration 0 loss 0.296941876411438\n",
      "Epoch 956 iteration 100 loss 1.3150280714035034\n",
      "Epoch 956 iteration 200 loss 6.507747173309326\n",
      "Epoch 956 Training loss 3.7238024743295766\n",
      "Epoch 957 iteration 0 loss 0.3330080211162567\n",
      "Epoch 957 iteration 100 loss 1.4264825582504272\n",
      "Epoch 957 iteration 200 loss 6.467574596405029\n",
      "Epoch 957 Training loss 3.725459999056166\n",
      "Epoch 958 iteration 0 loss 0.3246088922023773\n",
      "Epoch 958 iteration 100 loss 1.3536359071731567\n",
      "Epoch 958 iteration 200 loss 6.358860492706299\n",
      "Epoch 958 Training loss 3.7334601546006954\n",
      "Epoch 959 iteration 0 loss 0.3083607256412506\n",
      "Epoch 959 iteration 100 loss 1.3712494373321533\n",
      "Epoch 959 iteration 200 loss 6.503599166870117\n",
      "Epoch 959 Training loss 3.7528636987099047\n",
      "Epoch 960 iteration 0 loss 0.30269214510917664\n",
      "Epoch 960 iteration 100 loss 1.3135125637054443\n",
      "Epoch 960 iteration 200 loss 6.457279682159424\n",
      "Epoch 960 Training loss 3.755129004324406\n",
      "Evaluation loss 7.834036826918508\n",
      "Epoch 961 iteration 0 loss 0.2698691785335541\n",
      "Epoch 961 iteration 100 loss 1.4631049633026123\n",
      "Epoch 961 iteration 200 loss 6.321906566619873\n",
      "Epoch 961 Training loss 3.7613490496683855\n",
      "Epoch 962 iteration 0 loss 0.32514649629592896\n",
      "Epoch 962 iteration 100 loss 1.4005420207977295\n",
      "Epoch 962 iteration 200 loss 6.661555290222168\n",
      "Epoch 962 Training loss 3.75920654836295\n",
      "Epoch 963 iteration 0 loss 0.28294262290000916\n",
      "Epoch 963 iteration 100 loss 1.3817301988601685\n",
      "Epoch 963 iteration 200 loss 6.499973297119141\n",
      "Epoch 963 Training loss 3.7648465882682327\n",
      "Epoch 964 iteration 0 loss 0.2906293272972107\n",
      "Epoch 964 iteration 100 loss 1.4379836320877075\n",
      "Epoch 964 iteration 200 loss 6.436890125274658\n",
      "Epoch 964 Training loss 3.779432862715289\n",
      "Epoch 965 iteration 0 loss 0.34984907507896423\n",
      "Epoch 965 iteration 100 loss 1.3835558891296387\n",
      "Epoch 965 iteration 200 loss 6.417696475982666\n",
      "Epoch 965 Training loss 3.778377169169352\n",
      "Evaluation loss 7.834324048512837\n",
      "Epoch 966 iteration 0 loss 0.3654645085334778\n",
      "Epoch 966 iteration 100 loss 1.4431638717651367\n",
      "Epoch 966 iteration 200 loss 6.435330390930176\n",
      "Epoch 966 Training loss 3.772576996902836\n",
      "Epoch 967 iteration 0 loss 0.2760300040245056\n",
      "Epoch 967 iteration 100 loss 1.3995317220687866\n",
      "Epoch 967 iteration 200 loss 6.359409332275391\n",
      "Epoch 967 Training loss 3.773321360585749\n",
      "Epoch 968 iteration 0 loss 0.2539975941181183\n",
      "Epoch 968 iteration 100 loss 1.4109185934066772\n",
      "Epoch 968 iteration 200 loss 6.428811073303223\n",
      "Epoch 968 Training loss 3.7839659639341265\n",
      "Epoch 969 iteration 0 loss 0.3510122299194336\n",
      "Epoch 969 iteration 100 loss 1.445222020149231\n",
      "Epoch 969 iteration 200 loss 6.549422740936279\n",
      "Epoch 969 Training loss 3.791101659951498\n",
      "Epoch 970 iteration 0 loss 0.25571438670158386\n",
      "Epoch 970 iteration 100 loss 1.3880133628845215\n",
      "Epoch 970 iteration 200 loss 6.612664222717285\n",
      "Epoch 970 Training loss 3.7800695067151224\n",
      "Evaluation loss 7.825516913913495\n",
      "Epoch 971 iteration 0 loss 0.30429965257644653\n",
      "Epoch 971 iteration 100 loss 1.4979175329208374\n",
      "Epoch 971 iteration 200 loss 6.50934362411499\n",
      "Epoch 971 Training loss 3.7864451681013493\n",
      "Epoch 972 iteration 0 loss 0.31778281927108765\n",
      "Epoch 972 iteration 100 loss 1.470579981803894\n",
      "Epoch 972 iteration 200 loss 6.598084926605225\n",
      "Epoch 972 Training loss 3.789121981889461\n",
      "Epoch 973 iteration 0 loss 0.32527440786361694\n",
      "Epoch 973 iteration 100 loss 1.501595377922058\n",
      "Epoch 973 iteration 200 loss 6.515960216522217\n",
      "Epoch 973 Training loss 3.8041918019293908\n",
      "Epoch 974 iteration 0 loss 0.3425058424472809\n",
      "Epoch 974 iteration 100 loss 1.4680275917053223\n",
      "Epoch 974 iteration 200 loss 6.622531890869141\n",
      "Epoch 974 Training loss 3.8093833336118568\n",
      "Epoch 975 iteration 0 loss 0.30543631315231323\n",
      "Epoch 975 iteration 100 loss 1.505776047706604\n",
      "Epoch 975 iteration 200 loss 6.487919807434082\n",
      "Epoch 975 Training loss 3.8201976409814327\n",
      "Evaluation loss 7.874340438157545\n",
      "Epoch 976 iteration 0 loss 0.2905494272708893\n",
      "Epoch 976 iteration 100 loss 1.4653887748718262\n",
      "Epoch 976 iteration 200 loss 6.687248706817627\n",
      "Epoch 976 Training loss 3.828202836626129\n",
      "Epoch 977 iteration 0 loss 0.285706102848053\n",
      "Epoch 977 iteration 100 loss 1.3849842548370361\n",
      "Epoch 977 iteration 200 loss 6.465397834777832\n",
      "Epoch 977 Training loss 3.8352953175727382\n",
      "Epoch 978 iteration 0 loss 0.326532244682312\n",
      "Epoch 978 iteration 100 loss 1.338074803352356\n",
      "Epoch 978 iteration 200 loss 6.661466121673584\n",
      "Epoch 978 Training loss 3.84729234333402\n",
      "Epoch 979 iteration 0 loss 0.271850049495697\n",
      "Epoch 979 iteration 100 loss 1.5046547651290894\n",
      "Epoch 979 iteration 200 loss 6.464932441711426\n",
      "Epoch 979 Training loss 3.8394082006954355\n",
      "Epoch 980 iteration 0 loss 0.28478267788887024\n",
      "Epoch 980 iteration 100 loss 1.406197190284729\n",
      "Epoch 980 iteration 200 loss 6.697767734527588\n",
      "Epoch 980 Training loss 3.835051026504411\n",
      "Evaluation loss 7.826019892788835\n",
      "Epoch 981 iteration 0 loss 0.341633141040802\n",
      "Epoch 981 iteration 100 loss 1.3756784200668335\n",
      "Epoch 981 iteration 200 loss 6.631746292114258\n",
      "Epoch 981 Training loss 3.8520183312657936\n",
      "Epoch 982 iteration 0 loss 0.30588531494140625\n",
      "Epoch 982 iteration 100 loss 1.4589660167694092\n",
      "Epoch 982 iteration 200 loss 6.546489715576172\n",
      "Epoch 982 Training loss 3.8582954644056335\n",
      "Epoch 983 iteration 0 loss 0.29703009128570557\n",
      "Epoch 983 iteration 100 loss 1.4405126571655273\n",
      "Epoch 983 iteration 200 loss 6.489341735839844\n",
      "Epoch 983 Training loss 3.8556601608562393\n",
      "Epoch 984 iteration 0 loss 0.33940252661705017\n",
      "Epoch 984 iteration 100 loss 1.469818115234375\n",
      "Epoch 984 iteration 200 loss 6.558821678161621\n",
      "Epoch 984 Training loss 3.8542045081109455\n",
      "Epoch 985 iteration 0 loss 0.3382653594017029\n",
      "Epoch 985 iteration 100 loss 1.5275251865386963\n",
      "Epoch 985 iteration 200 loss 6.608643054962158\n",
      "Epoch 985 Training loss 3.863115946758893\n",
      "Evaluation loss 7.8731961862306115\n",
      "Epoch 986 iteration 0 loss 0.3107369840145111\n",
      "Epoch 986 iteration 100 loss 1.3725014925003052\n",
      "Epoch 986 iteration 200 loss 6.592044830322266\n",
      "Epoch 986 Training loss 3.8558915775358797\n",
      "Epoch 987 iteration 0 loss 0.2799935042858124\n",
      "Epoch 987 iteration 100 loss 1.5195999145507812\n",
      "Epoch 987 iteration 200 loss 6.658606052398682\n",
      "Epoch 987 Training loss 3.8650971166738586\n",
      "Epoch 988 iteration 0 loss 0.3070877492427826\n",
      "Epoch 988 iteration 100 loss 1.52006995677948\n",
      "Epoch 988 iteration 200 loss 6.588397979736328\n",
      "Epoch 988 Training loss 3.8613780983187014\n",
      "Epoch 989 iteration 0 loss 0.3076566457748413\n",
      "Epoch 989 iteration 100 loss 1.461946964263916\n",
      "Epoch 989 iteration 200 loss 6.693785190582275\n",
      "Epoch 989 Training loss 3.881091599202531\n",
      "Epoch 990 iteration 0 loss 0.2967430055141449\n",
      "Epoch 990 iteration 100 loss 1.4424331188201904\n",
      "Epoch 990 iteration 200 loss 6.590707778930664\n",
      "Epoch 990 Training loss 3.883190886734303\n",
      "Evaluation loss 7.898196952301849\n",
      "Epoch 991 iteration 0 loss 0.32967644929885864\n",
      "Epoch 991 iteration 100 loss 1.417718768119812\n",
      "Epoch 991 iteration 200 loss 6.588742256164551\n",
      "Epoch 991 Training loss 3.8917616168220492\n",
      "Epoch 992 iteration 0 loss 0.35416632890701294\n",
      "Epoch 992 iteration 100 loss 1.4810158014297485\n",
      "Epoch 992 iteration 200 loss 6.695648193359375\n",
      "Epoch 992 Training loss 3.8915037240188015\n",
      "Epoch 993 iteration 0 loss 0.3203432559967041\n",
      "Epoch 993 iteration 100 loss 1.4938548803329468\n",
      "Epoch 993 iteration 200 loss 6.600031852722168\n",
      "Epoch 993 Training loss 3.8927184227209803\n",
      "Epoch 994 iteration 0 loss 0.28994816541671753\n",
      "Epoch 994 iteration 100 loss 1.5137064456939697\n",
      "Epoch 994 iteration 200 loss 6.659825801849365\n",
      "Epoch 994 Training loss 3.8913476402209097\n",
      "Epoch 995 iteration 0 loss 0.335571825504303\n",
      "Epoch 995 iteration 100 loss 1.490797758102417\n",
      "Epoch 995 iteration 200 loss 6.72170877456665\n",
      "Epoch 995 Training loss 3.8966198065286695\n",
      "Evaluation loss 7.891493787832095\n",
      "Epoch 996 iteration 0 loss 0.35098913311958313\n",
      "Epoch 996 iteration 100 loss 1.5402085781097412\n",
      "Epoch 996 iteration 200 loss 6.654894828796387\n",
      "Epoch 996 Training loss 3.892152999355429\n",
      "Epoch 997 iteration 0 loss 0.37574759125709534\n",
      "Epoch 997 iteration 100 loss 1.494118571281433\n",
      "Epoch 997 iteration 200 loss 6.718432903289795\n",
      "Epoch 997 Training loss 3.912564224913801\n",
      "Epoch 998 iteration 0 loss 0.3713027834892273\n",
      "Epoch 998 iteration 100 loss 1.5805460214614868\n",
      "Epoch 998 iteration 200 loss 6.720317840576172\n",
      "Epoch 998 Training loss 3.9089457463237265\n",
      "Epoch 999 iteration 0 loss 0.3985947370529175\n",
      "Epoch 999 iteration 100 loss 1.5001252889633179\n",
      "Epoch 999 iteration 200 loss 6.699394226074219\n",
      "Epoch 999 Training loss 3.92151061459327\n",
      "Epoch 1000 iteration 0 loss 0.47797971963882446\n",
      "Epoch 1000 iteration 100 loss 1.4259123802185059\n",
      "Epoch 1000 iteration 200 loss 6.672759532928467\n",
      "Epoch 1000 Training loss 3.9239562663723864\n",
      "Evaluation loss 7.846836040903427\n",
      "Epoch 1001 iteration 0 loss 0.3710941672325134\n",
      "Epoch 1001 iteration 100 loss 1.6320255994796753\n",
      "Epoch 1001 iteration 200 loss 6.836629390716553\n",
      "Epoch 1001 Training loss 3.922068417294662\n",
      "Epoch 1002 iteration 0 loss 0.36885493993759155\n",
      "Epoch 1002 iteration 100 loss 1.5766651630401611\n",
      "Epoch 1002 iteration 200 loss 6.572016716003418\n",
      "Epoch 1002 Training loss 3.919800229729518\n",
      "Epoch 1003 iteration 0 loss 0.2992718815803528\n",
      "Epoch 1003 iteration 100 loss 1.6300253868103027\n",
      "Epoch 1003 iteration 200 loss 6.670182704925537\n",
      "Epoch 1003 Training loss 3.9166951724601655\n",
      "Epoch 1004 iteration 0 loss 0.3200015127658844\n",
      "Epoch 1004 iteration 100 loss 1.592720627784729\n",
      "Epoch 1004 iteration 200 loss 6.599545955657959\n",
      "Epoch 1004 Training loss 3.9351274685365536\n",
      "Epoch 1005 iteration 0 loss 0.3711393475532532\n",
      "Epoch 1005 iteration 100 loss 1.6849935054779053\n",
      "Epoch 1005 iteration 200 loss 6.660747528076172\n",
      "Epoch 1005 Training loss 3.9384395270306145\n",
      "Evaluation loss 7.8780172332146385\n",
      "Epoch 1006 iteration 0 loss 0.316972553730011\n",
      "Epoch 1006 iteration 100 loss 1.572550654411316\n",
      "Epoch 1006 iteration 200 loss 6.793471336364746\n",
      "Epoch 1006 Training loss 3.9346777289031833\n",
      "Epoch 1007 iteration 0 loss 0.29402387142181396\n",
      "Epoch 1007 iteration 100 loss 1.5562626123428345\n",
      "Epoch 1007 iteration 200 loss 6.851095676422119\n",
      "Epoch 1007 Training loss 3.9437644933419906\n",
      "Epoch 1008 iteration 0 loss 0.31282931566238403\n",
      "Epoch 1008 iteration 100 loss 1.6000862121582031\n",
      "Epoch 1008 iteration 200 loss 6.775860786437988\n",
      "Epoch 1008 Training loss 3.936494357968198\n",
      "Epoch 1009 iteration 0 loss 0.35331887006759644\n",
      "Epoch 1009 iteration 100 loss 1.5665645599365234\n",
      "Epoch 1009 iteration 200 loss 6.81456184387207\n",
      "Epoch 1009 Training loss 3.9536558936484316\n",
      "Epoch 1010 iteration 0 loss 0.33403992652893066\n",
      "Epoch 1010 iteration 100 loss 1.533699870109558\n",
      "Epoch 1010 iteration 200 loss 6.615450859069824\n",
      "Epoch 1010 Training loss 3.9561697229023847\n",
      "Evaluation loss 7.8633505831072155\n",
      "Epoch 1011 iteration 0 loss 0.36224955320358276\n",
      "Epoch 1011 iteration 100 loss 1.5156900882720947\n",
      "Epoch 1011 iteration 200 loss 6.724059581756592\n",
      "Epoch 1011 Training loss 3.9572190230055755\n",
      "Epoch 1012 iteration 0 loss 0.30818191170692444\n",
      "Epoch 1012 iteration 100 loss 1.5794172286987305\n",
      "Epoch 1012 iteration 200 loss 6.814991474151611\n",
      "Epoch 1012 Training loss 3.9608514044946337\n",
      "Epoch 1013 iteration 0 loss 0.4297342896461487\n",
      "Epoch 1013 iteration 100 loss 1.5108612775802612\n",
      "Epoch 1013 iteration 200 loss 6.785892963409424\n",
      "Epoch 1013 Training loss 3.973276012749825\n",
      "Epoch 1014 iteration 0 loss 0.289226233959198\n",
      "Epoch 1014 iteration 100 loss 1.5502251386642456\n",
      "Epoch 1014 iteration 200 loss 6.654461860656738\n",
      "Epoch 1014 Training loss 3.967679251076614\n",
      "Epoch 1015 iteration 0 loss 0.3752448558807373\n",
      "Epoch 1015 iteration 100 loss 1.6018282175064087\n",
      "Epoch 1015 iteration 200 loss 6.794147491455078\n",
      "Epoch 1015 Training loss 3.982924166896721\n",
      "Evaluation loss 7.886668070724774\n",
      "Epoch 1016 iteration 0 loss 0.3732760548591614\n",
      "Epoch 1016 iteration 100 loss 1.6007670164108276\n",
      "Epoch 1016 iteration 200 loss 6.81568717956543\n",
      "Epoch 1016 Training loss 3.9855918703515294\n",
      "Epoch 1017 iteration 0 loss 0.3071308135986328\n",
      "Epoch 1017 iteration 100 loss 1.554384708404541\n",
      "Epoch 1017 iteration 200 loss 6.718179702758789\n",
      "Epoch 1017 Training loss 3.989593464021681\n",
      "Epoch 1018 iteration 0 loss 0.3827133774757385\n",
      "Epoch 1018 iteration 100 loss 1.58368980884552\n",
      "Epoch 1018 iteration 200 loss 6.813283920288086\n",
      "Epoch 1018 Training loss 3.987347875335074\n",
      "Epoch 1019 iteration 0 loss 0.3288576006889343\n",
      "Epoch 1019 iteration 100 loss 1.5975788831710815\n",
      "Epoch 1019 iteration 200 loss 6.633782863616943\n",
      "Epoch 1019 Training loss 3.9866863239597095\n",
      "Epoch 1020 iteration 0 loss 0.29434067010879517\n",
      "Epoch 1020 iteration 100 loss 1.562273383140564\n",
      "Epoch 1020 iteration 200 loss 6.746488571166992\n",
      "Epoch 1020 Training loss 4.017236547413373\n",
      "Evaluation loss 7.915552021309486\n",
      "Epoch 1021 iteration 0 loss 0.30463021993637085\n",
      "Epoch 1021 iteration 100 loss 1.569772481918335\n",
      "Epoch 1021 iteration 200 loss 6.953315258026123\n",
      "Epoch 1021 Training loss 4.025248779084051\n",
      "Epoch 1022 iteration 0 loss 0.2772252559661865\n",
      "Epoch 1022 iteration 100 loss 1.6234105825424194\n",
      "Epoch 1022 iteration 200 loss 6.82975959777832\n",
      "Epoch 1022 Training loss 4.007923199705517\n",
      "Epoch 1023 iteration 0 loss 0.3068090081214905\n",
      "Epoch 1023 iteration 100 loss 1.5515141487121582\n",
      "Epoch 1023 iteration 200 loss 6.838517665863037\n",
      "Epoch 1023 Training loss 4.0133312989328305\n",
      "Epoch 1024 iteration 0 loss 0.3198530673980713\n",
      "Epoch 1024 iteration 100 loss 1.543817162513733\n",
      "Epoch 1024 iteration 200 loss 6.952937126159668\n",
      "Epoch 1024 Training loss 4.012130708237741\n",
      "Epoch 1025 iteration 0 loss 0.3323807120323181\n",
      "Epoch 1025 iteration 100 loss 1.5202734470367432\n",
      "Epoch 1025 iteration 200 loss 6.768831729888916\n",
      "Epoch 1025 Training loss 4.015449254889724\n",
      "Evaluation loss 7.898982537490651\n",
      "Epoch 1026 iteration 0 loss 0.33183836936950684\n",
      "Epoch 1026 iteration 100 loss 1.632960319519043\n",
      "Epoch 1026 iteration 200 loss 6.803389072418213\n",
      "Epoch 1026 Training loss 4.013385040707887\n",
      "Epoch 1027 iteration 0 loss 0.32901591062545776\n",
      "Epoch 1027 iteration 100 loss 1.6635347604751587\n",
      "Epoch 1027 iteration 200 loss 6.70054817199707\n",
      "Epoch 1027 Training loss 4.016863594851217\n",
      "Epoch 1028 iteration 0 loss 0.34421437978744507\n",
      "Epoch 1028 iteration 100 loss 1.571467399597168\n",
      "Epoch 1028 iteration 200 loss 6.891120433807373\n",
      "Epoch 1028 Training loss 4.018408727245013\n",
      "Epoch 1029 iteration 0 loss 0.2902321219444275\n",
      "Epoch 1029 iteration 100 loss 1.673836588859558\n",
      "Epoch 1029 iteration 200 loss 6.885164260864258\n",
      "Epoch 1029 Training loss 4.034777165919792\n",
      "Epoch 1030 iteration 0 loss 0.2927481532096863\n",
      "Epoch 1030 iteration 100 loss 1.6512459516525269\n",
      "Epoch 1030 iteration 200 loss 6.827737331390381\n",
      "Epoch 1030 Training loss 4.036273692554844\n",
      "Evaluation loss 7.94864389428676\n",
      "Epoch 1031 iteration 0 loss 0.24703478813171387\n",
      "Epoch 1031 iteration 100 loss 1.750723958015442\n",
      "Epoch 1031 iteration 200 loss 6.741387844085693\n",
      "Epoch 1031 Training loss 4.048803459263011\n",
      "Epoch 1032 iteration 0 loss 0.2795356214046478\n",
      "Epoch 1032 iteration 100 loss 1.6857303380966187\n",
      "Epoch 1032 iteration 200 loss 6.89036226272583\n",
      "Epoch 1032 Training loss 4.044837026345291\n",
      "Epoch 1033 iteration 0 loss 0.3508548140525818\n",
      "Epoch 1033 iteration 100 loss 1.6471707820892334\n",
      "Epoch 1033 iteration 200 loss 6.888631820678711\n",
      "Epoch 1033 Training loss 4.058459133839439\n",
      "Epoch 1034 iteration 0 loss 0.3097027838230133\n",
      "Epoch 1034 iteration 100 loss 1.6616102457046509\n",
      "Epoch 1034 iteration 200 loss 6.832283020019531\n",
      "Epoch 1034 Training loss 4.054846923796307\n",
      "Epoch 1035 iteration 0 loss 0.3063560128211975\n",
      "Epoch 1035 iteration 100 loss 1.8166685104370117\n",
      "Epoch 1035 iteration 200 loss 6.878640174865723\n",
      "Epoch 1035 Training loss 4.055944143440357\n",
      "Evaluation loss 7.886338257243118\n",
      "Epoch 1036 iteration 0 loss 0.38367313146591187\n",
      "Epoch 1036 iteration 100 loss 1.6877074241638184\n",
      "Epoch 1036 iteration 200 loss 7.010475158691406\n",
      "Epoch 1036 Training loss 4.052757648455146\n",
      "Epoch 1037 iteration 0 loss 0.3771776258945465\n",
      "Epoch 1037 iteration 100 loss 1.6661466360092163\n",
      "Epoch 1037 iteration 200 loss 7.006004333496094\n",
      "Epoch 1037 Training loss 4.063667830445156\n",
      "Epoch 1038 iteration 0 loss 0.31622815132141113\n",
      "Epoch 1038 iteration 100 loss 1.7291146516799927\n",
      "Epoch 1038 iteration 200 loss 6.817392349243164\n",
      "Epoch 1038 Training loss 4.069815285984701\n",
      "Epoch 1039 iteration 0 loss 0.2924794852733612\n",
      "Epoch 1039 iteration 100 loss 1.639494776725769\n",
      "Epoch 1039 iteration 200 loss 6.877851963043213\n",
      "Epoch 1039 Training loss 4.068933148987162\n",
      "Epoch 1040 iteration 0 loss 0.34199222922325134\n",
      "Epoch 1040 iteration 100 loss 1.7733956575393677\n",
      "Epoch 1040 iteration 200 loss 6.944822311401367\n",
      "Epoch 1040 Training loss 4.068741807054944\n",
      "Evaluation loss 7.914918052337674\n",
      "Epoch 1041 iteration 0 loss 0.2769409120082855\n",
      "Epoch 1041 iteration 100 loss 1.7156707048416138\n",
      "Epoch 1041 iteration 200 loss 6.948939323425293\n",
      "Epoch 1041 Training loss 4.082894226218488\n",
      "Epoch 1042 iteration 0 loss 0.24600215256214142\n",
      "Epoch 1042 iteration 100 loss 1.9257855415344238\n",
      "Epoch 1042 iteration 200 loss 6.933718681335449\n",
      "Epoch 1042 Training loss 4.089747206627502\n",
      "Epoch 1043 iteration 0 loss 0.3182171583175659\n",
      "Epoch 1043 iteration 100 loss 1.8564621210098267\n",
      "Epoch 1043 iteration 200 loss 6.815840244293213\n",
      "Epoch 1043 Training loss 4.089200713435499\n",
      "Epoch 1044 iteration 0 loss 0.283553808927536\n",
      "Epoch 1044 iteration 100 loss 1.7972278594970703\n",
      "Epoch 1044 iteration 200 loss 6.872885704040527\n",
      "Epoch 1044 Training loss 4.092301368214931\n",
      "Epoch 1045 iteration 0 loss 0.3047338128089905\n",
      "Epoch 1045 iteration 100 loss 1.81790030002594\n",
      "Epoch 1045 iteration 200 loss 6.9036545753479\n",
      "Epoch 1045 Training loss 4.093314131120169\n",
      "Evaluation loss 7.934136556938448\n",
      "Epoch 1046 iteration 0 loss 0.3750205636024475\n",
      "Epoch 1046 iteration 100 loss 1.8822505474090576\n",
      "Epoch 1046 iteration 200 loss 7.068498611450195\n",
      "Epoch 1046 Training loss 4.101510038238024\n",
      "Epoch 1047 iteration 0 loss 0.3516441881656647\n",
      "Epoch 1047 iteration 100 loss 1.8307747840881348\n",
      "Epoch 1047 iteration 200 loss 7.03689432144165\n",
      "Epoch 1047 Training loss 4.104295415542674\n",
      "Epoch 1048 iteration 0 loss 0.31527459621429443\n",
      "Epoch 1048 iteration 100 loss 1.8003394603729248\n",
      "Epoch 1048 iteration 200 loss 7.027277946472168\n",
      "Epoch 1048 Training loss 4.111810656791515\n",
      "Epoch 1049 iteration 0 loss 0.3130035996437073\n",
      "Epoch 1049 iteration 100 loss 2.023965358734131\n",
      "Epoch 1049 iteration 200 loss 7.050874710083008\n",
      "Epoch 1049 Training loss 4.105268933001375\n",
      "Epoch 1050 iteration 0 loss 0.34061017632484436\n",
      "Epoch 1050 iteration 100 loss 1.7193652391433716\n",
      "Epoch 1050 iteration 200 loss 6.990011692047119\n",
      "Epoch 1050 Training loss 4.116281668342687\n",
      "Evaluation loss 7.951427225158661\n",
      "Epoch 1051 iteration 0 loss 0.37278324365615845\n",
      "Epoch 1051 iteration 100 loss 1.9025763273239136\n",
      "Epoch 1051 iteration 200 loss 6.879476070404053\n",
      "Epoch 1051 Training loss 4.124848798062771\n",
      "Epoch 1052 iteration 0 loss 0.40440836548805237\n",
      "Epoch 1052 iteration 100 loss 1.8491441011428833\n",
      "Epoch 1052 iteration 200 loss 6.871795177459717\n",
      "Epoch 1052 Training loss 4.1062608342878955\n",
      "Epoch 1053 iteration 0 loss 0.36477118730545044\n",
      "Epoch 1053 iteration 100 loss 1.7876018285751343\n",
      "Epoch 1053 iteration 200 loss 6.895151615142822\n",
      "Epoch 1053 Training loss 4.120331359411642\n",
      "Epoch 1054 iteration 0 loss 0.30907490849494934\n",
      "Epoch 1054 iteration 100 loss 1.836272954940796\n",
      "Epoch 1054 iteration 200 loss 6.929234504699707\n",
      "Epoch 1054 Training loss 4.122193634726509\n",
      "Epoch 1055 iteration 0 loss 0.3743222653865814\n",
      "Epoch 1055 iteration 100 loss 1.8043943643569946\n",
      "Epoch 1055 iteration 200 loss 6.965879917144775\n",
      "Epoch 1055 Training loss 4.119984913722625\n",
      "Evaluation loss 7.97375946786127\n",
      "Epoch 1056 iteration 0 loss 0.40867891907691956\n",
      "Epoch 1056 iteration 100 loss 1.9385896921157837\n",
      "Epoch 1056 iteration 200 loss 6.799368381500244\n",
      "Epoch 1056 Training loss 4.128912836208453\n",
      "Epoch 1057 iteration 0 loss 0.31509631872177124\n",
      "Epoch 1057 iteration 100 loss 1.9343986511230469\n",
      "Epoch 1057 iteration 200 loss 6.930928707122803\n",
      "Epoch 1057 Training loss 4.149421130921625\n",
      "Epoch 1058 iteration 0 loss 0.34565746784210205\n",
      "Epoch 1058 iteration 100 loss 1.991228699684143\n",
      "Epoch 1058 iteration 200 loss 7.014960765838623\n",
      "Epoch 1058 Training loss 4.154245593867278\n",
      "Epoch 1059 iteration 0 loss 0.3610079288482666\n",
      "Epoch 1059 iteration 100 loss 1.9783767461776733\n",
      "Epoch 1059 iteration 200 loss 6.895524024963379\n",
      "Epoch 1059 Training loss 4.162661213404242\n",
      "Epoch 1060 iteration 0 loss 0.42437544465065\n",
      "Epoch 1060 iteration 100 loss 1.982794165611267\n",
      "Epoch 1060 iteration 200 loss 7.096519470214844\n",
      "Epoch 1060 Training loss 4.1340422570037365\n",
      "Evaluation loss 7.9374960500907\n",
      "Epoch 1061 iteration 0 loss 0.3314723074436188\n",
      "Epoch 1061 iteration 100 loss 1.812347412109375\n",
      "Epoch 1061 iteration 200 loss 7.106917858123779\n",
      "Epoch 1061 Training loss 4.157444408735421\n",
      "Epoch 1062 iteration 0 loss 0.4391649663448334\n",
      "Epoch 1062 iteration 100 loss 2.02280330657959\n",
      "Epoch 1062 iteration 200 loss 6.961339473724365\n",
      "Epoch 1062 Training loss 4.165626307210878\n",
      "Epoch 1063 iteration 0 loss 0.40217629075050354\n",
      "Epoch 1063 iteration 100 loss 1.9735194444656372\n",
      "Epoch 1063 iteration 200 loss 7.097123622894287\n",
      "Epoch 1063 Training loss 4.157350467835115\n",
      "Epoch 1064 iteration 0 loss 0.3611333966255188\n",
      "Epoch 1064 iteration 100 loss 1.8470885753631592\n",
      "Epoch 1064 iteration 200 loss 7.012678623199463\n",
      "Epoch 1064 Training loss 4.177026529900933\n",
      "Epoch 1065 iteration 0 loss 0.354800283908844\n",
      "Epoch 1065 iteration 100 loss 1.8386906385421753\n",
      "Epoch 1065 iteration 200 loss 7.126999855041504\n",
      "Epoch 1065 Training loss 4.165200260247652\n",
      "Evaluation loss 7.958623053217709\n",
      "Epoch 1066 iteration 0 loss 0.3853283226490021\n",
      "Epoch 1066 iteration 100 loss 1.887855052947998\n",
      "Epoch 1066 iteration 200 loss 7.161128520965576\n",
      "Epoch 1066 Training loss 4.189210450285226\n",
      "Epoch 1067 iteration 0 loss 0.3793601393699646\n",
      "Epoch 1067 iteration 100 loss 1.8899539709091187\n",
      "Epoch 1067 iteration 200 loss 7.091735363006592\n",
      "Epoch 1067 Training loss 4.186364260793816\n",
      "Epoch 1068 iteration 0 loss 0.4124489724636078\n",
      "Epoch 1068 iteration 100 loss 1.9137283563613892\n",
      "Epoch 1068 iteration 200 loss 7.038973808288574\n",
      "Epoch 1068 Training loss 4.198904487999569\n",
      "Epoch 1069 iteration 0 loss 0.3986107110977173\n",
      "Epoch 1069 iteration 100 loss 1.9715447425842285\n",
      "Epoch 1069 iteration 200 loss 7.142822265625\n",
      "Epoch 1069 Training loss 4.18652223535163\n",
      "Epoch 1070 iteration 0 loss 0.38410264253616333\n",
      "Epoch 1070 iteration 100 loss 1.8846542835235596\n",
      "Epoch 1070 iteration 200 loss 7.054637908935547\n",
      "Epoch 1070 Training loss 4.206222426962784\n",
      "Evaluation loss 8.01124179541193\n",
      "Epoch 1071 iteration 0 loss 0.3756099343299866\n",
      "Epoch 1071 iteration 100 loss 2.0079257488250732\n",
      "Epoch 1071 iteration 200 loss 6.99664306640625\n",
      "Epoch 1071 Training loss 4.215032671714815\n",
      "Epoch 1072 iteration 0 loss 0.27768319845199585\n",
      "Epoch 1072 iteration 100 loss 1.9966365098953247\n",
      "Epoch 1072 iteration 200 loss 6.885056495666504\n",
      "Epoch 1072 Training loss 4.213573036894318\n",
      "Epoch 1073 iteration 0 loss 0.32333606481552124\n",
      "Epoch 1073 iteration 100 loss 2.010890245437622\n",
      "Epoch 1073 iteration 200 loss 7.192091941833496\n",
      "Epoch 1073 Training loss 4.211641927852655\n",
      "Epoch 1074 iteration 0 loss 0.4031459391117096\n",
      "Epoch 1074 iteration 100 loss 1.9889190196990967\n",
      "Epoch 1074 iteration 200 loss 7.24604606628418\n",
      "Epoch 1074 Training loss 4.223111557796529\n",
      "Epoch 1075 iteration 0 loss 0.4312044084072113\n",
      "Epoch 1075 iteration 100 loss 1.9598976373672485\n",
      "Epoch 1075 iteration 200 loss 7.140460968017578\n",
      "Epoch 1075 Training loss 4.23250979243656\n",
      "Evaluation loss 7.977744964358235\n",
      "Epoch 1076 iteration 0 loss 0.4130212664604187\n",
      "Epoch 1076 iteration 100 loss 1.8184423446655273\n",
      "Epoch 1076 iteration 200 loss 6.982795238494873\n",
      "Epoch 1076 Training loss 4.242213380484885\n",
      "Epoch 1077 iteration 0 loss 0.37948793172836304\n",
      "Epoch 1077 iteration 100 loss 2.0459587574005127\n",
      "Epoch 1077 iteration 200 loss 7.076789855957031\n",
      "Epoch 1077 Training loss 4.236561746878277\n",
      "Epoch 1078 iteration 0 loss 0.39898762106895447\n",
      "Epoch 1078 iteration 100 loss 1.9918793439865112\n",
      "Epoch 1078 iteration 200 loss 7.090877532958984\n",
      "Epoch 1078 Training loss 4.233376494360231\n",
      "Epoch 1079 iteration 0 loss 0.3872569799423218\n",
      "Epoch 1079 iteration 100 loss 2.0002217292785645\n",
      "Epoch 1079 iteration 200 loss 7.060798168182373\n",
      "Epoch 1079 Training loss 4.2572553154362955\n",
      "Epoch 1080 iteration 0 loss 0.4643269181251526\n",
      "Epoch 1080 iteration 100 loss 2.0152084827423096\n",
      "Epoch 1080 iteration 200 loss 7.0542311668396\n",
      "Epoch 1080 Training loss 4.252710263465212\n",
      "Evaluation loss 7.9983284276657205\n",
      "Epoch 1081 iteration 0 loss 0.43019843101501465\n",
      "Epoch 1081 iteration 100 loss 2.0128419399261475\n",
      "Epoch 1081 iteration 200 loss 7.05810546875\n",
      "Epoch 1081 Training loss 4.252591347978227\n",
      "Epoch 1082 iteration 0 loss 0.40349048376083374\n",
      "Epoch 1082 iteration 100 loss 1.947856068611145\n",
      "Epoch 1082 iteration 200 loss 7.077334403991699\n",
      "Epoch 1082 Training loss 4.260092651764278\n",
      "Epoch 1083 iteration 0 loss 0.3589847683906555\n",
      "Epoch 1083 iteration 100 loss 1.8747363090515137\n",
      "Epoch 1083 iteration 200 loss 7.038728713989258\n",
      "Epoch 1083 Training loss 4.253168400197376\n",
      "Epoch 1084 iteration 0 loss 0.39504069089889526\n",
      "Epoch 1084 iteration 100 loss 1.9191901683807373\n",
      "Epoch 1084 iteration 200 loss 7.095919132232666\n",
      "Epoch 1084 Training loss 4.275880586656382\n",
      "Epoch 1085 iteration 0 loss 0.4261377453804016\n",
      "Epoch 1085 iteration 100 loss 1.822413444519043\n",
      "Epoch 1085 iteration 200 loss 7.081527233123779\n",
      "Epoch 1085 Training loss 4.274794648664469\n",
      "Evaluation loss 8.02302108447706\n",
      "Epoch 1086 iteration 0 loss 0.4488352835178375\n",
      "Epoch 1086 iteration 100 loss 2.013361692428589\n",
      "Epoch 1086 iteration 200 loss 7.108308792114258\n",
      "Epoch 1086 Training loss 4.264632328633274\n",
      "Epoch 1087 iteration 0 loss 0.45387035608291626\n",
      "Epoch 1087 iteration 100 loss 1.9235183000564575\n",
      "Epoch 1087 iteration 200 loss 7.061041831970215\n",
      "Epoch 1087 Training loss 4.263196343362046\n",
      "Epoch 1088 iteration 0 loss 0.39767977595329285\n",
      "Epoch 1088 iteration 100 loss 1.9737482070922852\n",
      "Epoch 1088 iteration 200 loss 7.006192207336426\n",
      "Epoch 1088 Training loss 4.26545598305147\n",
      "Epoch 1089 iteration 0 loss 0.3719300329685211\n",
      "Epoch 1089 iteration 100 loss 2.030524730682373\n",
      "Epoch 1089 iteration 200 loss 7.134841442108154\n",
      "Epoch 1089 Training loss 4.26831458361821\n",
      "Epoch 1090 iteration 0 loss 0.403704971075058\n",
      "Epoch 1090 iteration 100 loss 1.9400498867034912\n",
      "Epoch 1090 iteration 200 loss 7.082009315490723\n",
      "Epoch 1090 Training loss 4.275319234003682\n",
      "Evaluation loss 7.9826655984437584\n",
      "Epoch 1091 iteration 0 loss 0.3916115164756775\n",
      "Epoch 1091 iteration 100 loss 1.9294285774230957\n",
      "Epoch 1091 iteration 200 loss 7.170773983001709\n",
      "Epoch 1091 Training loss 4.2743766864282255\n",
      "Epoch 1092 iteration 0 loss 0.3841586410999298\n",
      "Epoch 1092 iteration 100 loss 1.9438730478286743\n",
      "Epoch 1092 iteration 200 loss 6.992513179779053\n",
      "Epoch 1092 Training loss 4.272374238528845\n",
      "Epoch 1093 iteration 0 loss 0.3696749806404114\n",
      "Epoch 1093 iteration 100 loss 1.9419465065002441\n",
      "Epoch 1093 iteration 200 loss 7.056743621826172\n",
      "Epoch 1093 Training loss 4.280202814053793\n",
      "Epoch 1094 iteration 0 loss 0.36224862933158875\n",
      "Epoch 1094 iteration 100 loss 1.9164296388626099\n",
      "Epoch 1094 iteration 200 loss 7.090264320373535\n",
      "Epoch 1094 Training loss 4.2726780216604325\n",
      "Epoch 1095 iteration 0 loss 0.44255465269088745\n",
      "Epoch 1095 iteration 100 loss 1.900346279144287\n",
      "Epoch 1095 iteration 200 loss 7.24154806137085\n",
      "Epoch 1095 Training loss 4.293715787630188\n",
      "Evaluation loss 7.989431600462586\n",
      "Epoch 1096 iteration 0 loss 0.36235636472702026\n",
      "Epoch 1096 iteration 100 loss 2.0514729022979736\n",
      "Epoch 1096 iteration 200 loss 7.151103973388672\n",
      "Epoch 1096 Training loss 4.294059934412608\n",
      "Epoch 1097 iteration 0 loss 0.3513215184211731\n",
      "Epoch 1097 iteration 100 loss 1.9390839338302612\n",
      "Epoch 1097 iteration 200 loss 7.023120403289795\n",
      "Epoch 1097 Training loss 4.309147402420645\n",
      "Epoch 1098 iteration 0 loss 0.3342468738555908\n",
      "Epoch 1098 iteration 100 loss 2.061317205429077\n",
      "Epoch 1098 iteration 200 loss 6.986289978027344\n",
      "Epoch 1098 Training loss 4.310141373284201\n",
      "Epoch 1099 iteration 0 loss 0.4720954895019531\n",
      "Epoch 1099 iteration 100 loss 2.027894973754883\n",
      "Epoch 1099 iteration 200 loss 6.902396202087402\n",
      "Epoch 1099 Training loss 4.307489266989289\n",
      "Epoch 1100 iteration 0 loss 0.36863675713539124\n",
      "Epoch 1100 iteration 100 loss 2.0124552249908447\n",
      "Epoch 1100 iteration 200 loss 7.112682819366455\n",
      "Epoch 1100 Training loss 4.31907760264539\n",
      "Evaluation loss 7.997614494057939\n",
      "Epoch 1101 iteration 0 loss 0.4322969317436218\n",
      "Epoch 1101 iteration 100 loss 2.0154616832733154\n",
      "Epoch 1101 iteration 200 loss 7.070923328399658\n",
      "Epoch 1101 Training loss 4.327775293612654\n",
      "Epoch 1102 iteration 0 loss 0.3539847731590271\n",
      "Epoch 1102 iteration 100 loss 1.9971901178359985\n",
      "Epoch 1102 iteration 200 loss 7.118823051452637\n",
      "Epoch 1102 Training loss 4.335172807201535\n",
      "Epoch 1103 iteration 0 loss 0.4197988510131836\n",
      "Epoch 1103 iteration 100 loss 2.0604796409606934\n",
      "Epoch 1103 iteration 200 loss 7.177299976348877\n",
      "Epoch 1103 Training loss 4.333282889088618\n",
      "Epoch 1104 iteration 0 loss 0.3153979778289795\n",
      "Epoch 1104 iteration 100 loss 2.0793566703796387\n",
      "Epoch 1104 iteration 200 loss 7.131499767303467\n",
      "Epoch 1104 Training loss 4.355273591785239\n",
      "Epoch 1105 iteration 0 loss 0.37621015310287476\n",
      "Epoch 1105 iteration 100 loss 2.136589765548706\n",
      "Epoch 1105 iteration 200 loss 7.048873424530029\n",
      "Epoch 1105 Training loss 4.350452656084554\n",
      "Evaluation loss 8.03505976807038\n",
      "Epoch 1106 iteration 0 loss 0.43593257665634155\n",
      "Epoch 1106 iteration 100 loss 2.0208680629730225\n",
      "Epoch 1106 iteration 200 loss 7.191772937774658\n",
      "Epoch 1106 Training loss 4.35558191817903\n",
      "Epoch 1107 iteration 0 loss 0.34744060039520264\n",
      "Epoch 1107 iteration 100 loss 2.0219368934631348\n",
      "Epoch 1107 iteration 200 loss 7.24875545501709\n",
      "Epoch 1107 Training loss 4.3506586811931856\n",
      "Epoch 1108 iteration 0 loss 0.43763846158981323\n",
      "Epoch 1108 iteration 100 loss 2.1395411491394043\n",
      "Epoch 1108 iteration 200 loss 7.194527626037598\n",
      "Epoch 1108 Training loss 4.362628775454669\n",
      "Epoch 1109 iteration 0 loss 0.4285384714603424\n",
      "Epoch 1109 iteration 100 loss 2.0025229454040527\n",
      "Epoch 1109 iteration 200 loss 7.322686672210693\n",
      "Epoch 1109 Training loss 4.373436926377079\n",
      "Epoch 1110 iteration 0 loss 0.44630375504493713\n",
      "Epoch 1110 iteration 100 loss 2.0363190174102783\n",
      "Epoch 1110 iteration 200 loss 7.165274620056152\n",
      "Epoch 1110 Training loss 4.375132175397308\n",
      "Evaluation loss 8.043810227956811\n",
      "Epoch 1111 iteration 0 loss 0.388479620218277\n",
      "Epoch 1111 iteration 100 loss 2.0293068885803223\n",
      "Epoch 1111 iteration 200 loss 7.151599407196045\n",
      "Epoch 1111 Training loss 4.380258575437571\n",
      "Epoch 1112 iteration 0 loss 0.4306871294975281\n",
      "Epoch 1112 iteration 100 loss 2.1414191722869873\n",
      "Epoch 1112 iteration 200 loss 7.199674606323242\n",
      "Epoch 1112 Training loss 4.384242806726377\n",
      "Epoch 1113 iteration 0 loss 0.3683648109436035\n",
      "Epoch 1113 iteration 100 loss 2.1034085750579834\n",
      "Epoch 1113 iteration 200 loss 7.026108741760254\n",
      "Epoch 1113 Training loss 4.3761397309041214\n",
      "Epoch 1114 iteration 0 loss 0.520650327205658\n",
      "Epoch 1114 iteration 100 loss 2.050452947616577\n",
      "Epoch 1114 iteration 200 loss 7.019864082336426\n",
      "Epoch 1114 Training loss 4.379978815101655\n",
      "Epoch 1115 iteration 0 loss 0.3319365382194519\n",
      "Epoch 1115 iteration 100 loss 2.187878370285034\n",
      "Epoch 1115 iteration 200 loss 7.138007164001465\n",
      "Epoch 1115 Training loss 4.387010848884682\n",
      "Evaluation loss 8.0408765553089\n",
      "Epoch 1116 iteration 0 loss 0.3993151783943176\n",
      "Epoch 1116 iteration 100 loss 2.0034990310668945\n",
      "Epoch 1116 iteration 200 loss 7.109439849853516\n",
      "Epoch 1116 Training loss 4.395281835721203\n",
      "Epoch 1117 iteration 0 loss 0.47953352332115173\n",
      "Epoch 1117 iteration 100 loss 2.112114906311035\n",
      "Epoch 1117 iteration 200 loss 7.212812900543213\n",
      "Epoch 1117 Training loss 4.413357336091692\n",
      "Epoch 1118 iteration 0 loss 0.5112630724906921\n",
      "Epoch 1118 iteration 100 loss 2.2288737297058105\n",
      "Epoch 1118 iteration 200 loss 7.363270282745361\n",
      "Epoch 1118 Training loss 4.413008393910009\n",
      "Epoch 1119 iteration 0 loss 0.4083317220211029\n",
      "Epoch 1119 iteration 100 loss 2.1946616172790527\n",
      "Epoch 1119 iteration 200 loss 7.222634792327881\n",
      "Epoch 1119 Training loss 4.417714209272511\n",
      "Epoch 1120 iteration 0 loss 0.4219784736633301\n",
      "Epoch 1120 iteration 100 loss 2.1523971557617188\n",
      "Epoch 1120 iteration 200 loss 7.137650489807129\n",
      "Epoch 1120 Training loss 4.418734046956104\n",
      "Evaluation loss 8.049167073895525\n",
      "Epoch 1121 iteration 0 loss 0.3713463246822357\n",
      "Epoch 1121 iteration 100 loss 2.1570310592651367\n",
      "Epoch 1121 iteration 200 loss 7.262396335601807\n",
      "Epoch 1121 Training loss 4.420656602063733\n",
      "Epoch 1122 iteration 0 loss 0.3648989796638489\n",
      "Epoch 1122 iteration 100 loss 2.3213651180267334\n",
      "Epoch 1122 iteration 200 loss 7.167989730834961\n",
      "Epoch 1122 Training loss 4.434319775106122\n",
      "Epoch 1123 iteration 0 loss 0.38341614603996277\n",
      "Epoch 1123 iteration 100 loss 2.106311559677124\n",
      "Epoch 1123 iteration 200 loss 7.223574161529541\n",
      "Epoch 1123 Training loss 4.410566701979822\n",
      "Epoch 1124 iteration 0 loss 0.38084396719932556\n",
      "Epoch 1124 iteration 100 loss 2.0890166759490967\n",
      "Epoch 1124 iteration 200 loss 7.244230270385742\n",
      "Epoch 1124 Training loss 4.4296391587267765\n",
      "Epoch 1125 iteration 0 loss 0.4210168421268463\n",
      "Epoch 1125 iteration 100 loss 2.105823278427124\n",
      "Epoch 1125 iteration 200 loss 7.266228199005127\n",
      "Epoch 1125 Training loss 4.433625686576642\n",
      "Evaluation loss 8.017872084238471\n",
      "Epoch 1126 iteration 0 loss 0.39054444432258606\n",
      "Epoch 1126 iteration 100 loss 1.966461181640625\n",
      "Epoch 1126 iteration 200 loss 7.186763286590576\n",
      "Epoch 1126 Training loss 4.43564376724783\n",
      "Epoch 1127 iteration 0 loss 0.3477276861667633\n",
      "Epoch 1127 iteration 100 loss 1.998836874961853\n",
      "Epoch 1127 iteration 200 loss 7.370463848114014\n",
      "Epoch 1127 Training loss 4.450404437765879\n",
      "Epoch 1128 iteration 0 loss 0.3186393678188324\n",
      "Epoch 1128 iteration 100 loss 2.1831705570220947\n",
      "Epoch 1128 iteration 200 loss 7.338387489318848\n",
      "Epoch 1128 Training loss 4.448138672407324\n",
      "Epoch 1129 iteration 0 loss 0.3580717444419861\n",
      "Epoch 1129 iteration 100 loss 2.1238303184509277\n",
      "Epoch 1129 iteration 200 loss 7.472586631774902\n",
      "Epoch 1129 Training loss 4.457116611302947\n",
      "Epoch 1130 iteration 0 loss 0.3925400376319885\n",
      "Epoch 1130 iteration 100 loss 2.1417226791381836\n",
      "Epoch 1130 iteration 200 loss 7.329079627990723\n",
      "Epoch 1130 Training loss 4.452805959827576\n",
      "Evaluation loss 8.005660168091813\n",
      "Epoch 1131 iteration 0 loss 0.45811089873313904\n",
      "Epoch 1131 iteration 100 loss 2.2457468509674072\n",
      "Epoch 1131 iteration 200 loss 7.297062397003174\n",
      "Epoch 1131 Training loss 4.462327267193746\n",
      "Epoch 1132 iteration 0 loss 0.5086995363235474\n",
      "Epoch 1132 iteration 100 loss 2.1407241821289062\n",
      "Epoch 1132 iteration 200 loss 7.312272071838379\n",
      "Epoch 1132 Training loss 4.465789870075972\n",
      "Epoch 1133 iteration 0 loss 0.42484086751937866\n",
      "Epoch 1133 iteration 100 loss 2.1196200847625732\n",
      "Epoch 1133 iteration 200 loss 7.213130950927734\n",
      "Epoch 1133 Training loss 4.462681570864239\n",
      "Epoch 1134 iteration 0 loss 0.33777332305908203\n",
      "Epoch 1134 iteration 100 loss 2.120913028717041\n",
      "Epoch 1134 iteration 200 loss 7.292037487030029\n",
      "Epoch 1134 Training loss 4.465713525130001\n",
      "Epoch 1135 iteration 0 loss 0.3433611989021301\n",
      "Epoch 1135 iteration 100 loss 2.1595325469970703\n",
      "Epoch 1135 iteration 200 loss 7.25952672958374\n",
      "Epoch 1135 Training loss 4.487558796628859\n",
      "Evaluation loss 8.022174394993877\n",
      "Epoch 1136 iteration 0 loss 0.4864419102668762\n",
      "Epoch 1136 iteration 100 loss 2.2696614265441895\n",
      "Epoch 1136 iteration 200 loss 7.417440414428711\n",
      "Epoch 1136 Training loss 4.488403145351388\n",
      "Epoch 1137 iteration 0 loss 0.39533957839012146\n",
      "Epoch 1137 iteration 100 loss 2.1530721187591553\n",
      "Epoch 1137 iteration 200 loss 7.27413272857666\n",
      "Epoch 1137 Training loss 4.492751927427075\n",
      "Epoch 1138 iteration 0 loss 0.41172581911087036\n",
      "Epoch 1138 iteration 100 loss 2.212944984436035\n",
      "Epoch 1138 iteration 200 loss 7.383438587188721\n",
      "Epoch 1138 Training loss 4.483526997210504\n",
      "Epoch 1139 iteration 0 loss 0.4493437707424164\n",
      "Epoch 1139 iteration 100 loss 2.0132198333740234\n",
      "Epoch 1139 iteration 200 loss 7.238210678100586\n",
      "Epoch 1139 Training loss 4.487305097822236\n",
      "Epoch 1140 iteration 0 loss 0.4168819487094879\n",
      "Epoch 1140 iteration 100 loss 2.1573596000671387\n",
      "Epoch 1140 iteration 200 loss 7.345481872558594\n",
      "Epoch 1140 Training loss 4.488553332811315\n",
      "Evaluation loss 8.04787933585772\n",
      "Epoch 1141 iteration 0 loss 0.47576409578323364\n",
      "Epoch 1141 iteration 100 loss 2.194873332977295\n",
      "Epoch 1141 iteration 200 loss 7.25216007232666\n",
      "Epoch 1141 Training loss 4.477316291031826\n",
      "Epoch 1142 iteration 0 loss 0.4839724600315094\n",
      "Epoch 1142 iteration 100 loss 2.266631603240967\n",
      "Epoch 1142 iteration 200 loss 7.449155330657959\n",
      "Epoch 1142 Training loss 4.501266344225562\n",
      "Epoch 1143 iteration 0 loss 0.41095882654190063\n",
      "Epoch 1143 iteration 100 loss 2.309574604034424\n",
      "Epoch 1143 iteration 200 loss 7.482545852661133\n",
      "Epoch 1143 Training loss 4.50139358913823\n",
      "Epoch 1144 iteration 0 loss 0.4121803343296051\n",
      "Epoch 1144 iteration 100 loss 2.296299934387207\n",
      "Epoch 1144 iteration 200 loss 7.319069862365723\n",
      "Epoch 1144 Training loss 4.509514006288506\n",
      "Epoch 1145 iteration 0 loss 0.4080086350440979\n",
      "Epoch 1145 iteration 100 loss 2.2201437950134277\n",
      "Epoch 1145 iteration 200 loss 7.282230377197266\n",
      "Epoch 1145 Training loss 4.522326879160269\n",
      "Evaluation loss 8.060222416158593\n",
      "Epoch 1146 iteration 0 loss 0.51927649974823\n",
      "Epoch 1146 iteration 100 loss 2.318796396255493\n",
      "Epoch 1146 iteration 200 loss 7.426364898681641\n",
      "Epoch 1146 Training loss 4.5336920235615885\n",
      "Epoch 1147 iteration 0 loss 0.456855446100235\n",
      "Epoch 1147 iteration 100 loss 2.2953128814697266\n",
      "Epoch 1147 iteration 200 loss 7.448841094970703\n",
      "Epoch 1147 Training loss 4.524006838246235\n",
      "Epoch 1148 iteration 0 loss 0.4331730306148529\n",
      "Epoch 1148 iteration 100 loss 2.3215575218200684\n",
      "Epoch 1148 iteration 200 loss 7.409152984619141\n",
      "Epoch 1148 Training loss 4.52876760688242\n",
      "Epoch 1149 iteration 0 loss 0.3817228376865387\n",
      "Epoch 1149 iteration 100 loss 2.2133283615112305\n",
      "Epoch 1149 iteration 200 loss 7.464428424835205\n",
      "Epoch 1149 Training loss 4.5495849322631265\n",
      "Epoch 1150 iteration 0 loss 0.41279974579811096\n",
      "Epoch 1150 iteration 100 loss 2.440808057785034\n",
      "Epoch 1150 iteration 200 loss 7.325577735900879\n",
      "Epoch 1150 Training loss 4.531875963660778\n",
      "Evaluation loss 8.0382326491562\n",
      "Epoch 1151 iteration 0 loss 0.433706134557724\n",
      "Epoch 1151 iteration 100 loss 2.2914962768554688\n",
      "Epoch 1151 iteration 200 loss 7.422743797302246\n",
      "Epoch 1151 Training loss 4.528475631820938\n",
      "Epoch 1152 iteration 0 loss 0.44256725907325745\n",
      "Epoch 1152 iteration 100 loss 2.3284401893615723\n",
      "Epoch 1152 iteration 200 loss 7.410462856292725\n",
      "Epoch 1152 Training loss 4.545996398456713\n",
      "Epoch 1153 iteration 0 loss 0.36267805099487305\n",
      "Epoch 1153 iteration 100 loss 2.3568899631500244\n",
      "Epoch 1153 iteration 200 loss 7.405864715576172\n",
      "Epoch 1153 Training loss 4.543253339711504\n",
      "Epoch 1154 iteration 0 loss 0.43935903906822205\n",
      "Epoch 1154 iteration 100 loss 2.4188973903656006\n",
      "Epoch 1154 iteration 200 loss 7.357603549957275\n",
      "Epoch 1154 Training loss 4.5372441661478184\n",
      "Epoch 1155 iteration 0 loss 0.4422469735145569\n",
      "Epoch 1155 iteration 100 loss 2.3451998233795166\n",
      "Epoch 1155 iteration 200 loss 7.387826919555664\n",
      "Epoch 1155 Training loss 4.5758523991033515\n",
      "Evaluation loss 8.072234085743434\n",
      "Epoch 1156 iteration 0 loss 0.44641804695129395\n",
      "Epoch 1156 iteration 100 loss 2.439708709716797\n",
      "Epoch 1156 iteration 200 loss 7.402054309844971\n",
      "Epoch 1156 Training loss 4.577148075379013\n",
      "Epoch 1157 iteration 0 loss 0.4314773678779602\n",
      "Epoch 1157 iteration 100 loss 2.4922027587890625\n",
      "Epoch 1157 iteration 200 loss 7.336331844329834\n",
      "Epoch 1157 Training loss 4.580489946548051\n",
      "Epoch 1158 iteration 0 loss 0.4026853144168854\n",
      "Epoch 1158 iteration 100 loss 2.4377708435058594\n",
      "Epoch 1158 iteration 200 loss 7.508076190948486\n",
      "Epoch 1158 Training loss 4.577327181924362\n",
      "Epoch 1159 iteration 0 loss 0.45207563042640686\n",
      "Epoch 1159 iteration 100 loss 2.5647521018981934\n",
      "Epoch 1159 iteration 200 loss 7.31979513168335\n",
      "Epoch 1159 Training loss 4.583012486818097\n",
      "Epoch 1160 iteration 0 loss 0.4242253303527832\n",
      "Epoch 1160 iteration 100 loss 2.521075487136841\n",
      "Epoch 1160 iteration 200 loss 7.327576160430908\n",
      "Epoch 1160 Training loss 4.572079088939173\n",
      "Evaluation loss 8.121729575024801\n",
      "Epoch 1161 iteration 0 loss 0.3961776793003082\n",
      "Epoch 1161 iteration 100 loss 2.4141016006469727\n",
      "Epoch 1161 iteration 200 loss 7.506565570831299\n",
      "Epoch 1161 Training loss 4.577296591281402\n",
      "Epoch 1162 iteration 0 loss 0.4896823465824127\n",
      "Epoch 1162 iteration 100 loss 2.251452922821045\n",
      "Epoch 1162 iteration 200 loss 7.495995998382568\n",
      "Epoch 1162 Training loss 4.594979026758541\n",
      "Epoch 1163 iteration 0 loss 0.4724093973636627\n",
      "Epoch 1163 iteration 100 loss 2.2591638565063477\n",
      "Epoch 1163 iteration 200 loss 7.562892436981201\n",
      "Epoch 1163 Training loss 4.591357047622021\n",
      "Epoch 1164 iteration 0 loss 0.37356671690940857\n",
      "Epoch 1164 iteration 100 loss 2.265429735183716\n",
      "Epoch 1164 iteration 200 loss 7.374866962432861\n",
      "Epoch 1164 Training loss 4.600380838121313\n",
      "Epoch 1165 iteration 0 loss 0.44105252623558044\n",
      "Epoch 1165 iteration 100 loss 2.322622060775757\n",
      "Epoch 1165 iteration 200 loss 7.605432987213135\n",
      "Epoch 1165 Training loss 4.582536438801536\n",
      "Evaluation loss 8.073568379173864\n",
      "Epoch 1166 iteration 0 loss 0.3890158534049988\n",
      "Epoch 1166 iteration 100 loss 2.4574756622314453\n",
      "Epoch 1166 iteration 200 loss 7.496331214904785\n",
      "Epoch 1166 Training loss 4.579374476389007\n",
      "Epoch 1167 iteration 0 loss 0.43671664595603943\n",
      "Epoch 1167 iteration 100 loss 2.3271331787109375\n",
      "Epoch 1167 iteration 200 loss 7.32588005065918\n",
      "Epoch 1167 Training loss 4.590695514955705\n",
      "Epoch 1168 iteration 0 loss 0.42434507608413696\n",
      "Epoch 1168 iteration 100 loss 2.3377785682678223\n",
      "Epoch 1168 iteration 200 loss 7.326149940490723\n",
      "Epoch 1168 Training loss 4.600974974392192\n",
      "Epoch 1169 iteration 0 loss 0.4162171483039856\n",
      "Epoch 1169 iteration 100 loss 2.3170108795166016\n",
      "Epoch 1169 iteration 200 loss 7.442574977874756\n",
      "Epoch 1169 Training loss 4.611245567842164\n",
      "Epoch 1170 iteration 0 loss 0.50664883852005\n",
      "Epoch 1170 iteration 100 loss 2.4480140209198\n",
      "Epoch 1170 iteration 200 loss 7.559877395629883\n",
      "Epoch 1170 Training loss 4.6214109683967255\n",
      "Evaluation loss 8.110137053674359\n",
      "Epoch 1171 iteration 0 loss 0.4403951168060303\n",
      "Epoch 1171 iteration 100 loss 2.270617961883545\n",
      "Epoch 1171 iteration 200 loss 7.6015305519104\n",
      "Epoch 1171 Training loss 4.624798678543073\n",
      "Epoch 1172 iteration 0 loss 0.4222750663757324\n",
      "Epoch 1172 iteration 100 loss 2.332627773284912\n",
      "Epoch 1172 iteration 200 loss 7.503337860107422\n",
      "Epoch 1172 Training loss 4.623171007368709\n",
      "Epoch 1173 iteration 0 loss 0.5243228673934937\n",
      "Epoch 1173 iteration 100 loss 2.42134428024292\n",
      "Epoch 1173 iteration 200 loss 7.463107109069824\n",
      "Epoch 1173 Training loss 4.628454844763266\n",
      "Epoch 1174 iteration 0 loss 0.4538347125053406\n",
      "Epoch 1174 iteration 100 loss 2.1942179203033447\n",
      "Epoch 1174 iteration 200 loss 7.459832191467285\n",
      "Epoch 1174 Training loss 4.607933905262743\n",
      "Epoch 1175 iteration 0 loss 0.37575387954711914\n",
      "Epoch 1175 iteration 100 loss 2.2938766479492188\n",
      "Epoch 1175 iteration 200 loss 7.407320976257324\n",
      "Epoch 1175 Training loss 4.626540620513722\n",
      "Evaluation loss 8.04434699600346\n",
      "Epoch 1176 iteration 0 loss 0.40232959389686584\n",
      "Epoch 1176 iteration 100 loss 2.3751368522644043\n",
      "Epoch 1176 iteration 200 loss 7.401843547821045\n",
      "Epoch 1176 Training loss 4.616294820851354\n",
      "Epoch 1177 iteration 0 loss 0.4494260251522064\n",
      "Epoch 1177 iteration 100 loss 2.399855375289917\n",
      "Epoch 1177 iteration 200 loss 7.400898456573486\n",
      "Epoch 1177 Training loss 4.640054300951685\n",
      "Epoch 1178 iteration 0 loss 0.43491679430007935\n",
      "Epoch 1178 iteration 100 loss 2.25555682182312\n",
      "Epoch 1178 iteration 200 loss 7.609653472900391\n",
      "Epoch 1178 Training loss 4.646465453134109\n",
      "Epoch 1179 iteration 0 loss 0.5420815944671631\n",
      "Epoch 1179 iteration 100 loss 2.411283016204834\n",
      "Epoch 1179 iteration 200 loss 7.454989433288574\n",
      "Epoch 1179 Training loss 4.656967066196069\n",
      "Epoch 1180 iteration 0 loss 0.5284686088562012\n",
      "Epoch 1180 iteration 100 loss 2.4313113689422607\n",
      "Epoch 1180 iteration 200 loss 7.307076930999756\n",
      "Epoch 1180 Training loss 4.664745069255328\n",
      "Evaluation loss 8.094526813211564\n",
      "Epoch 1181 iteration 0 loss 0.474536657333374\n",
      "Epoch 1181 iteration 100 loss 2.4560019969940186\n",
      "Epoch 1181 iteration 200 loss 7.435670375823975\n",
      "Epoch 1181 Training loss 4.6624881554500215\n",
      "Epoch 1182 iteration 0 loss 0.5781351327896118\n",
      "Epoch 1182 iteration 100 loss 2.2931647300720215\n",
      "Epoch 1182 iteration 200 loss 7.451970100402832\n",
      "Epoch 1182 Training loss 4.658664207505112\n",
      "Epoch 1183 iteration 0 loss 0.4794865548610687\n",
      "Epoch 1183 iteration 100 loss 2.296907424926758\n",
      "Epoch 1183 iteration 200 loss 7.420188903808594\n",
      "Epoch 1183 Training loss 4.670360629589322\n",
      "Epoch 1184 iteration 0 loss 0.5311445593833923\n",
      "Epoch 1184 iteration 100 loss 2.5173556804656982\n",
      "Epoch 1184 iteration 200 loss 7.453200817108154\n",
      "Epoch 1184 Training loss 4.670837618781306\n",
      "Epoch 1185 iteration 0 loss 0.5609508752822876\n",
      "Epoch 1185 iteration 100 loss 2.435434579849243\n",
      "Epoch 1185 iteration 200 loss 7.601622104644775\n",
      "Epoch 1185 Training loss 4.678901408194469\n",
      "Evaluation loss 8.105723694739709\n",
      "Epoch 1186 iteration 0 loss 0.5380774736404419\n",
      "Epoch 1186 iteration 100 loss 2.4179847240448\n",
      "Epoch 1186 iteration 200 loss 7.713833808898926\n",
      "Epoch 1186 Training loss 4.683937883396143\n",
      "Epoch 1187 iteration 0 loss 0.5356382131576538\n",
      "Epoch 1187 iteration 100 loss 2.387847661972046\n",
      "Epoch 1187 iteration 200 loss 7.633573532104492\n",
      "Epoch 1187 Training loss 4.696034897270518\n",
      "Epoch 1188 iteration 0 loss 0.5157008171081543\n",
      "Epoch 1188 iteration 100 loss 2.2694530487060547\n",
      "Epoch 1188 iteration 200 loss 7.597121238708496\n",
      "Epoch 1188 Training loss 4.689147759966664\n",
      "Epoch 1189 iteration 0 loss 0.4856816232204437\n",
      "Epoch 1189 iteration 100 loss 2.4959912300109863\n",
      "Epoch 1189 iteration 200 loss 7.559054374694824\n",
      "Epoch 1189 Training loss 4.702107884329716\n",
      "Epoch 1190 iteration 0 loss 0.5909015536308289\n",
      "Epoch 1190 iteration 100 loss 2.3332130908966064\n",
      "Epoch 1190 iteration 200 loss 7.764991760253906\n",
      "Epoch 1190 Training loss 4.684852090887405\n",
      "Evaluation loss 8.11048017245208\n",
      "Epoch 1191 iteration 0 loss 0.5259701609611511\n",
      "Epoch 1191 iteration 100 loss 2.415900707244873\n",
      "Epoch 1191 iteration 200 loss 7.496406078338623\n",
      "Epoch 1191 Training loss 4.694939397013602\n",
      "Epoch 1192 iteration 0 loss 0.44192153215408325\n",
      "Epoch 1192 iteration 100 loss 2.277496814727783\n",
      "Epoch 1192 iteration 200 loss 7.696902275085449\n",
      "Epoch 1192 Training loss 4.708067033499619\n",
      "Epoch 1193 iteration 0 loss 0.4236989915370941\n",
      "Epoch 1193 iteration 100 loss 2.432318925857544\n",
      "Epoch 1193 iteration 200 loss 7.40925407409668\n",
      "Epoch 1193 Training loss 4.698659295298005\n",
      "Epoch 1194 iteration 0 loss 0.45678573846817017\n",
      "Epoch 1194 iteration 100 loss 2.3581643104553223\n",
      "Epoch 1194 iteration 200 loss 7.648171424865723\n",
      "Epoch 1194 Training loss 4.7033472878635045\n",
      "Epoch 1195 iteration 0 loss 0.5550459623336792\n",
      "Epoch 1195 iteration 100 loss 2.296916961669922\n",
      "Epoch 1195 iteration 200 loss 7.493532180786133\n",
      "Epoch 1195 Training loss 4.719782926390268\n",
      "Evaluation loss 8.139082825594643\n",
      "Epoch 1196 iteration 0 loss 0.43391838669776917\n",
      "Epoch 1196 iteration 100 loss 2.3835675716400146\n",
      "Epoch 1196 iteration 200 loss 7.56504487991333\n",
      "Epoch 1196 Training loss 4.7221800044432225\n",
      "Epoch 1197 iteration 0 loss 0.4567212462425232\n",
      "Epoch 1197 iteration 100 loss 2.4279985427856445\n",
      "Epoch 1197 iteration 200 loss 7.631459712982178\n",
      "Epoch 1197 Training loss 4.721258821052652\n",
      "Epoch 1198 iteration 0 loss 0.43875718116760254\n",
      "Epoch 1198 iteration 100 loss 2.4408812522888184\n",
      "Epoch 1198 iteration 200 loss 7.672730922698975\n",
      "Epoch 1198 Training loss 4.725260597083615\n",
      "Epoch 1199 iteration 0 loss 0.43354910612106323\n",
      "Epoch 1199 iteration 100 loss 2.3494908809661865\n",
      "Epoch 1199 iteration 200 loss 7.874075889587402\n",
      "Epoch 1199 Training loss 4.719240522420127\n",
      "Epoch 1200 iteration 0 loss 0.43420109152793884\n",
      "Epoch 1200 iteration 100 loss 2.5199837684631348\n",
      "Epoch 1200 iteration 200 loss 7.673880100250244\n",
      "Epoch 1200 Training loss 4.750165791472144\n",
      "Evaluation loss 8.125791144118898\n",
      "Epoch 1201 iteration 0 loss 0.5263489484786987\n",
      "Epoch 1201 iteration 100 loss 2.358144760131836\n",
      "Epoch 1201 iteration 200 loss 7.569223403930664\n",
      "Epoch 1201 Training loss 4.74863412963574\n",
      "Epoch 1202 iteration 0 loss 0.4496740698814392\n",
      "Epoch 1202 iteration 100 loss 2.3794233798980713\n",
      "Epoch 1202 iteration 200 loss 7.836265563964844\n",
      "Epoch 1202 Training loss 4.740174630205284\n",
      "Epoch 1203 iteration 0 loss 0.4333499073982239\n",
      "Epoch 1203 iteration 100 loss 2.4664559364318848\n",
      "Epoch 1203 iteration 200 loss 7.540241718292236\n",
      "Epoch 1203 Training loss 4.7435094839582375\n",
      "Epoch 1204 iteration 0 loss 0.4154946208000183\n",
      "Epoch 1204 iteration 100 loss 2.3376333713531494\n",
      "Epoch 1204 iteration 200 loss 7.51885461807251\n",
      "Epoch 1204 Training loss 4.763178954638808\n",
      "Epoch 1205 iteration 0 loss 0.36447346210479736\n",
      "Epoch 1205 iteration 100 loss 2.3406412601470947\n",
      "Epoch 1205 iteration 200 loss 7.733283996582031\n",
      "Epoch 1205 Training loss 4.770603846064886\n",
      "Evaluation loss 8.163996032487946\n",
      "Epoch 1206 iteration 0 loss 0.5024574398994446\n",
      "Epoch 1206 iteration 100 loss 2.528993844985962\n",
      "Epoch 1206 iteration 200 loss 7.540121555328369\n",
      "Epoch 1206 Training loss 4.756189596773783\n",
      "Epoch 1207 iteration 0 loss 0.371896356344223\n",
      "Epoch 1207 iteration 100 loss 2.487382173538208\n",
      "Epoch 1207 iteration 200 loss 7.665314197540283\n",
      "Epoch 1207 Training loss 4.771808654463373\n",
      "Epoch 1208 iteration 0 loss 0.42689356207847595\n",
      "Epoch 1208 iteration 100 loss 2.4774093627929688\n",
      "Epoch 1208 iteration 200 loss 7.616642951965332\n",
      "Epoch 1208 Training loss 4.78015493865667\n",
      "Epoch 1209 iteration 0 loss 0.2846367657184601\n",
      "Epoch 1209 iteration 100 loss 2.4839704036712646\n",
      "Epoch 1209 iteration 200 loss 7.671724319458008\n",
      "Epoch 1209 Training loss 4.780733906289447\n",
      "Epoch 1210 iteration 0 loss 0.4662962853908539\n",
      "Epoch 1210 iteration 100 loss 2.518988609313965\n",
      "Epoch 1210 iteration 200 loss 7.712950229644775\n",
      "Epoch 1210 Training loss 4.778481594633485\n",
      "Evaluation loss 8.110346429388889\n",
      "Epoch 1211 iteration 0 loss 0.3931207060813904\n",
      "Epoch 1211 iteration 100 loss 2.704632043838501\n",
      "Epoch 1211 iteration 200 loss 7.54444694519043\n",
      "Epoch 1211 Training loss 4.774226364349997\n",
      "Epoch 1212 iteration 0 loss 0.39118534326553345\n",
      "Epoch 1212 iteration 100 loss 2.5849199295043945\n",
      "Epoch 1212 iteration 200 loss 7.774043560028076\n",
      "Epoch 1212 Training loss 4.798941040718889\n",
      "Epoch 1213 iteration 0 loss 0.4124901294708252\n",
      "Epoch 1213 iteration 100 loss 2.546877384185791\n",
      "Epoch 1213 iteration 200 loss 7.647149085998535\n",
      "Epoch 1213 Training loss 4.793692090311642\n",
      "Epoch 1214 iteration 0 loss 0.36434587836265564\n",
      "Epoch 1214 iteration 100 loss 2.5105297565460205\n",
      "Epoch 1214 iteration 200 loss 7.722323417663574\n",
      "Epoch 1214 Training loss 4.803246527923502\n",
      "Epoch 1215 iteration 0 loss 0.4295574724674225\n",
      "Epoch 1215 iteration 100 loss 2.70615291595459\n",
      "Epoch 1215 iteration 200 loss 7.924910545349121\n",
      "Epoch 1215 Training loss 4.805865658492991\n",
      "Evaluation loss 8.165295150353227\n",
      "Epoch 1216 iteration 0 loss 0.4409019351005554\n",
      "Epoch 1216 iteration 100 loss 2.479290008544922\n",
      "Epoch 1216 iteration 200 loss 7.776002407073975\n",
      "Epoch 1216 Training loss 4.813689397173587\n",
      "Epoch 1217 iteration 0 loss 0.38803669810295105\n",
      "Epoch 1217 iteration 100 loss 2.594959020614624\n",
      "Epoch 1217 iteration 200 loss 7.754059314727783\n",
      "Epoch 1217 Training loss 4.81361581326027\n",
      "Epoch 1218 iteration 0 loss 0.3246782720088959\n",
      "Epoch 1218 iteration 100 loss 2.6532235145568848\n",
      "Epoch 1218 iteration 200 loss 7.815680980682373\n",
      "Epoch 1218 Training loss 4.790102581881431\n",
      "Epoch 1219 iteration 0 loss 0.36403757333755493\n",
      "Epoch 1219 iteration 100 loss 2.5821361541748047\n",
      "Epoch 1219 iteration 200 loss 7.630123138427734\n",
      "Epoch 1219 Training loss 4.802434623344745\n",
      "Epoch 1220 iteration 0 loss 0.4090879559516907\n",
      "Epoch 1220 iteration 100 loss 2.507716178894043\n",
      "Epoch 1220 iteration 200 loss 7.865193843841553\n",
      "Epoch 1220 Training loss 4.8011136457418715\n",
      "Evaluation loss 8.124377981151747\n",
      "Epoch 1221 iteration 0 loss 0.404127836227417\n",
      "Epoch 1221 iteration 100 loss 2.6084141731262207\n",
      "Epoch 1221 iteration 200 loss 7.718673229217529\n",
      "Epoch 1221 Training loss 4.814052651849973\n",
      "Epoch 1222 iteration 0 loss 0.35957664251327515\n",
      "Epoch 1222 iteration 100 loss 2.535041570663452\n",
      "Epoch 1222 iteration 200 loss 7.73583984375\n",
      "Epoch 1222 Training loss 4.8094671032429845\n",
      "Epoch 1223 iteration 0 loss 0.38830655813217163\n",
      "Epoch 1223 iteration 100 loss 2.609468698501587\n",
      "Epoch 1223 iteration 200 loss 7.841063976287842\n",
      "Epoch 1223 Training loss 4.814213291798929\n",
      "Epoch 1224 iteration 0 loss 0.42587384581565857\n",
      "Epoch 1224 iteration 100 loss 2.7448136806488037\n",
      "Epoch 1224 iteration 200 loss 7.762017726898193\n",
      "Epoch 1224 Training loss 4.832680397446488\n",
      "Epoch 1225 iteration 0 loss 0.4466519355773926\n",
      "Epoch 1225 iteration 100 loss 2.7026894092559814\n",
      "Epoch 1225 iteration 200 loss 7.704010009765625\n",
      "Epoch 1225 Training loss 4.815093734653716\n",
      "Evaluation loss 8.1838492223417\n",
      "Epoch 1226 iteration 0 loss 0.448904812335968\n",
      "Epoch 1226 iteration 100 loss 2.5950253009796143\n",
      "Epoch 1226 iteration 200 loss 7.788044452667236\n",
      "Epoch 1226 Training loss 4.823816166180925\n",
      "Epoch 1227 iteration 0 loss 0.4007244110107422\n",
      "Epoch 1227 iteration 100 loss 2.5726094245910645\n",
      "Epoch 1227 iteration 200 loss 7.808144569396973\n",
      "Epoch 1227 Training loss 4.830920566171795\n",
      "Epoch 1228 iteration 0 loss 0.42161983251571655\n",
      "Epoch 1228 iteration 100 loss 2.7108302116394043\n",
      "Epoch 1228 iteration 200 loss 7.82960319519043\n",
      "Epoch 1228 Training loss 4.838894904129927\n",
      "Epoch 1229 iteration 0 loss 0.43770265579223633\n",
      "Epoch 1229 iteration 100 loss 2.6408796310424805\n",
      "Epoch 1229 iteration 200 loss 7.746453285217285\n",
      "Epoch 1229 Training loss 4.848327042362669\n",
      "Epoch 1230 iteration 0 loss 0.3819788992404938\n",
      "Epoch 1230 iteration 100 loss 2.592266798019409\n",
      "Epoch 1230 iteration 200 loss 7.838563919067383\n",
      "Epoch 1230 Training loss 4.8403942330541705\n",
      "Evaluation loss 8.122977999270894\n",
      "Epoch 1231 iteration 0 loss 0.46377500891685486\n",
      "Epoch 1231 iteration 100 loss 2.526075839996338\n",
      "Epoch 1231 iteration 200 loss 7.870179653167725\n",
      "Epoch 1231 Training loss 4.848373561429912\n",
      "Epoch 1232 iteration 0 loss 0.3642808496952057\n",
      "Epoch 1232 iteration 100 loss 2.550353527069092\n",
      "Epoch 1232 iteration 200 loss 7.963552951812744\n",
      "Epoch 1232 Training loss 4.863557730390693\n",
      "Epoch 1233 iteration 0 loss 0.41013818979263306\n",
      "Epoch 1233 iteration 100 loss 2.6250784397125244\n",
      "Epoch 1233 iteration 200 loss 7.803515434265137\n",
      "Epoch 1233 Training loss 4.860960645328258\n",
      "Epoch 1234 iteration 0 loss 0.37003010511398315\n",
      "Epoch 1234 iteration 100 loss 2.625993251800537\n",
      "Epoch 1234 iteration 200 loss 7.843220233917236\n",
      "Epoch 1234 Training loss 4.855816109740204\n",
      "Epoch 1235 iteration 0 loss 0.3457264006137848\n",
      "Epoch 1235 iteration 100 loss 2.444871664047241\n",
      "Epoch 1235 iteration 200 loss 7.697685718536377\n",
      "Epoch 1235 Training loss 4.866864493688893\n",
      "Evaluation loss 8.145714920409047\n",
      "Epoch 1236 iteration 0 loss 0.42192792892456055\n",
      "Epoch 1236 iteration 100 loss 2.520205497741699\n",
      "Epoch 1236 iteration 200 loss 7.754805564880371\n",
      "Epoch 1236 Training loss 4.852429602135201\n",
      "Epoch 1237 iteration 0 loss 0.39757412672042847\n",
      "Epoch 1237 iteration 100 loss 2.4647176265716553\n",
      "Epoch 1237 iteration 200 loss 7.651730537414551\n",
      "Epoch 1237 Training loss 4.860780782840868\n",
      "Epoch 1238 iteration 0 loss 0.47677841782569885\n",
      "Epoch 1238 iteration 100 loss 2.6859662532806396\n",
      "Epoch 1238 iteration 200 loss 7.839452266693115\n",
      "Epoch 1238 Training loss 4.873602692044357\n",
      "Epoch 1239 iteration 0 loss 0.4071930944919586\n",
      "Epoch 1239 iteration 100 loss 2.531765937805176\n",
      "Epoch 1239 iteration 200 loss 7.764411449432373\n",
      "Epoch 1239 Training loss 4.871713334127852\n",
      "Epoch 1240 iteration 0 loss 0.47139209508895874\n",
      "Epoch 1240 iteration 100 loss 2.505239725112915\n",
      "Epoch 1240 iteration 200 loss 7.943082809448242\n",
      "Epoch 1240 Training loss 4.876126601846379\n",
      "Evaluation loss 8.176155313091686\n",
      "Epoch 1241 iteration 0 loss 0.45646291971206665\n",
      "Epoch 1241 iteration 100 loss 2.6178503036499023\n",
      "Epoch 1241 iteration 200 loss 7.910623073577881\n",
      "Epoch 1241 Training loss 4.891604728407011\n",
      "Epoch 1242 iteration 0 loss 0.3789964020252228\n",
      "Epoch 1242 iteration 100 loss 2.6402883529663086\n",
      "Epoch 1242 iteration 200 loss 7.878505229949951\n",
      "Epoch 1242 Training loss 4.891023855167275\n",
      "Epoch 1243 iteration 0 loss 0.48081961274147034\n",
      "Epoch 1243 iteration 100 loss 2.5841708183288574\n",
      "Epoch 1243 iteration 200 loss 7.778500556945801\n",
      "Epoch 1243 Training loss 4.885734190338907\n",
      "Epoch 1244 iteration 0 loss 0.48247838020324707\n",
      "Epoch 1244 iteration 100 loss 2.6694984436035156\n",
      "Epoch 1244 iteration 200 loss 7.820755958557129\n",
      "Epoch 1244 Training loss 4.889075826877415\n",
      "Epoch 1245 iteration 0 loss 0.4938476085662842\n",
      "Epoch 1245 iteration 100 loss 2.7060163021087646\n",
      "Epoch 1245 iteration 200 loss 7.855442047119141\n",
      "Epoch 1245 Training loss 4.90022803070827\n",
      "Evaluation loss 8.191706168038133\n",
      "Epoch 1246 iteration 0 loss 0.47293439507484436\n",
      "Epoch 1246 iteration 100 loss 2.6964590549468994\n",
      "Epoch 1246 iteration 200 loss 7.770399570465088\n",
      "Epoch 1246 Training loss 4.897136569669359\n",
      "Epoch 1247 iteration 0 loss 0.40842920541763306\n",
      "Epoch 1247 iteration 100 loss 2.6843981742858887\n",
      "Epoch 1247 iteration 200 loss 7.871647834777832\n",
      "Epoch 1247 Training loss 4.893680999192546\n",
      "Epoch 1248 iteration 0 loss 0.44494637846946716\n",
      "Epoch 1248 iteration 100 loss 2.5425920486450195\n",
      "Epoch 1248 iteration 200 loss 7.739988327026367\n",
      "Epoch 1248 Training loss 4.895342739193466\n",
      "Epoch 1249 iteration 0 loss 0.47282594442367554\n",
      "Epoch 1249 iteration 100 loss 2.693528652191162\n",
      "Epoch 1249 iteration 200 loss 7.805783271789551\n",
      "Epoch 1249 Training loss 4.917490587084427\n",
      "Epoch 1250 iteration 0 loss 0.44329673051834106\n",
      "Epoch 1250 iteration 100 loss 2.7758162021636963\n",
      "Epoch 1250 iteration 200 loss 7.869225025177002\n",
      "Epoch 1250 Training loss 4.913285580946834\n",
      "Evaluation loss 8.200339339265986\n",
      "Epoch 1251 iteration 0 loss 0.48191529512405396\n",
      "Epoch 1251 iteration 100 loss 2.6506595611572266\n",
      "Epoch 1251 iteration 200 loss 7.878126621246338\n",
      "Epoch 1251 Training loss 4.916135109789155\n",
      "Epoch 1252 iteration 0 loss 0.3962807059288025\n",
      "Epoch 1252 iteration 100 loss 2.6185779571533203\n",
      "Epoch 1252 iteration 200 loss 7.912489414215088\n",
      "Epoch 1252 Training loss 4.912799755211208\n",
      "Epoch 1253 iteration 0 loss 0.47755375504493713\n",
      "Epoch 1253 iteration 100 loss 2.6621620655059814\n",
      "Epoch 1253 iteration 200 loss 7.807697296142578\n",
      "Epoch 1253 Training loss 4.920714686920859\n",
      "Epoch 1254 iteration 0 loss 0.4070073962211609\n",
      "Epoch 1254 iteration 100 loss 2.6438794136047363\n",
      "Epoch 1254 iteration 200 loss 7.867447853088379\n",
      "Epoch 1254 Training loss 4.918003265789476\n",
      "Epoch 1255 iteration 0 loss 0.4871703088283539\n",
      "Epoch 1255 iteration 100 loss 2.841625690460205\n",
      "Epoch 1255 iteration 200 loss 7.888868808746338\n",
      "Epoch 1255 Training loss 4.915910064578135\n",
      "Evaluation loss 8.223284139569481\n",
      "Epoch 1256 iteration 0 loss 0.41518163681030273\n",
      "Epoch 1256 iteration 100 loss 2.6832234859466553\n",
      "Epoch 1256 iteration 200 loss 7.781672954559326\n",
      "Epoch 1256 Training loss 4.9350915735464\n",
      "Epoch 1257 iteration 0 loss 0.4597055912017822\n",
      "Epoch 1257 iteration 100 loss 2.8014566898345947\n",
      "Epoch 1257 iteration 200 loss 7.846413612365723\n",
      "Epoch 1257 Training loss 4.937729613680909\n",
      "Epoch 1258 iteration 0 loss 0.4160328507423401\n",
      "Epoch 1258 iteration 100 loss 2.8197338581085205\n",
      "Epoch 1258 iteration 200 loss 7.847602367401123\n",
      "Epoch 1258 Training loss 4.930663950376502\n",
      "Epoch 1259 iteration 0 loss 0.4289230704307556\n",
      "Epoch 1259 iteration 100 loss 2.7346134185791016\n",
      "Epoch 1259 iteration 200 loss 7.860766410827637\n",
      "Epoch 1259 Training loss 4.936585958952343\n",
      "Epoch 1260 iteration 0 loss 0.395484060049057\n",
      "Epoch 1260 iteration 100 loss 2.5474138259887695\n",
      "Epoch 1260 iteration 200 loss 7.869657516479492\n",
      "Epoch 1260 Training loss 4.935276384289952\n",
      "Evaluation loss 8.214664218204137\n",
      "Epoch 1261 iteration 0 loss 0.5109405517578125\n",
      "Epoch 1261 iteration 100 loss 2.679016351699829\n",
      "Epoch 1261 iteration 200 loss 7.736539840698242\n",
      "Epoch 1261 Training loss 4.942996238674597\n",
      "Epoch 1262 iteration 0 loss 0.4779256284236908\n",
      "Epoch 1262 iteration 100 loss 2.7358815670013428\n",
      "Epoch 1262 iteration 200 loss 7.852133274078369\n",
      "Epoch 1262 Training loss 4.944356706951402\n",
      "Epoch 1263 iteration 0 loss 0.39807581901550293\n",
      "Epoch 1263 iteration 100 loss 2.6601250171661377\n",
      "Epoch 1263 iteration 200 loss 7.839004993438721\n",
      "Epoch 1263 Training loss 4.933284161711886\n",
      "Epoch 1264 iteration 0 loss 0.4606429636478424\n",
      "Epoch 1264 iteration 100 loss 2.737044095993042\n",
      "Epoch 1264 iteration 200 loss 7.99811315536499\n",
      "Epoch 1264 Training loss 4.953621938205178\n",
      "Epoch 1265 iteration 0 loss 0.4676958918571472\n",
      "Epoch 1265 iteration 100 loss 2.815448760986328\n",
      "Epoch 1265 iteration 200 loss 7.886616230010986\n",
      "Epoch 1265 Training loss 4.949795817340138\n",
      "Evaluation loss 8.206268398564987\n",
      "Epoch 1266 iteration 0 loss 0.45753931999206543\n",
      "Epoch 1266 iteration 100 loss 2.74330472946167\n",
      "Epoch 1266 iteration 200 loss 7.84783935546875\n",
      "Epoch 1266 Training loss 4.950571964068995\n",
      "Epoch 1267 iteration 0 loss 0.4301135540008545\n",
      "Epoch 1267 iteration 100 loss 2.7044596672058105\n",
      "Epoch 1267 iteration 200 loss 7.857151031494141\n",
      "Epoch 1267 Training loss 4.944925290835468\n",
      "Epoch 1268 iteration 0 loss 0.44954052567481995\n",
      "Epoch 1268 iteration 100 loss 2.7250351905822754\n",
      "Epoch 1268 iteration 200 loss 7.718386173248291\n",
      "Epoch 1268 Training loss 4.946693677726158\n",
      "Epoch 1269 iteration 0 loss 0.4476490020751953\n",
      "Epoch 1269 iteration 100 loss 2.851508378982544\n",
      "Epoch 1269 iteration 200 loss 7.930727481842041\n",
      "Epoch 1269 Training loss 4.952094414428986\n",
      "Epoch 1270 iteration 0 loss 0.4613613188266754\n",
      "Epoch 1270 iteration 100 loss 2.8425498008728027\n",
      "Epoch 1270 iteration 200 loss 7.782121181488037\n",
      "Epoch 1270 Training loss 4.967042188269359\n",
      "Evaluation loss 8.235969388496416\n",
      "Epoch 1271 iteration 0 loss 0.4567430913448334\n",
      "Epoch 1271 iteration 100 loss 2.8013241291046143\n",
      "Epoch 1271 iteration 200 loss 7.771780014038086\n",
      "Epoch 1271 Training loss 4.967514449171447\n",
      "Epoch 1272 iteration 0 loss 0.4746970534324646\n",
      "Epoch 1272 iteration 100 loss 2.7991182804107666\n",
      "Epoch 1272 iteration 200 loss 7.891031265258789\n",
      "Epoch 1272 Training loss 4.979773798547194\n",
      "Epoch 1273 iteration 0 loss 0.5287774205207825\n",
      "Epoch 1273 iteration 100 loss 2.771850347518921\n",
      "Epoch 1273 iteration 200 loss 7.914345741271973\n",
      "Epoch 1273 Training loss 4.980989320273211\n",
      "Epoch 1274 iteration 0 loss 0.46520286798477173\n",
      "Epoch 1274 iteration 100 loss 2.686459541320801\n",
      "Epoch 1274 iteration 200 loss 7.871428966522217\n",
      "Epoch 1274 Training loss 4.976520984049067\n",
      "Epoch 1275 iteration 0 loss 0.5003419518470764\n",
      "Epoch 1275 iteration 100 loss 2.7350242137908936\n",
      "Epoch 1275 iteration 200 loss 7.767007350921631\n",
      "Epoch 1275 Training loss 4.987554968613564\n",
      "Evaluation loss 8.20431750504611\n",
      "Epoch 1276 iteration 0 loss 0.4794226288795471\n",
      "Epoch 1276 iteration 100 loss 2.798281669616699\n",
      "Epoch 1276 iteration 200 loss 7.858097076416016\n",
      "Epoch 1276 Training loss 4.98820248170909\n",
      "Epoch 1277 iteration 0 loss 0.5224997997283936\n",
      "Epoch 1277 iteration 100 loss 2.8326523303985596\n",
      "Epoch 1277 iteration 200 loss 7.776723861694336\n",
      "Epoch 1277 Training loss 4.983507388296746\n",
      "Epoch 1278 iteration 0 loss 0.4671332836151123\n",
      "Epoch 1278 iteration 100 loss 2.9166669845581055\n",
      "Epoch 1278 iteration 200 loss 7.908007621765137\n",
      "Epoch 1278 Training loss 4.989203325719894\n",
      "Epoch 1279 iteration 0 loss 0.40730729699134827\n",
      "Epoch 1279 iteration 100 loss 2.860835552215576\n",
      "Epoch 1279 iteration 200 loss 7.881520748138428\n",
      "Epoch 1279 Training loss 5.008395678669018\n",
      "Epoch 1280 iteration 0 loss 0.46396154165267944\n",
      "Epoch 1280 iteration 100 loss 2.74198842048645\n",
      "Epoch 1280 iteration 200 loss 8.010663032531738\n",
      "Epoch 1280 Training loss 5.007732673424746\n",
      "Evaluation loss 8.207063348043702\n",
      "Epoch 1281 iteration 0 loss 0.5058115720748901\n",
      "Epoch 1281 iteration 100 loss 2.8837742805480957\n",
      "Epoch 1281 iteration 200 loss 7.941089630126953\n",
      "Epoch 1281 Training loss 5.026137768529279\n",
      "Epoch 1282 iteration 0 loss 0.4636412262916565\n",
      "Epoch 1282 iteration 100 loss 2.775204658508301\n",
      "Epoch 1282 iteration 200 loss 7.99389123916626\n",
      "Epoch 1282 Training loss 5.032315079552557\n",
      "Epoch 1283 iteration 0 loss 0.41870051622390747\n",
      "Epoch 1283 iteration 100 loss 2.870972156524658\n",
      "Epoch 1283 iteration 200 loss 7.897820949554443\n",
      "Epoch 1283 Training loss 5.025183353314883\n",
      "Epoch 1284 iteration 0 loss 0.45178136229515076\n",
      "Epoch 1284 iteration 100 loss 2.7125096321105957\n",
      "Epoch 1284 iteration 200 loss 8.00031566619873\n",
      "Epoch 1284 Training loss 5.047449660642505\n",
      "Epoch 1285 iteration 0 loss 0.4681197702884674\n",
      "Epoch 1285 iteration 100 loss 2.769092559814453\n",
      "Epoch 1285 iteration 200 loss 7.953309535980225\n",
      "Epoch 1285 Training loss 5.0486529027207885\n",
      "Evaluation loss 8.224128442852203\n",
      "Epoch 1286 iteration 0 loss 0.5076805949211121\n",
      "Epoch 1286 iteration 100 loss 2.8525898456573486\n",
      "Epoch 1286 iteration 200 loss 7.972311973571777\n",
      "Epoch 1286 Training loss 5.0535960209769275\n",
      "Epoch 1287 iteration 0 loss 0.4531119465827942\n",
      "Epoch 1287 iteration 100 loss 2.845043897628784\n",
      "Epoch 1287 iteration 200 loss 7.922086715698242\n",
      "Epoch 1287 Training loss 5.050807633158379\n",
      "Epoch 1288 iteration 0 loss 0.47584134340286255\n",
      "Epoch 1288 iteration 100 loss 2.87614369392395\n",
      "Epoch 1288 iteration 200 loss 7.976928234100342\n",
      "Epoch 1288 Training loss 5.050474417156962\n",
      "Epoch 1289 iteration 0 loss 0.5087481737136841\n",
      "Epoch 1289 iteration 100 loss 2.8343143463134766\n",
      "Epoch 1289 iteration 200 loss 7.922946929931641\n",
      "Epoch 1289 Training loss 5.036771833656808\n",
      "Epoch 1290 iteration 0 loss 0.5591133832931519\n",
      "Epoch 1290 iteration 100 loss 2.8553614616394043\n",
      "Epoch 1290 iteration 200 loss 7.858236789703369\n",
      "Epoch 1290 Training loss 5.038756392178262\n",
      "Evaluation loss 8.207079013711448\n",
      "Epoch 1291 iteration 0 loss 0.4693102240562439\n",
      "Epoch 1291 iteration 100 loss 2.9034645557403564\n",
      "Epoch 1291 iteration 200 loss 8.100258827209473\n",
      "Epoch 1291 Training loss 5.069196327836051\n",
      "Epoch 1292 iteration 0 loss 0.4821591377258301\n",
      "Epoch 1292 iteration 100 loss 3.068382978439331\n",
      "Epoch 1292 iteration 200 loss 8.009858131408691\n",
      "Epoch 1292 Training loss 5.076779508610029\n",
      "Epoch 1293 iteration 0 loss 0.4771568179130554\n",
      "Epoch 1293 iteration 100 loss 2.9402008056640625\n",
      "Epoch 1293 iteration 200 loss 7.915047645568848\n",
      "Epoch 1293 Training loss 5.0714657309049445\n",
      "Epoch 1294 iteration 0 loss 0.46474266052246094\n",
      "Epoch 1294 iteration 100 loss 2.842129945755005\n",
      "Epoch 1294 iteration 200 loss 8.003971099853516\n",
      "Epoch 1294 Training loss 5.078174915278317\n",
      "Epoch 1295 iteration 0 loss 0.5536622405052185\n",
      "Epoch 1295 iteration 100 loss 3.0253825187683105\n",
      "Epoch 1295 iteration 200 loss 7.963552474975586\n",
      "Epoch 1295 Training loss 5.062012752269555\n",
      "Evaluation loss 8.267056850155784\n",
      "Epoch 1296 iteration 0 loss 0.5288788676261902\n",
      "Epoch 1296 iteration 100 loss 2.8941733837127686\n",
      "Epoch 1296 iteration 200 loss 7.995035648345947\n",
      "Epoch 1296 Training loss 5.060431382677233\n",
      "Epoch 1297 iteration 0 loss 0.5141924619674683\n",
      "Epoch 1297 iteration 100 loss 2.9098238945007324\n",
      "Epoch 1297 iteration 200 loss 7.939422607421875\n",
      "Epoch 1297 Training loss 5.076573046002209\n",
      "Epoch 1298 iteration 0 loss 0.5158937573432922\n",
      "Epoch 1298 iteration 100 loss 2.9058570861816406\n",
      "Epoch 1298 iteration 200 loss 7.860729217529297\n",
      "Epoch 1298 Training loss 5.0804755597285745\n",
      "Epoch 1299 iteration 0 loss 0.5218566060066223\n",
      "Epoch 1299 iteration 100 loss 2.9179091453552246\n",
      "Epoch 1299 iteration 200 loss 7.923937797546387\n",
      "Epoch 1299 Training loss 5.089685240514744\n",
      "Epoch 1300 iteration 0 loss 0.5193483233451843\n",
      "Epoch 1300 iteration 100 loss 2.969064950942993\n",
      "Epoch 1300 iteration 200 loss 7.977883815765381\n",
      "Epoch 1300 Training loss 5.094178704859407\n",
      "Evaluation loss 8.261626868778073\n",
      "Epoch 1301 iteration 0 loss 0.49481645226478577\n",
      "Epoch 1301 iteration 100 loss 2.8768882751464844\n",
      "Epoch 1301 iteration 200 loss 8.009438514709473\n",
      "Epoch 1301 Training loss 5.0996523426514555\n",
      "Epoch 1302 iteration 0 loss 0.5583693981170654\n",
      "Epoch 1302 iteration 100 loss 3.0009334087371826\n",
      "Epoch 1302 iteration 200 loss 7.965597629547119\n",
      "Epoch 1302 Training loss 5.101619979230045\n",
      "Epoch 1303 iteration 0 loss 0.5356699228286743\n",
      "Epoch 1303 iteration 100 loss 2.9621660709381104\n",
      "Epoch 1303 iteration 200 loss 8.11575698852539\n",
      "Epoch 1303 Training loss 5.087188004688356\n",
      "Epoch 1304 iteration 0 loss 0.6062918901443481\n",
      "Epoch 1304 iteration 100 loss 2.9256842136383057\n",
      "Epoch 1304 iteration 200 loss 8.052040100097656\n",
      "Epoch 1304 Training loss 5.117120686395215\n",
      "Epoch 1305 iteration 0 loss 0.5316671133041382\n",
      "Epoch 1305 iteration 100 loss 2.889641761779785\n",
      "Epoch 1305 iteration 200 loss 7.940962314605713\n",
      "Epoch 1305 Training loss 5.12174830271631\n",
      "Evaluation loss 8.257701985502946\n",
      "Epoch 1306 iteration 0 loss 0.5544556975364685\n",
      "Epoch 1306 iteration 100 loss 2.7122578620910645\n",
      "Epoch 1306 iteration 200 loss 8.05105972290039\n",
      "Epoch 1306 Training loss 5.109530594873415\n",
      "Epoch 1307 iteration 0 loss 0.4888378083705902\n",
      "Epoch 1307 iteration 100 loss 2.7557780742645264\n",
      "Epoch 1307 iteration 200 loss 8.029818534851074\n",
      "Epoch 1307 Training loss 5.11550552820789\n",
      "Epoch 1308 iteration 0 loss 0.5217993855476379\n",
      "Epoch 1308 iteration 100 loss 2.7421650886535645\n",
      "Epoch 1308 iteration 200 loss 7.955945014953613\n",
      "Epoch 1308 Training loss 5.10209845503085\n",
      "Epoch 1309 iteration 0 loss 0.5642889142036438\n",
      "Epoch 1309 iteration 100 loss 2.868769407272339\n",
      "Epoch 1309 iteration 200 loss 7.991136074066162\n",
      "Epoch 1309 Training loss 5.113194240529122\n",
      "Epoch 1310 iteration 0 loss 0.5097770690917969\n",
      "Epoch 1310 iteration 100 loss 2.739494800567627\n",
      "Epoch 1310 iteration 200 loss 7.963436126708984\n",
      "Epoch 1310 Training loss 5.130633645294961\n",
      "Evaluation loss 8.219836298237022\n",
      "Epoch 1311 iteration 0 loss 0.47553008794784546\n",
      "Epoch 1311 iteration 100 loss 2.85471773147583\n",
      "Epoch 1311 iteration 200 loss 8.128362655639648\n",
      "Epoch 1311 Training loss 5.113843920120236\n",
      "Epoch 1312 iteration 0 loss 0.502113938331604\n",
      "Epoch 1312 iteration 100 loss 2.9009697437286377\n",
      "Epoch 1312 iteration 200 loss 7.749252796173096\n",
      "Epoch 1312 Training loss 5.11689957990885\n",
      "Epoch 1313 iteration 0 loss 0.5023989677429199\n",
      "Epoch 1313 iteration 100 loss 2.862718105316162\n",
      "Epoch 1313 iteration 200 loss 8.028407096862793\n",
      "Epoch 1313 Training loss 5.118985056305954\n",
      "Epoch 1314 iteration 0 loss 0.5453130006790161\n",
      "Epoch 1314 iteration 100 loss 2.939464569091797\n",
      "Epoch 1314 iteration 200 loss 7.857295989990234\n",
      "Epoch 1314 Training loss 5.116048409521615\n",
      "Epoch 1315 iteration 0 loss 0.5379678606987\n",
      "Epoch 1315 iteration 100 loss 2.9547829627990723\n",
      "Epoch 1315 iteration 200 loss 7.960642337799072\n",
      "Epoch 1315 Training loss 5.140865214881845\n",
      "Evaluation loss 8.293634669570698\n",
      "Epoch 1316 iteration 0 loss 0.5083316564559937\n",
      "Epoch 1316 iteration 100 loss 2.893826484680176\n",
      "Epoch 1316 iteration 200 loss 7.914653778076172\n",
      "Epoch 1316 Training loss 5.133509480527914\n",
      "Epoch 1317 iteration 0 loss 0.48220139741897583\n",
      "Epoch 1317 iteration 100 loss 2.906006336212158\n",
      "Epoch 1317 iteration 200 loss 7.981125354766846\n",
      "Epoch 1317 Training loss 5.1160143845704775\n",
      "Epoch 1318 iteration 0 loss 0.5427235960960388\n",
      "Epoch 1318 iteration 100 loss 3.0508925914764404\n",
      "Epoch 1318 iteration 200 loss 7.963995456695557\n",
      "Epoch 1318 Training loss 5.134597311730524\n",
      "Epoch 1319 iteration 0 loss 0.49649062752723694\n",
      "Epoch 1319 iteration 100 loss 2.896716356277466\n",
      "Epoch 1319 iteration 200 loss 7.964861869812012\n",
      "Epoch 1319 Training loss 5.129669522896937\n",
      "Epoch 1320 iteration 0 loss 0.5304124355316162\n",
      "Epoch 1320 iteration 100 loss 2.959529399871826\n",
      "Epoch 1320 iteration 200 loss 8.012003898620605\n",
      "Epoch 1320 Training loss 5.126273339625813\n",
      "Evaluation loss 8.270078190182623\n",
      "Epoch 1321 iteration 0 loss 0.5298783779144287\n",
      "Epoch 1321 iteration 100 loss 2.9660048484802246\n",
      "Epoch 1321 iteration 200 loss 7.991837978363037\n",
      "Epoch 1321 Training loss 5.150904538216568\n",
      "Epoch 1322 iteration 0 loss 0.5278012156486511\n",
      "Epoch 1322 iteration 100 loss 2.9000279903411865\n",
      "Epoch 1322 iteration 200 loss 8.00963306427002\n",
      "Epoch 1322 Training loss 5.157719385688181\n",
      "Epoch 1323 iteration 0 loss 0.48804038763046265\n",
      "Epoch 1323 iteration 100 loss 3.0433263778686523\n",
      "Epoch 1323 iteration 200 loss 7.986217975616455\n",
      "Epoch 1323 Training loss 5.169446469449439\n",
      "Epoch 1324 iteration 0 loss 0.5276311635971069\n",
      "Epoch 1324 iteration 100 loss 2.9092624187469482\n",
      "Epoch 1324 iteration 200 loss 7.953449726104736\n",
      "Epoch 1324 Training loss 5.171297950861568\n",
      "Epoch 1325 iteration 0 loss 0.5284534692764282\n",
      "Epoch 1325 iteration 100 loss 2.8841969966888428\n",
      "Epoch 1325 iteration 200 loss 7.916975021362305\n",
      "Epoch 1325 Training loss 5.1652369665761855\n",
      "Evaluation loss 8.312723976575441\n",
      "Epoch 1326 iteration 0 loss 0.5463669896125793\n",
      "Epoch 1326 iteration 100 loss 2.9967057704925537\n",
      "Epoch 1326 iteration 200 loss 8.063206672668457\n",
      "Epoch 1326 Training loss 5.165203658516388\n",
      "Epoch 1327 iteration 0 loss 0.49384158849716187\n",
      "Epoch 1327 iteration 100 loss 3.058136463165283\n",
      "Epoch 1327 iteration 200 loss 8.139681816101074\n",
      "Epoch 1327 Training loss 5.196930047992349\n",
      "Epoch 1328 iteration 0 loss 0.5007317662239075\n",
      "Epoch 1328 iteration 100 loss 3.0590736865997314\n",
      "Epoch 1328 iteration 200 loss 8.11862564086914\n",
      "Epoch 1328 Training loss 5.204687028092812\n",
      "Epoch 1329 iteration 0 loss 0.5662996172904968\n",
      "Epoch 1329 iteration 100 loss 3.060490846633911\n",
      "Epoch 1329 iteration 200 loss 8.091703414916992\n",
      "Epoch 1329 Training loss 5.203695775436612\n",
      "Epoch 1330 iteration 0 loss 0.49932631850242615\n",
      "Epoch 1330 iteration 100 loss 3.1900196075439453\n",
      "Epoch 1330 iteration 200 loss 8.022401809692383\n",
      "Epoch 1330 Training loss 5.206998302080424\n",
      "Evaluation loss 8.319120508368751\n",
      "Epoch 1331 iteration 0 loss 0.5003405809402466\n",
      "Epoch 1331 iteration 100 loss 3.0207467079162598\n",
      "Epoch 1331 iteration 200 loss 8.224431991577148\n",
      "Epoch 1331 Training loss 5.2201582854658755\n",
      "Epoch 1332 iteration 0 loss 0.546012282371521\n",
      "Epoch 1332 iteration 100 loss 3.2517271041870117\n",
      "Epoch 1332 iteration 200 loss 8.029382705688477\n",
      "Epoch 1332 Training loss 5.215191562147744\n",
      "Epoch 1333 iteration 0 loss 0.5447984933853149\n",
      "Epoch 1333 iteration 100 loss 3.1006968021392822\n",
      "Epoch 1333 iteration 200 loss 8.113775253295898\n",
      "Epoch 1333 Training loss 5.220125377314685\n",
      "Epoch 1334 iteration 0 loss 0.5445104837417603\n",
      "Epoch 1334 iteration 100 loss 2.9940505027770996\n",
      "Epoch 1334 iteration 200 loss 8.001803398132324\n",
      "Epoch 1334 Training loss 5.21585086667204\n",
      "Epoch 1335 iteration 0 loss 0.5690979957580566\n",
      "Epoch 1335 iteration 100 loss 3.12430739402771\n",
      "Epoch 1335 iteration 200 loss 8.039080619812012\n",
      "Epoch 1335 Training loss 5.218151260392445\n",
      "Evaluation loss 8.35235977432225\n",
      "Epoch 1336 iteration 0 loss 0.5363975167274475\n",
      "Epoch 1336 iteration 100 loss 3.0161514282226562\n",
      "Epoch 1336 iteration 200 loss 8.244452476501465\n",
      "Epoch 1336 Training loss 5.228995385920722\n",
      "Epoch 1337 iteration 0 loss 0.6633256077766418\n",
      "Epoch 1337 iteration 100 loss 2.9689931869506836\n",
      "Epoch 1337 iteration 200 loss 8.077274322509766\n",
      "Epoch 1337 Training loss 5.209219470411272\n",
      "Epoch 1338 iteration 0 loss 0.5427273511886597\n",
      "Epoch 1338 iteration 100 loss 3.0883257389068604\n",
      "Epoch 1338 iteration 200 loss 8.07712173461914\n",
      "Epoch 1338 Training loss 5.227499276607955\n",
      "Epoch 1339 iteration 0 loss 0.6413536071777344\n",
      "Epoch 1339 iteration 100 loss 3.052863836288452\n",
      "Epoch 1339 iteration 200 loss 8.058917999267578\n",
      "Epoch 1339 Training loss 5.225021181937977\n",
      "Epoch 1340 iteration 0 loss 0.5408113598823547\n",
      "Epoch 1340 iteration 100 loss 3.0356907844543457\n",
      "Epoch 1340 iteration 200 loss 8.094558715820312\n",
      "Epoch 1340 Training loss 5.236457313525057\n",
      "Evaluation loss 8.35377917014436\n",
      "Epoch 1341 iteration 0 loss 0.516871988773346\n",
      "Epoch 1341 iteration 100 loss 3.0438883304595947\n",
      "Epoch 1341 iteration 200 loss 8.125560760498047\n",
      "Epoch 1341 Training loss 5.232996802044772\n",
      "Epoch 1342 iteration 0 loss 0.5678263306617737\n",
      "Epoch 1342 iteration 100 loss 3.124095916748047\n",
      "Epoch 1342 iteration 200 loss 7.966056823730469\n",
      "Epoch 1342 Training loss 5.241663333308208\n",
      "Epoch 1343 iteration 0 loss 0.5247266888618469\n",
      "Epoch 1343 iteration 100 loss 2.950212240219116\n",
      "Epoch 1343 iteration 200 loss 8.093990325927734\n",
      "Epoch 1343 Training loss 5.245133018322315\n",
      "Epoch 1344 iteration 0 loss 0.5974194407463074\n",
      "Epoch 1344 iteration 100 loss 2.935288667678833\n",
      "Epoch 1344 iteration 200 loss 8.05672836303711\n",
      "Epoch 1344 Training loss 5.265285106342354\n",
      "Epoch 1345 iteration 0 loss 0.5158323049545288\n",
      "Epoch 1345 iteration 100 loss 2.963608503341675\n",
      "Epoch 1345 iteration 200 loss 8.103399276733398\n",
      "Epoch 1345 Training loss 5.265140932458579\n",
      "Evaluation loss 8.2993231160145\n",
      "Epoch 1346 iteration 0 loss 0.5091577768325806\n",
      "Epoch 1346 iteration 100 loss 3.1686155796051025\n",
      "Epoch 1346 iteration 200 loss 8.030481338500977\n",
      "Epoch 1346 Training loss 5.265057509342061\n",
      "Epoch 1347 iteration 0 loss 0.5827574133872986\n",
      "Epoch 1347 iteration 100 loss 3.001337766647339\n",
      "Epoch 1347 iteration 200 loss 8.065773010253906\n",
      "Epoch 1347 Training loss 5.265406350024002\n",
      "Epoch 1348 iteration 0 loss 0.5930696129798889\n",
      "Epoch 1348 iteration 100 loss 3.0638346672058105\n",
      "Epoch 1348 iteration 200 loss 8.079118728637695\n",
      "Epoch 1348 Training loss 5.278685713933007\n",
      "Epoch 1349 iteration 0 loss 0.6453633308410645\n",
      "Epoch 1349 iteration 100 loss 3.2589688301086426\n",
      "Epoch 1349 iteration 200 loss 8.07875919342041\n",
      "Epoch 1349 Training loss 5.289301892493717\n",
      "Epoch 1350 iteration 0 loss 0.5881039500236511\n",
      "Epoch 1350 iteration 100 loss 2.9822146892547607\n",
      "Epoch 1350 iteration 200 loss 8.027616500854492\n",
      "Epoch 1350 Training loss 5.269025439145917\n",
      "Evaluation loss 8.383835580205046\n",
      "Epoch 1351 iteration 0 loss 0.6559053659439087\n",
      "Epoch 1351 iteration 100 loss 3.1320385932922363\n",
      "Epoch 1351 iteration 200 loss 8.095852851867676\n",
      "Epoch 1351 Training loss 5.285819011531392\n",
      "Epoch 1352 iteration 0 loss 0.6081403493881226\n",
      "Epoch 1352 iteration 100 loss 3.058129072189331\n",
      "Epoch 1352 iteration 200 loss 8.24160099029541\n",
      "Epoch 1352 Training loss 5.284736536923055\n",
      "Epoch 1353 iteration 0 loss 0.6194612979888916\n",
      "Epoch 1353 iteration 100 loss 3.0263988971710205\n",
      "Epoch 1353 iteration 200 loss 8.129067420959473\n",
      "Epoch 1353 Training loss 5.297580333937828\n",
      "Epoch 1354 iteration 0 loss 0.59333336353302\n",
      "Epoch 1354 iteration 100 loss 3.0754988193511963\n",
      "Epoch 1354 iteration 200 loss 8.132169723510742\n",
      "Epoch 1354 Training loss 5.30496612993886\n",
      "Epoch 1355 iteration 0 loss 0.6161237359046936\n",
      "Epoch 1355 iteration 100 loss 3.1881930828094482\n",
      "Epoch 1355 iteration 200 loss 8.219948768615723\n",
      "Epoch 1355 Training loss 5.2999820672316185\n",
      "Evaluation loss 8.377135564144135\n",
      "Epoch 1356 iteration 0 loss 0.6673954129219055\n",
      "Epoch 1356 iteration 100 loss 3.184126615524292\n",
      "Epoch 1356 iteration 200 loss 8.246007919311523\n",
      "Epoch 1356 Training loss 5.310120162752333\n",
      "Epoch 1357 iteration 0 loss 0.6374339461326599\n",
      "Epoch 1357 iteration 100 loss 3.171393632888794\n",
      "Epoch 1357 iteration 200 loss 8.065186500549316\n",
      "Epoch 1357 Training loss 5.291213834398393\n",
      "Epoch 1358 iteration 0 loss 0.6163021326065063\n",
      "Epoch 1358 iteration 100 loss 3.094682455062866\n",
      "Epoch 1358 iteration 200 loss 8.192350387573242\n",
      "Epoch 1358 Training loss 5.312715828171793\n",
      "Epoch 1359 iteration 0 loss 0.5896129608154297\n",
      "Epoch 1359 iteration 100 loss 3.0506887435913086\n",
      "Epoch 1359 iteration 200 loss 8.189815521240234\n",
      "Epoch 1359 Training loss 5.288793190852548\n",
      "Epoch 1360 iteration 0 loss 0.6254097819328308\n",
      "Epoch 1360 iteration 100 loss 3.233616352081299\n",
      "Epoch 1360 iteration 200 loss 8.153499603271484\n",
      "Epoch 1360 Training loss 5.309027778125187\n",
      "Evaluation loss 8.382809586800743\n",
      "Epoch 1361 iteration 0 loss 0.5997920632362366\n",
      "Epoch 1361 iteration 100 loss 3.1085586547851562\n",
      "Epoch 1361 iteration 200 loss 8.139154434204102\n",
      "Epoch 1361 Training loss 5.313177886317272\n",
      "Epoch 1362 iteration 0 loss 0.5936237573623657\n",
      "Epoch 1362 iteration 100 loss 3.099508285522461\n",
      "Epoch 1362 iteration 200 loss 8.32214069366455\n",
      "Epoch 1362 Training loss 5.314737245196362\n",
      "Epoch 1363 iteration 0 loss 0.6016952991485596\n",
      "Epoch 1363 iteration 100 loss 3.1246249675750732\n",
      "Epoch 1363 iteration 200 loss 8.258143424987793\n",
      "Epoch 1363 Training loss 5.318324384891158\n",
      "Epoch 1364 iteration 0 loss 0.6744675040245056\n",
      "Epoch 1364 iteration 100 loss 3.0640175342559814\n",
      "Epoch 1364 iteration 200 loss 8.249003410339355\n",
      "Epoch 1364 Training loss 5.319591557101129\n",
      "Epoch 1365 iteration 0 loss 0.6201176643371582\n",
      "Epoch 1365 iteration 100 loss 3.1186532974243164\n",
      "Epoch 1365 iteration 200 loss 8.093902587890625\n",
      "Epoch 1365 Training loss 5.308539541562502\n",
      "Evaluation loss 8.351711439011472\n",
      "Epoch 1366 iteration 0 loss 0.6060362458229065\n",
      "Epoch 1366 iteration 100 loss 3.092686176300049\n",
      "Epoch 1366 iteration 200 loss 8.116341590881348\n",
      "Epoch 1366 Training loss 5.311079178933269\n",
      "Epoch 1367 iteration 0 loss 0.5408695936203003\n",
      "Epoch 1367 iteration 100 loss 3.167548418045044\n",
      "Epoch 1367 iteration 200 loss 8.207488059997559\n",
      "Epoch 1367 Training loss 5.304099087491597\n",
      "Epoch 1368 iteration 0 loss 0.5718470811843872\n",
      "Epoch 1368 iteration 100 loss 3.1883790493011475\n",
      "Epoch 1368 iteration 200 loss 8.122790336608887\n",
      "Epoch 1368 Training loss 5.3215545393370105\n",
      "Epoch 1369 iteration 0 loss 0.6181410551071167\n",
      "Epoch 1369 iteration 100 loss 3.183986186981201\n",
      "Epoch 1369 iteration 200 loss 8.052580833435059\n",
      "Epoch 1369 Training loss 5.34157163277216\n",
      "Epoch 1370 iteration 0 loss 0.6616343855857849\n",
      "Epoch 1370 iteration 100 loss 3.168511390686035\n",
      "Epoch 1370 iteration 200 loss 7.978029727935791\n",
      "Epoch 1370 Training loss 5.33332214081376\n",
      "Evaluation loss 8.410789224799029\n",
      "Epoch 1371 iteration 0 loss 0.6659764647483826\n",
      "Epoch 1371 iteration 100 loss 3.156794309616089\n",
      "Epoch 1371 iteration 200 loss 8.227453231811523\n",
      "Epoch 1371 Training loss 5.339254733415687\n",
      "Epoch 1372 iteration 0 loss 0.5765549540519714\n",
      "Epoch 1372 iteration 100 loss 3.1440629959106445\n",
      "Epoch 1372 iteration 200 loss 8.277162551879883\n",
      "Epoch 1372 Training loss 5.338566006790868\n",
      "Epoch 1373 iteration 0 loss 0.557259202003479\n",
      "Epoch 1373 iteration 100 loss 3.329857587814331\n",
      "Epoch 1373 iteration 200 loss 8.222190856933594\n",
      "Epoch 1373 Training loss 5.34123398579328\n",
      "Epoch 1374 iteration 0 loss 0.5615060329437256\n",
      "Epoch 1374 iteration 100 loss 3.1396851539611816\n",
      "Epoch 1374 iteration 200 loss 8.30163288116455\n",
      "Epoch 1374 Training loss 5.330790263946036\n",
      "Epoch 1375 iteration 0 loss 0.6425902843475342\n",
      "Epoch 1375 iteration 100 loss 3.173118829727173\n",
      "Epoch 1375 iteration 200 loss 8.40124225616455\n",
      "Epoch 1375 Training loss 5.360542393718073\n",
      "Evaluation loss 8.38557636491117\n",
      "Epoch 1376 iteration 0 loss 0.6851906776428223\n",
      "Epoch 1376 iteration 100 loss 3.0585360527038574\n",
      "Epoch 1376 iteration 200 loss 8.316662788391113\n",
      "Epoch 1376 Training loss 5.359647801552383\n",
      "Epoch 1377 iteration 0 loss 0.691811740398407\n",
      "Epoch 1377 iteration 100 loss 3.0563597679138184\n",
      "Epoch 1377 iteration 200 loss 8.135758399963379\n",
      "Epoch 1377 Training loss 5.353414637877853\n",
      "Epoch 1378 iteration 0 loss 0.7055559158325195\n",
      "Epoch 1378 iteration 100 loss 3.28926682472229\n",
      "Epoch 1378 iteration 200 loss 8.203500747680664\n",
      "Epoch 1378 Training loss 5.376290326178863\n",
      "Epoch 1379 iteration 0 loss 0.6810126900672913\n",
      "Epoch 1379 iteration 100 loss 3.2435896396636963\n",
      "Epoch 1379 iteration 200 loss 8.29979419708252\n",
      "Epoch 1379 Training loss 5.3613818948213385\n",
      "Epoch 1380 iteration 0 loss 0.5945824384689331\n",
      "Epoch 1380 iteration 100 loss 3.2853972911834717\n",
      "Epoch 1380 iteration 200 loss 8.139888763427734\n",
      "Epoch 1380 Training loss 5.378794778353999\n",
      "Evaluation loss 8.427561206831621\n",
      "Epoch 1381 iteration 0 loss 0.6732970476150513\n",
      "Epoch 1381 iteration 100 loss 3.102447271347046\n",
      "Epoch 1381 iteration 200 loss 8.119279861450195\n",
      "Epoch 1381 Training loss 5.373975014093336\n",
      "Epoch 1382 iteration 0 loss 0.614843487739563\n",
      "Epoch 1382 iteration 100 loss 3.2968668937683105\n",
      "Epoch 1382 iteration 200 loss 8.141586303710938\n",
      "Epoch 1382 Training loss 5.385668603578985\n",
      "Epoch 1383 iteration 0 loss 0.6477109789848328\n",
      "Epoch 1383 iteration 100 loss 3.1386606693267822\n",
      "Epoch 1383 iteration 200 loss 8.186262130737305\n",
      "Epoch 1383 Training loss 5.3810061828615465\n",
      "Epoch 1384 iteration 0 loss 0.6259117126464844\n",
      "Epoch 1384 iteration 100 loss 3.253490686416626\n",
      "Epoch 1384 iteration 200 loss 8.182343482971191\n",
      "Epoch 1384 Training loss 5.386914354346045\n",
      "Epoch 1385 iteration 0 loss 0.7049142122268677\n",
      "Epoch 1385 iteration 100 loss 3.297403573989868\n",
      "Epoch 1385 iteration 200 loss 8.147557258605957\n",
      "Epoch 1385 Training loss 5.383532418882578\n",
      "Evaluation loss 8.415978186080942\n",
      "Epoch 1386 iteration 0 loss 0.7075464129447937\n",
      "Epoch 1386 iteration 100 loss 3.1768581867218018\n",
      "Epoch 1386 iteration 200 loss 8.280113220214844\n",
      "Epoch 1386 Training loss 5.36770143596198\n",
      "Epoch 1387 iteration 0 loss 0.6421054005622864\n",
      "Epoch 1387 iteration 100 loss 3.2323241233825684\n",
      "Epoch 1387 iteration 200 loss 8.307467460632324\n",
      "Epoch 1387 Training loss 5.383591508250844\n",
      "Epoch 1388 iteration 0 loss 0.6384419202804565\n",
      "Epoch 1388 iteration 100 loss 3.3023507595062256\n",
      "Epoch 1388 iteration 200 loss 8.214947700500488\n",
      "Epoch 1388 Training loss 5.404813685498638\n",
      "Epoch 1389 iteration 0 loss 0.6020862460136414\n",
      "Epoch 1389 iteration 100 loss 3.272967576980591\n",
      "Epoch 1389 iteration 200 loss 8.284310340881348\n",
      "Epoch 1389 Training loss 5.422586065217361\n",
      "Epoch 1390 iteration 0 loss 0.6771223545074463\n",
      "Epoch 1390 iteration 100 loss 3.270583391189575\n",
      "Epoch 1390 iteration 200 loss 8.30734634399414\n",
      "Epoch 1390 Training loss 5.419508602478924\n",
      "Evaluation loss 8.46315504954133\n",
      "Epoch 1391 iteration 0 loss 0.7070754170417786\n",
      "Epoch 1391 iteration 100 loss 3.272257089614868\n",
      "Epoch 1391 iteration 200 loss 8.349807739257812\n",
      "Epoch 1391 Training loss 5.444932495361726\n",
      "Epoch 1392 iteration 0 loss 0.5999321341514587\n",
      "Epoch 1392 iteration 100 loss 3.171461582183838\n",
      "Epoch 1392 iteration 200 loss 8.258687019348145\n",
      "Epoch 1392 Training loss 5.425933720752872\n",
      "Epoch 1393 iteration 0 loss 0.6920563578605652\n",
      "Epoch 1393 iteration 100 loss 3.190639019012451\n",
      "Epoch 1393 iteration 200 loss 8.36772346496582\n",
      "Epoch 1393 Training loss 5.433900294410952\n",
      "Epoch 1394 iteration 0 loss 0.5891389846801758\n",
      "Epoch 1394 iteration 100 loss 3.381868839263916\n",
      "Epoch 1394 iteration 200 loss 8.297331809997559\n",
      "Epoch 1394 Training loss 5.44052733329468\n",
      "Epoch 1395 iteration 0 loss 0.6260396838188171\n",
      "Epoch 1395 iteration 100 loss 3.3190810680389404\n",
      "Epoch 1395 iteration 200 loss 8.366290092468262\n",
      "Epoch 1395 Training loss 5.455036836094326\n",
      "Evaluation loss 8.475691553926634\n",
      "Epoch 1396 iteration 0 loss 0.675291121006012\n",
      "Epoch 1396 iteration 100 loss 3.3361551761627197\n",
      "Epoch 1396 iteration 200 loss 8.182807922363281\n",
      "Epoch 1396 Training loss 5.44495530298895\n",
      "Epoch 1397 iteration 0 loss 0.5712944269180298\n",
      "Epoch 1397 iteration 100 loss 3.402733564376831\n",
      "Epoch 1397 iteration 200 loss 8.458739280700684\n",
      "Epoch 1397 Training loss 5.439199886929083\n",
      "Epoch 1398 iteration 0 loss 0.6759169101715088\n",
      "Epoch 1398 iteration 100 loss 3.3799970149993896\n",
      "Epoch 1398 iteration 200 loss 8.279916763305664\n",
      "Epoch 1398 Training loss 5.458462958480586\n",
      "Epoch 1399 iteration 0 loss 0.5795546174049377\n",
      "Epoch 1399 iteration 100 loss 3.406799077987671\n",
      "Epoch 1399 iteration 200 loss 8.271245002746582\n",
      "Epoch 1399 Training loss 5.46705275866572\n",
      "Epoch 1400 iteration 0 loss 0.6307088732719421\n",
      "Epoch 1400 iteration 100 loss 3.3128604888916016\n",
      "Epoch 1400 iteration 200 loss 8.394403457641602\n",
      "Epoch 1400 Training loss 5.454179119233719\n",
      "Evaluation loss 8.495577509982416\n",
      "Epoch 1401 iteration 0 loss 0.5731248259544373\n",
      "Epoch 1401 iteration 100 loss 3.309352397918701\n",
      "Epoch 1401 iteration 200 loss 8.35684585571289\n",
      "Epoch 1401 Training loss 5.45257910355288\n",
      "Epoch 1402 iteration 0 loss 0.612686038017273\n",
      "Epoch 1402 iteration 100 loss 3.3474855422973633\n",
      "Epoch 1402 iteration 200 loss 8.230940818786621\n",
      "Epoch 1402 Training loss 5.464293815933788\n",
      "Epoch 1403 iteration 0 loss 0.6530684232711792\n",
      "Epoch 1403 iteration 100 loss 3.22094464302063\n",
      "Epoch 1403 iteration 200 loss 8.30506420135498\n",
      "Epoch 1403 Training loss 5.481189482019771\n",
      "Epoch 1404 iteration 0 loss 0.6759659051895142\n",
      "Epoch 1404 iteration 100 loss 3.371342897415161\n",
      "Epoch 1404 iteration 200 loss 8.407415390014648\n",
      "Epoch 1404 Training loss 5.476903148299386\n",
      "Epoch 1405 iteration 0 loss 0.6380149126052856\n",
      "Epoch 1405 iteration 100 loss 3.325463056564331\n",
      "Epoch 1405 iteration 200 loss 8.241291046142578\n",
      "Epoch 1405 Training loss 5.474207138910551\n",
      "Evaluation loss 8.458785065126547\n",
      "Epoch 1406 iteration 0 loss 0.6343981027603149\n",
      "Epoch 1406 iteration 100 loss 3.323652505874634\n",
      "Epoch 1406 iteration 200 loss 8.374800682067871\n",
      "Epoch 1406 Training loss 5.487166549545939\n",
      "Epoch 1407 iteration 0 loss 0.6005470752716064\n",
      "Epoch 1407 iteration 100 loss 3.350529193878174\n",
      "Epoch 1407 iteration 200 loss 8.413836479187012\n",
      "Epoch 1407 Training loss 5.4789024616395485\n",
      "Epoch 1408 iteration 0 loss 0.6937728524208069\n",
      "Epoch 1408 iteration 100 loss 3.405339002609253\n",
      "Epoch 1408 iteration 200 loss 8.378324508666992\n",
      "Epoch 1408 Training loss 5.478319941854543\n",
      "Epoch 1409 iteration 0 loss 0.6391081809997559\n",
      "Epoch 1409 iteration 100 loss 3.359309434890747\n",
      "Epoch 1409 iteration 200 loss 8.37672233581543\n",
      "Epoch 1409 Training loss 5.469910428612319\n",
      "Epoch 1410 iteration 0 loss 0.7581071853637695\n",
      "Epoch 1410 iteration 100 loss 3.427165985107422\n",
      "Epoch 1410 iteration 200 loss 8.424897193908691\n",
      "Epoch 1410 Training loss 5.479393522203655\n",
      "Evaluation loss 8.506532990280412\n",
      "Epoch 1411 iteration 0 loss 0.5917166471481323\n",
      "Epoch 1411 iteration 100 loss 3.3476338386535645\n",
      "Epoch 1411 iteration 200 loss 8.357253074645996\n",
      "Epoch 1411 Training loss 5.499084647992668\n",
      "Epoch 1412 iteration 0 loss 0.7744288444519043\n",
      "Epoch 1412 iteration 100 loss 3.366793155670166\n",
      "Epoch 1412 iteration 200 loss 8.232378959655762\n",
      "Epoch 1412 Training loss 5.478064216713739\n",
      "Epoch 1413 iteration 0 loss 0.60912024974823\n",
      "Epoch 1413 iteration 100 loss 3.3207786083221436\n",
      "Epoch 1413 iteration 200 loss 8.225431442260742\n",
      "Epoch 1413 Training loss 5.477440123032512\n",
      "Epoch 1414 iteration 0 loss 0.6566702127456665\n",
      "Epoch 1414 iteration 100 loss 3.439312219619751\n",
      "Epoch 1414 iteration 200 loss 8.484193801879883\n",
      "Epoch 1414 Training loss 5.487838381433021\n",
      "Epoch 1415 iteration 0 loss 0.5857460498809814\n",
      "Epoch 1415 iteration 100 loss 3.276625394821167\n",
      "Epoch 1415 iteration 200 loss 8.432331085205078\n",
      "Epoch 1415 Training loss 5.5029377981051395\n",
      "Evaluation loss 8.463178371047645\n",
      "Epoch 1416 iteration 0 loss 0.6409512758255005\n",
      "Epoch 1416 iteration 100 loss 3.35457444190979\n",
      "Epoch 1416 iteration 200 loss 8.437783241271973\n",
      "Epoch 1416 Training loss 5.506997902811508\n",
      "Epoch 1417 iteration 0 loss 0.6025897264480591\n",
      "Epoch 1417 iteration 100 loss 3.6131653785705566\n",
      "Epoch 1417 iteration 200 loss 8.360408782958984\n",
      "Epoch 1417 Training loss 5.506745304303043\n",
      "Epoch 1418 iteration 0 loss 0.7741436958312988\n",
      "Epoch 1418 iteration 100 loss 3.3902995586395264\n",
      "Epoch 1418 iteration 200 loss 8.444832801818848\n",
      "Epoch 1418 Training loss 5.505182321279909\n",
      "Epoch 1419 iteration 0 loss 0.6605211496353149\n",
      "Epoch 1419 iteration 100 loss 3.44728946685791\n",
      "Epoch 1419 iteration 200 loss 8.225199699401855\n",
      "Epoch 1419 Training loss 5.498800040859279\n",
      "Epoch 1420 iteration 0 loss 0.6934748291969299\n",
      "Epoch 1420 iteration 100 loss 3.2825827598571777\n",
      "Epoch 1420 iteration 200 loss 8.333685874938965\n",
      "Epoch 1420 Training loss 5.49623063857842\n",
      "Evaluation loss 8.412910497738078\n",
      "Epoch 1421 iteration 0 loss 0.5517393350601196\n",
      "Epoch 1421 iteration 100 loss 3.246229887008667\n",
      "Epoch 1421 iteration 200 loss 8.383670806884766\n",
      "Epoch 1421 Training loss 5.495226240202066\n",
      "Epoch 1422 iteration 0 loss 0.6504181623458862\n",
      "Epoch 1422 iteration 100 loss 3.341383457183838\n",
      "Epoch 1422 iteration 200 loss 8.193742752075195\n",
      "Epoch 1422 Training loss 5.502641965028468\n",
      "Epoch 1423 iteration 0 loss 0.6034820079803467\n",
      "Epoch 1423 iteration 100 loss 3.447108030319214\n",
      "Epoch 1423 iteration 200 loss 8.165031433105469\n",
      "Epoch 1423 Training loss 5.506293246428343\n",
      "Epoch 1424 iteration 0 loss 0.6535518765449524\n",
      "Epoch 1424 iteration 100 loss 3.3867785930633545\n",
      "Epoch 1424 iteration 200 loss 8.257320404052734\n",
      "Epoch 1424 Training loss 5.505370492399439\n",
      "Epoch 1425 iteration 0 loss 0.5813277363777161\n",
      "Epoch 1425 iteration 100 loss 3.3529183864593506\n",
      "Epoch 1425 iteration 200 loss 8.504782676696777\n",
      "Epoch 1425 Training loss 5.516874854411327\n",
      "Evaluation loss 8.457350143879367\n",
      "Epoch 1426 iteration 0 loss 0.7809109687805176\n",
      "Epoch 1426 iteration 100 loss 3.5142574310302734\n",
      "Epoch 1426 iteration 200 loss 8.398734092712402\n",
      "Epoch 1426 Training loss 5.531937583822214\n",
      "Epoch 1427 iteration 0 loss 0.629723846912384\n",
      "Epoch 1427 iteration 100 loss 3.426692247390747\n",
      "Epoch 1427 iteration 200 loss 8.386312484741211\n",
      "Epoch 1427 Training loss 5.526321028121727\n",
      "Epoch 1428 iteration 0 loss 0.6406043171882629\n",
      "Epoch 1428 iteration 100 loss 3.3124215602874756\n",
      "Epoch 1428 iteration 200 loss 8.444334983825684\n",
      "Epoch 1428 Training loss 5.526120740165124\n",
      "Epoch 1429 iteration 0 loss 0.4948190152645111\n",
      "Epoch 1429 iteration 100 loss 3.538437843322754\n",
      "Epoch 1429 iteration 200 loss 8.310043334960938\n",
      "Epoch 1429 Training loss 5.532028956065573\n",
      "Epoch 1430 iteration 0 loss 0.6634563207626343\n",
      "Epoch 1430 iteration 100 loss 3.439939260482788\n",
      "Epoch 1430 iteration 200 loss 8.54743766784668\n",
      "Epoch 1430 Training loss 5.528508856553631\n",
      "Evaluation loss 8.399796523395437\n",
      "Epoch 1431 iteration 0 loss 0.7137444615364075\n",
      "Epoch 1431 iteration 100 loss 3.3842813968658447\n",
      "Epoch 1431 iteration 200 loss 8.55984878540039\n",
      "Epoch 1431 Training loss 5.5604827975608755\n",
      "Epoch 1432 iteration 0 loss 0.6473470330238342\n",
      "Epoch 1432 iteration 100 loss 3.3951151371002197\n",
      "Epoch 1432 iteration 200 loss 8.45417594909668\n",
      "Epoch 1432 Training loss 5.556906754547548\n",
      "Epoch 1433 iteration 0 loss 0.5945926904678345\n",
      "Epoch 1433 iteration 100 loss 3.573909044265747\n",
      "Epoch 1433 iteration 200 loss 8.472052574157715\n",
      "Epoch 1433 Training loss 5.571848735726192\n",
      "Epoch 1434 iteration 0 loss 0.5937075614929199\n",
      "Epoch 1434 iteration 100 loss 3.480886220932007\n",
      "Epoch 1434 iteration 200 loss 8.385860443115234\n",
      "Epoch 1434 Training loss 5.581565865668634\n",
      "Epoch 1435 iteration 0 loss 0.6804274916648865\n",
      "Epoch 1435 iteration 100 loss 3.44868803024292\n",
      "Epoch 1435 iteration 200 loss 8.440825462341309\n",
      "Epoch 1435 Training loss 5.562293618750273\n",
      "Evaluation loss 8.451554539437321\n",
      "Epoch 1436 iteration 0 loss 0.6728652715682983\n",
      "Epoch 1436 iteration 100 loss 3.3850722312927246\n",
      "Epoch 1436 iteration 200 loss 8.365209579467773\n",
      "Epoch 1436 Training loss 5.549661722557072\n",
      "Epoch 1437 iteration 0 loss 0.7333287000656128\n",
      "Epoch 1437 iteration 100 loss 3.446056365966797\n",
      "Epoch 1437 iteration 200 loss 8.474197387695312\n",
      "Epoch 1437 Training loss 5.564797150040987\n",
      "Epoch 1438 iteration 0 loss 0.6977882981300354\n",
      "Epoch 1438 iteration 100 loss 3.4870762825012207\n",
      "Epoch 1438 iteration 200 loss 8.602200508117676\n",
      "Epoch 1438 Training loss 5.566379649045364\n",
      "Epoch 1439 iteration 0 loss 0.6839554905891418\n",
      "Epoch 1439 iteration 100 loss 3.411839246749878\n",
      "Epoch 1439 iteration 200 loss 8.471890449523926\n",
      "Epoch 1439 Training loss 5.570550052125838\n",
      "Epoch 1440 iteration 0 loss 0.6657965183258057\n",
      "Epoch 1440 iteration 100 loss 3.442349672317505\n",
      "Epoch 1440 iteration 200 loss 8.676568984985352\n",
      "Epoch 1440 Training loss 5.594593087685003\n",
      "Evaluation loss 8.41851203617632\n",
      "Epoch 1441 iteration 0 loss 0.6130071878433228\n",
      "Epoch 1441 iteration 100 loss 3.3535220623016357\n",
      "Epoch 1441 iteration 200 loss 8.645315170288086\n",
      "Epoch 1441 Training loss 5.588860969468029\n",
      "Epoch 1442 iteration 0 loss 0.5501577258110046\n",
      "Epoch 1442 iteration 100 loss 3.530182361602783\n",
      "Epoch 1442 iteration 200 loss 8.415130615234375\n",
      "Epoch 1442 Training loss 5.601726575240659\n",
      "Epoch 1443 iteration 0 loss 0.5885206460952759\n",
      "Epoch 1443 iteration 100 loss 3.5233867168426514\n",
      "Epoch 1443 iteration 200 loss 8.671412467956543\n",
      "Epoch 1443 Training loss 5.58660443908879\n",
      "Epoch 1444 iteration 0 loss 0.5902835130691528\n",
      "Epoch 1444 iteration 100 loss 3.406397581100464\n",
      "Epoch 1444 iteration 200 loss 8.66866683959961\n",
      "Epoch 1444 Training loss 5.573944376506066\n",
      "Epoch 1445 iteration 0 loss 0.6214612722396851\n",
      "Epoch 1445 iteration 100 loss 3.358316659927368\n",
      "Epoch 1445 iteration 200 loss 8.619658470153809\n",
      "Epoch 1445 Training loss 5.589784020395911\n",
      "Evaluation loss 8.4508869243357\n",
      "Epoch 1446 iteration 0 loss 0.629784107208252\n",
      "Epoch 1446 iteration 100 loss 3.2823421955108643\n",
      "Epoch 1446 iteration 200 loss 8.500704765319824\n",
      "Epoch 1446 Training loss 5.597671324526167\n",
      "Epoch 1447 iteration 0 loss 0.7967525720596313\n",
      "Epoch 1447 iteration 100 loss 3.4471218585968018\n",
      "Epoch 1447 iteration 200 loss 8.589532852172852\n",
      "Epoch 1447 Training loss 5.601736183413908\n",
      "Epoch 1448 iteration 0 loss 0.6782177686691284\n",
      "Epoch 1448 iteration 100 loss 3.479609727859497\n",
      "Epoch 1448 iteration 200 loss 8.520706176757812\n",
      "Epoch 1448 Training loss 5.621892633483599\n",
      "Epoch 1449 iteration 0 loss 0.6223608255386353\n",
      "Epoch 1449 iteration 100 loss 3.5413248538970947\n",
      "Epoch 1449 iteration 200 loss 8.552486419677734\n",
      "Epoch 1449 Training loss 5.612947257009963\n",
      "Epoch 1450 iteration 0 loss 0.5857627391815186\n",
      "Epoch 1450 iteration 100 loss 3.620190382003784\n",
      "Epoch 1450 iteration 200 loss 8.529784202575684\n",
      "Epoch 1450 Training loss 5.6020041872527955\n",
      "Evaluation loss 8.43375695740375\n",
      "Epoch 1451 iteration 0 loss 0.8172897100448608\n",
      "Epoch 1451 iteration 100 loss 3.5069408416748047\n",
      "Epoch 1451 iteration 200 loss 8.515528678894043\n",
      "Epoch 1451 Training loss 5.617891886076082\n",
      "Epoch 1452 iteration 0 loss 0.6474823951721191\n",
      "Epoch 1452 iteration 100 loss 3.638606071472168\n",
      "Epoch 1452 iteration 200 loss 8.587696075439453\n",
      "Epoch 1452 Training loss 5.634623009828453\n",
      "Epoch 1453 iteration 0 loss 0.615940272808075\n",
      "Epoch 1453 iteration 100 loss 3.586329936981201\n",
      "Epoch 1453 iteration 200 loss 8.42716121673584\n",
      "Epoch 1453 Training loss 5.649814782566324\n",
      "Epoch 1454 iteration 0 loss 0.6886879801750183\n",
      "Epoch 1454 iteration 100 loss 3.5196352005004883\n",
      "Epoch 1454 iteration 200 loss 8.799034118652344\n",
      "Epoch 1454 Training loss 5.656498591839859\n",
      "Epoch 1455 iteration 0 loss 0.6630278825759888\n",
      "Epoch 1455 iteration 100 loss 3.538424015045166\n",
      "Epoch 1455 iteration 200 loss 8.529988288879395\n",
      "Epoch 1455 Training loss 5.6486119796063665\n",
      "Evaluation loss 8.435808922050995\n",
      "Epoch 1456 iteration 0 loss 0.6221989393234253\n",
      "Epoch 1456 iteration 100 loss 3.69177508354187\n",
      "Epoch 1456 iteration 200 loss 8.463483810424805\n",
      "Epoch 1456 Training loss 5.642055243185199\n",
      "Epoch 1457 iteration 0 loss 0.6595485210418701\n",
      "Epoch 1457 iteration 100 loss 3.4811248779296875\n",
      "Epoch 1457 iteration 200 loss 8.447919845581055\n",
      "Epoch 1457 Training loss 5.657698562249926\n",
      "Epoch 1458 iteration 0 loss 0.6903776526451111\n",
      "Epoch 1458 iteration 100 loss 3.5594723224639893\n",
      "Epoch 1458 iteration 200 loss 8.464975357055664\n",
      "Epoch 1458 Training loss 5.659688592889083\n",
      "Epoch 1459 iteration 0 loss 0.6658896803855896\n",
      "Epoch 1459 iteration 100 loss 3.6360504627227783\n",
      "Epoch 1459 iteration 200 loss 8.560175895690918\n",
      "Epoch 1459 Training loss 5.64757053581681\n",
      "Epoch 1460 iteration 0 loss 0.6955126523971558\n",
      "Epoch 1460 iteration 100 loss 3.6897873878479004\n",
      "Epoch 1460 iteration 200 loss 8.633283615112305\n",
      "Epoch 1460 Training loss 5.662661525403058\n",
      "Evaluation loss 8.452778623550822\n",
      "Epoch 1461 iteration 0 loss 0.6147804856300354\n",
      "Epoch 1461 iteration 100 loss 3.6422622203826904\n",
      "Epoch 1461 iteration 200 loss 8.54016399383545\n",
      "Epoch 1461 Training loss 5.6508665465354\n",
      "Epoch 1462 iteration 0 loss 0.5615783929824829\n",
      "Epoch 1462 iteration 100 loss 3.6564974784851074\n",
      "Epoch 1462 iteration 200 loss 8.383551597595215\n",
      "Epoch 1462 Training loss 5.63122647630552\n",
      "Epoch 1463 iteration 0 loss 0.5082006454467773\n",
      "Epoch 1463 iteration 100 loss 3.6239051818847656\n",
      "Epoch 1463 iteration 200 loss 8.552680969238281\n",
      "Epoch 1463 Training loss 5.645687668075065\n",
      "Epoch 1464 iteration 0 loss 0.5642259120941162\n",
      "Epoch 1464 iteration 100 loss 3.746947765350342\n",
      "Epoch 1464 iteration 200 loss 8.51437759399414\n",
      "Epoch 1464 Training loss 5.6657646335044145\n",
      "Epoch 1465 iteration 0 loss 0.5798326730728149\n",
      "Epoch 1465 iteration 100 loss 3.7505877017974854\n",
      "Epoch 1465 iteration 200 loss 8.50863265991211\n",
      "Epoch 1465 Training loss 5.649152726524201\n",
      "Evaluation loss 8.454757124018176\n",
      "Epoch 1466 iteration 0 loss 0.6348986029624939\n",
      "Epoch 1466 iteration 100 loss 3.623440742492676\n",
      "Epoch 1466 iteration 200 loss 8.391194343566895\n",
      "Epoch 1466 Training loss 5.664688627016621\n",
      "Epoch 1467 iteration 0 loss 0.6348178386688232\n",
      "Epoch 1467 iteration 100 loss 3.607137680053711\n",
      "Epoch 1467 iteration 200 loss 8.562801361083984\n",
      "Epoch 1467 Training loss 5.65917492669141\n",
      "Epoch 1468 iteration 0 loss 0.6044144630432129\n",
      "Epoch 1468 iteration 100 loss 3.653594493865967\n",
      "Epoch 1468 iteration 200 loss 8.690650939941406\n",
      "Epoch 1468 Training loss 5.688694362663325\n",
      "Epoch 1469 iteration 0 loss 0.7054362297058105\n",
      "Epoch 1469 iteration 100 loss 3.493723154067993\n",
      "Epoch 1469 iteration 200 loss 8.499879837036133\n",
      "Epoch 1469 Training loss 5.678085241669386\n",
      "Epoch 1470 iteration 0 loss 0.7150542140007019\n",
      "Epoch 1470 iteration 100 loss 3.662916898727417\n",
      "Epoch 1470 iteration 200 loss 8.52672290802002\n",
      "Epoch 1470 Training loss 5.687803793097225\n",
      "Evaluation loss 8.469885575580118\n",
      "Epoch 1471 iteration 0 loss 0.7186681032180786\n",
      "Epoch 1471 iteration 100 loss 3.6739537715911865\n",
      "Epoch 1471 iteration 200 loss 8.552103996276855\n",
      "Epoch 1471 Training loss 5.690109322995746\n",
      "Epoch 1472 iteration 0 loss 0.6292480230331421\n",
      "Epoch 1472 iteration 100 loss 3.612640857696533\n",
      "Epoch 1472 iteration 200 loss 8.736958503723145\n",
      "Epoch 1472 Training loss 5.699660003208731\n",
      "Epoch 1473 iteration 0 loss 0.7358738780021667\n",
      "Epoch 1473 iteration 100 loss 3.668271541595459\n",
      "Epoch 1473 iteration 200 loss 8.544132232666016\n",
      "Epoch 1473 Training loss 5.702923799226195\n",
      "Epoch 1474 iteration 0 loss 0.707246720790863\n",
      "Epoch 1474 iteration 100 loss 3.6860759258270264\n",
      "Epoch 1474 iteration 200 loss 8.53244400024414\n",
      "Epoch 1474 Training loss 5.710513029840869\n",
      "Epoch 1475 iteration 0 loss 0.6157591938972473\n",
      "Epoch 1475 iteration 100 loss 3.655730724334717\n",
      "Epoch 1475 iteration 200 loss 8.563735008239746\n",
      "Epoch 1475 Training loss 5.7061809604425155\n",
      "Evaluation loss 8.47837978728428\n",
      "Epoch 1476 iteration 0 loss 0.7352163195610046\n",
      "Epoch 1476 iteration 100 loss 3.6639771461486816\n",
      "Epoch 1476 iteration 200 loss 8.468940734863281\n",
      "Epoch 1476 Training loss 5.711469184882235\n",
      "Epoch 1477 iteration 0 loss 0.6822026968002319\n",
      "Epoch 1477 iteration 100 loss 3.631300210952759\n",
      "Epoch 1477 iteration 200 loss 8.813546180725098\n",
      "Epoch 1477 Training loss 5.719408939794629\n",
      "Epoch 1478 iteration 0 loss 0.622380793094635\n",
      "Epoch 1478 iteration 100 loss 3.6142683029174805\n",
      "Epoch 1478 iteration 200 loss 8.519183158874512\n",
      "Epoch 1478 Training loss 5.71202005032867\n",
      "Epoch 1479 iteration 0 loss 0.6533883213996887\n",
      "Epoch 1479 iteration 100 loss 3.681363582611084\n",
      "Epoch 1479 iteration 200 loss 8.621745109558105\n",
      "Epoch 1479 Training loss 5.72514362870721\n",
      "Epoch 1480 iteration 0 loss 0.7336679697036743\n",
      "Epoch 1480 iteration 100 loss 3.726877450942993\n",
      "Epoch 1480 iteration 200 loss 8.568882942199707\n",
      "Epoch 1480 Training loss 5.71668675949449\n",
      "Evaluation loss 8.47911120669861\n",
      "Epoch 1481 iteration 0 loss 0.6455335021018982\n",
      "Epoch 1481 iteration 100 loss 3.6476826667785645\n",
      "Epoch 1481 iteration 200 loss 8.692095756530762\n",
      "Epoch 1481 Training loss 5.71713199465407\n",
      "Epoch 1482 iteration 0 loss 0.7147905230522156\n",
      "Epoch 1482 iteration 100 loss 3.610492467880249\n",
      "Epoch 1482 iteration 200 loss 8.697148323059082\n",
      "Epoch 1482 Training loss 5.74031333198895\n",
      "Epoch 1483 iteration 0 loss 0.5928129553794861\n",
      "Epoch 1483 iteration 100 loss 3.676732301712036\n",
      "Epoch 1483 iteration 200 loss 8.493483543395996\n",
      "Epoch 1483 Training loss 5.72492821974501\n",
      "Epoch 1484 iteration 0 loss 0.6069093942642212\n",
      "Epoch 1484 iteration 100 loss 3.7279415130615234\n",
      "Epoch 1484 iteration 200 loss 8.520890235900879\n",
      "Epoch 1484 Training loss 5.71977727776269\n",
      "Epoch 1485 iteration 0 loss 0.6905208826065063\n",
      "Epoch 1485 iteration 100 loss 3.6847448348999023\n",
      "Epoch 1485 iteration 200 loss 8.510860443115234\n",
      "Epoch 1485 Training loss 5.732692313718655\n",
      "Evaluation loss 8.48039678772566\n",
      "Epoch 1486 iteration 0 loss 0.6367611885070801\n",
      "Epoch 1486 iteration 100 loss 3.7551589012145996\n",
      "Epoch 1486 iteration 200 loss 8.529006004333496\n",
      "Epoch 1486 Training loss 5.728117942914394\n",
      "Epoch 1487 iteration 0 loss 0.6233394145965576\n",
      "Epoch 1487 iteration 100 loss 3.766101598739624\n",
      "Epoch 1487 iteration 200 loss 8.538019180297852\n",
      "Epoch 1487 Training loss 5.723621915760951\n",
      "Epoch 1488 iteration 0 loss 0.5612937211990356\n",
      "Epoch 1488 iteration 100 loss 3.806253671646118\n",
      "Epoch 1488 iteration 200 loss 8.612188339233398\n",
      "Epoch 1488 Training loss 5.74446328649671\n",
      "Epoch 1489 iteration 0 loss 0.6833833456039429\n",
      "Epoch 1489 iteration 100 loss 3.880398988723755\n",
      "Epoch 1489 iteration 200 loss 8.575769424438477\n",
      "Epoch 1489 Training loss 5.738988633074772\n",
      "Epoch 1490 iteration 0 loss 0.609254002571106\n",
      "Epoch 1490 iteration 100 loss 3.91160249710083\n",
      "Epoch 1490 iteration 200 loss 8.509325981140137\n",
      "Epoch 1490 Training loss 5.752244354563597\n",
      "Evaluation loss 8.497714254348582\n",
      "Epoch 1491 iteration 0 loss 0.6255456805229187\n",
      "Epoch 1491 iteration 100 loss 3.7218966484069824\n",
      "Epoch 1491 iteration 200 loss 8.33910083770752\n",
      "Epoch 1491 Training loss 5.744297588800658\n",
      "Epoch 1492 iteration 0 loss 0.6174149513244629\n",
      "Epoch 1492 iteration 100 loss 3.9340977668762207\n",
      "Epoch 1492 iteration 200 loss 8.662751197814941\n",
      "Epoch 1492 Training loss 5.7674266397118545\n",
      "Epoch 1493 iteration 0 loss 0.7127029299736023\n",
      "Epoch 1493 iteration 100 loss 4.090981960296631\n",
      "Epoch 1493 iteration 200 loss 8.600408554077148\n",
      "Epoch 1493 Training loss 5.754909537062097\n",
      "Epoch 1494 iteration 0 loss 0.6618393659591675\n",
      "Epoch 1494 iteration 100 loss 3.7714672088623047\n",
      "Epoch 1494 iteration 200 loss 8.698651313781738\n",
      "Epoch 1494 Training loss 5.768557667869583\n",
      "Epoch 1495 iteration 0 loss 0.6781444549560547\n",
      "Epoch 1495 iteration 100 loss 3.969637870788574\n",
      "Epoch 1495 iteration 200 loss 8.667688369750977\n",
      "Epoch 1495 Training loss 5.758363546125234\n",
      "Evaluation loss 8.516908117822807\n",
      "Epoch 1496 iteration 0 loss 0.722804844379425\n",
      "Epoch 1496 iteration 100 loss 3.872659921646118\n",
      "Epoch 1496 iteration 200 loss 8.569964408874512\n",
      "Epoch 1496 Training loss 5.784883688795769\n",
      "Epoch 1497 iteration 0 loss 0.7321088910102844\n",
      "Epoch 1497 iteration 100 loss 3.8775768280029297\n",
      "Epoch 1497 iteration 200 loss 8.80115032196045\n",
      "Epoch 1497 Training loss 5.778028349465707\n",
      "Epoch 1498 iteration 0 loss 0.6821615695953369\n",
      "Epoch 1498 iteration 100 loss 3.7230052947998047\n",
      "Epoch 1498 iteration 200 loss 8.597983360290527\n",
      "Epoch 1498 Training loss 5.762392071108726\n",
      "Epoch 1499 iteration 0 loss 0.6836921572685242\n",
      "Epoch 1499 iteration 100 loss 3.7794320583343506\n",
      "Epoch 1499 iteration 200 loss 8.632588386535645\n",
      "Epoch 1499 Training loss 5.7629197078544125\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, num_epochs=1500)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAHHCAYAAACRAnNyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABlnklEQVR4nO3deVhU1R8G8HfYhn3YN2VRXEBwC5cU933J3FdS0Mo0zS1LrZ+mWZFlZVlpWqGVZmliZpqiaS6576i5pIIb4sa+z5zfH8SVkQEBh7kDvJ/n4YF77mXme3BkXs4991yFEEKAiIiIyAiZyF0AERERUXEYVIiIiMhoMagQERGR0WJQISIiIqPFoEJERERGi0GFiIiIjBaDChERERktBhUiIiIyWgwqREREZLQYVIjKICIiAn5+fuX63rlz50KhUOi3ICNz9epVKBQKrFixQu5SiuXn54eIiAhZnrsy/HyIjA2DClUJCoWiVB+7du2Su1QCsGvXrhL/ndasWSN3iU9k9erVWLRokdxlaImIiICtra3cZRCVmZncBRDpw/fff6+1/d133yEmJqZIe2Bg4BM9z/Lly6HRaMr1vf/73/8wc+bMJ3r+qmbSpElo3rx5kfZWrVrJUI3+rF69GrGxsZgyZYpWu6+vLzIzM2Fubi5PYUSVEIMKVQnPPfec1vaBAwcQExNTpP1RGRkZsLa2LvXzPMkbjJmZGczM+F+usLZt22LQoEFyl2EwCoUClpaWcpdBVKnw1A9VGx06dEBwcDCOHj2Kdu3awdraGm+88QYA4Ndff0Xv3r3h5eUFpVIJf39/zJ8/H2q1WusxHp2jUjDnYOHChVi2bBn8/f2hVCrRvHlzHD58WOt7dc1RUSgUmDhxIjZs2IDg4GAolUoEBQXhjz/+KFL/rl270KxZM1haWsLf3x9fffVVqee97NmzB4MHD4aPjw+USiW8vb0xdepUZGZmFumfra0tbty4gX79+sHW1haurq6YPn16kZ9FUlISIiIioFKp4ODggPDwcCQlJT22lrIIDg5Gx44di7RrNBrUqFFDK+QsXLgQrVu3hrOzM6ysrBASEoJ169Y99jmK+xmuWLECCoUCV69eldpK8zrp0KEDfv/9d8TFxUmnsgpeM8XNUfnzzz/Rtm1b2NjYwMHBAX379sW5c+d01nnp0iVERETAwcEBKpUKo0ePRkZGxmP7WVpr165FSEgIrKys4OLigueeew43btzQOiYhIQGjR49GzZo1oVQq4enpib59+2r9rI4cOYLu3bvDxcUFVlZWqFWrFsaMGaO3Oqn64J93VK3cu3cPPXv2xLBhw/Dcc8/B3d0dQP6bkq2tLaZNmwZbW1v8+eefmDNnDlJSUvDhhx8+9nFXr16N1NRUvPTSS1AoFPjggw8wYMAAXL58+bGjMHv37sX69evx8ssvw87ODp999hkGDhyI+Ph4ODs7AwCOHz+OHj16wNPTE/PmzYNarcbbb78NV1fXUvV77dq1yMjIwPjx4+Hs7IxDhw5h8eLFuH79OtauXat1rFqtRvfu3dGyZUssXLgQ27dvx0cffQR/f3+MHz8eACCEQN++fbF3716MGzcOgYGBiI6ORnh4eKnqKZCamoq7d+8WaXd2doZCocDQoUMxd+5cJCQkwMPDQ+tndvPmTQwbNkxq+/TTT/Hss88iLCwMOTk5WLNmDQYPHoxNmzahd+/eZaqrOKV5nbz55ptITk7G9evX8cknnwBAiXNDtm/fjp49e6J27dqYO3cuMjMzsXjxYoSGhuLYsWNFJm8PGTIEtWrVQmRkJI4dO4avv/4abm5uWLBggV76N3r0aDRv3hyRkZG4ffs2Pv30U+zbtw/Hjx+Hg4MDAGDgwIE4c+YMXnnlFfj5+SExMRExMTGIj4+Xtrt16wZXV1fMnDkTDg4OuHr1KtavX//ENVI1JIiqoAkTJohHX97t27cXAMTSpUuLHJ+RkVGk7aWXXhLW1tYiKytLagsPDxe+vr7S9pUrVwQA4ezsLO7fvy+1//rrrwKA+O2336S2t956q0hNAISFhYW4dOmS1Hby5EkBQCxevFhq69Onj7C2thY3btyQ2i5evCjMzMyKPKYuuvoXGRkpFAqFiIuL0+ofAPH2229rHdu0aVMREhIibW/YsEEAEB988IHUlpeXJ9q2bSsAiKioqBLr2blzpwBQ7MetW7eEEEKcP3++yM9CCCFefvllYWtrq9WvR/uYk5MjgoODRadOnbTafX19RXh4uLSt699FCCGioqIEAHHlypVin0MI3a+T3r17a71OChS8Xgr/fJo0aSLc3NzEvXv3pLaTJ08KExMTMWrUqCJ1jhkzRusx+/fvL5ydnYs816PCw8OFjY1NsftzcnKEm5ubCA4OFpmZmVL7pk2bBAAxZ84cIYQQDx48EADEhx9+WOxjRUdHCwDi8OHDj62L6HF46oeqFaVSidGjRxdpt7Kykr4u+Cu/bdu2yMjIwD///PPYxx06dCgcHR2l7bZt2wIALl++/Njv7dKlC/z9/aXtRo0awd7eXvpetVqN7du3o1+/fvDy8pKOq1OnDnr27PnYxwe0+5eeno67d++idevWEELg+PHjRY4fN26c1nbbtm21+rJ582aYmZlJIywAYGpqildeeaVU9RSYM2cOYmJiinw4OTkBAOrVq4cmTZrgp59+kr5HrVZj3bp16NOnj1a/Cn/94MEDJCcno23btjh27FiZairJk75OHnXr1i2cOHECERERUp+B/NdA165dsXnz5iLfo+vf5t69e0hJSSnz8xd25MgRJCYm4uWXX9aaR9O7d28EBATg999/B5D/M7CwsMCuXbvw4MEDnY9VMPKyadMm5ObmPlFdRAwqVK3UqFEDFhYWRdrPnDmD/v37Q6VSwd7eHq6urtJE3OTk5Mc+ro+Pj9Z2QWgp7hd5Sd9b8P0F35uYmIjMzEzUqVOnyHG62nSJj4+X3gwL5p20b98eQNH+WVpaFjmlVLgeAIiLi4Onp2eRUxr169cvVT0FGjZsiC5duhT5KPxvNHToUOzbt0+aJ7Fr1y4kJiZi6NChWo+1adMmPP3007C0tISTkxNcXV2xZMmSUv37ldaTvk4eFRcXB0D3zy0wMBB3795Fenq6VvuTvNbKW0tAQIC0X6lUYsGCBdiyZQvc3d3Rrl07fPDBB0hISJCOb9++PQYOHIh58+bBxcUFffv2RVRUFLKzs5+oRqqeGFSoWin8F3GBpKQktG/fHidPnsTbb7+N3377DTExMdI5/9JcjmxqaqqzXQhRod9bGmq1Gl27dsXvv/+OGTNmYMOGDYiJiZEmdD7av+LqkcvQoUMhhJDm0vz8889QqVTo0aOHdMyePXvw7LPPwtLSEl9++SU2b96MmJgYjBgx4rE/x+ImI+uaPPykrxN9qOjXS2lMmTIFFy5cQGRkJCwtLTF79mwEBgZKo3MKhQLr1q3D/v37MXHiRNy4cQNjxoxBSEgI0tLSDFYnVQ2cTEvV3q5du3Dv3j2sX78e7dq1k9qvXLkiY1UPubm5wdLSEpcuXSqyT1fbo06fPo0LFy5g5cqVGDVqlNQeExNT7pp8fX2xY8cOpKWlaY2qnD9/vtyPWZxatWqhRYsW+OmnnzBx4kSsX78e/fr1g1KplI755ZdfYGlpia1bt2q1R0VFPfbxC0YkkpKSpFMWwMMRhgJleZ2UdgViX19fALp/bv/88w9cXFxgY2NTqsd6UoVr6dSpk9a+8+fPS/sL+Pv749VXX8Wrr76KixcvokmTJvjoo4/www8/SMc8/fTTePrpp/Huu+9i9erVCAsLw5o1a/DCCy9UfIeoyuCIClV7BX+hFv6LNCcnB19++aVcJWkxNTVFly5dsGHDBty8eVNqv3TpErZs2VKq7we0+yeEwKefflrumnr16oW8vDwsWbJEalOr1Vi8eHG5H7MkQ4cOxYEDB/Dtt9/i7t27RU77mJqaQqFQaI2CXL16FRs2bHjsYxfMD9q9e7fUlp6ejpUrVxZ5DqB0rxMbG5tSnQry9PREkyZNsHLlSq1Lu2NjY7Ft2zb06tXrsY+hL82aNYObmxuWLl2qdYpmy5YtOHfunHTlVEZGBrKysrS+19/fH3Z2dtL3PXjwoMgIT5MmTQCAp3+ozDiiQtVe69at4ejoiPDwcEyaNAkKhQLff/+9QYfSH2fu3LnYtm0bQkNDMX78eKjVanz++ecIDg7GiRMnSvzegIAA+Pv7Y/r06bhx4wbs7e3xyy+/PNGchj59+iA0NBQzZ87E1atX0aBBA6xfv77M8zT27NlT5E0PyJ9M2qhRI2l7yJAhmD59OqZPnw4nJyd06dJF6/jevXvj448/Ro8ePTBixAgkJibiiy++QJ06dXDq1KkSa+jWrRt8fHzw/PPP47XXXoOpqSm+/fZbuLq6Ij4+XjquLK+TkJAQ/PTTT5g2bRqaN28OW1tb9OnTR+fzf/jhh+jZsydatWqF559/Xro8WaVSYe7cuSXWXla5ubl45513irQ7OTnh5ZdfxoIFCzB69Gi0b98ew4cPly5P9vPzw9SpUwEAFy5cQOfOnTFkyBA0aNAAZmZmiI6Oxu3bt6XLxVeuXIkvv/wS/fv3h7+/P1JTU7F8+XLY29sbNHxRFSHLtUZEFay4y5ODgoJ0Hr9v3z7x9NNPCysrK+Hl5SVef/11sXXrVgFA7Ny5UzquuMuTdV2qCUC89dZb0nZxlydPmDChyPc+egmtEELs2LFDNG3aVFhYWAh/f3/x9ddfi1dffVVYWloW81N46OzZs6JLly7C1tZWuLi4iBdffFG6DLrwpbLFXcKqq/Z79+6JkSNHCnt7e6FSqcTIkSPF8ePH9XJ5cuGfW4HQ0FABQLzwwgs6H/Obb74RdevWFUqlUgQEBIioqCiddev62R49elS0bNlSWFhYCB8fH/Hxxx/rvDy5tK+TtLQ0MWLECOHg4CAASK8ZXZcnCyHE9u3bRWhoqLCyshL29vaiT58+4uzZs1rHFPTlzp07Wu266tSl4NJzXR/+/v7ScT/99JNo2rSpUCqVwsnJSYSFhYnr169L++/evSsmTJggAgIChI2NjVCpVKJly5bi559/lo45duyYGD58uPDx8RFKpVK4ubmJZ555Rhw5cqTEGol0UQhhRH82ElGZ9OvXD2fOnMHFixflLoWIqEJwjgpRJfHocvcXL17E5s2b0aFDB3kKIiIyAI6oEFUSnp6eiIiIQO3atREXF4clS5YgOzsbx48fR926deUuj4ioQnAyLVEl0aNHD/z4449ISEiAUqlEq1at8N577zGkEFGVxhEVIiIiMlqco0JERERGi0GFiIiIjFalnqOi0Whw8+ZN2NnZlXrJaiIiIpKXEAKpqanw8vKCiUnJYyaVOqjcvHkT3t7ecpdBRERE5XDt2jXUrFmzxGMqdVCxs7MDkN9Re3t7mashIiKi0khJSYG3t7f0Pl6SSh1UCk732NvbM6gQERFVMqWZtsHJtERERGS0GFSIiIjIaDGoEBERkdGq1HNUiIiobNRqNXJzc+Uug6o4c3NzmJqa6uWxGFSIiKoBIQQSEhKQlJQkdylUTTg4OMDDw+OJ1zljUCEiqgYKQoqbmxusra25SCZVGCEEMjIykJiYCCD/zu9PgkGFiKiKU6vVUkhxdnaWuxyqBqysrAAAiYmJcHNze6LTQJxMS0RUxRXMSbG2tpa5EqpOCl5vTzonikGFiKia4OkeMiR9vd4YVIiIiMhoMagQEVG14ufnh0WLFpX6+F27dkGhUPCKKZkwqBARkVFSKBQlfsydO7dcj3v48GGMHTu21Me3bt0at27dgkqlKtfzlRYDkW686ofko84F4vcDvm0AE2ZmItJ269Yt6euffvoJc+bMwfnz56U2W1tb6WshBNRqNczMHv+25urqWqY6LCws4OHhUabvIf3huwMZxo2jwLoxwN2LD9su/AGs7AOs6CVfXURktDw8PKQPlUoFhUIhbf/zzz+ws7PDli1bEBISAqVSib179+Lff/9F37594e7uDltbWzRv3hzbt2/XetxHT/0oFAp8/fXX6N+/P6ytrVG3bl1s3LhR2v/oSMeKFSvg4OCArVu3IjAwELa2tujRo4dWsMrLy8OkSZPg4OAAZ2dnzJgxA+Hh4ejXr1+5fx4PHjzAqFGj4OjoCGtra/Ts2RMXLz78nRoXF4c+ffrA0dERNjY2CAoKwubNm6XvDQsLg6urK6ysrFC3bl1ERUWVuxZDYlChipeWCCzvBMT+Avw4LL8t5RYQPT7/a++W8tVGVE0JIZCRk2fwDyGEXvsxc+ZMvP/++zh37hwaNWqEtLQ09OrVCzt27MDx48fRo0cP9OnTB/Hx8SU+zrx58zBkyBCcOnUKvXr1QlhYGO7fv1/s8RkZGVi4cCG+//577N69G/Hx8Zg+fbq0f8GCBVi1ahWioqKwb98+pKSkYMOGDU/U14iICBw5cgQbN27E/v37IYRAr169pMt/J0yYgOzsbOzevRunT5/GggULpFGn2bNn4+zZs9iyZQvOnTuHJUuWwMXF5YnqMRSe+qGKd/Crh1/fuwRsmQmcWgPkpAJQAE+Nkq00ouoqM1eNBnO2Gvx5z77dHdYW+nvrefvtt9G1a1dp28nJCY0bN5a258+fj+joaGzcuBETJ04s9nEiIiIwfPhwAMB7772Hzz77DIcOHUKPHj10Hp+bm4ulS5fC398fADBx4kS8/fbb0v7Fixdj1qxZ6N+/PwDg888/l0Y3yuPixYvYuHEj9u3bh9atWwMAVq1aBW9vb2zYsAGDBw9GfHw8Bg4ciIYNGwIAateuLX1/fHw8mjZtimbNmgHIH1WqLDiiQhVLCGDPQu22g0uAzAeAqQUw9AfA2V+e2oio0it44y2QlpaG6dOnIzAwEA4ODrC1tcW5c+ceO6LSqFEj6WsbGxvY29tLS8DrYm1tLYUUIH+Z+ILjk5OTcfv2bbRo0ULab2pqipCQkDL1rbBz587BzMwMLVs+HIF2dnZG/fr1ce7cOQDApEmT8M477yA0NBRvvfUWTp06JR07fvx4rFmzBk2aNMHrr7+Ov//+u9y1GBpHVKhibXyl+H19PgMCnzFcLUQksTI3xdm3u8vyvPpkY2OjtT19+nTExMRg4cKFqFOnDqysrDBo0CDk5OSU+Djm5uZa2wqFAhqNpkzH6/u0Vlm98MIL6N69O37//Xds27YNkZGR+Oijj/DKK6+gZ8+eiIuLw+bNmxETE4POnTtjwoQJWLhw4eMfWGYcUaGKs/cT4Pj3uvc9swhoMtyg5RDRQwqFAtYWZgb/qOjVcfft24eIiAj0798fDRs2hIeHB65evVqhz/kolUoFd3d3HD58WGpTq9U4duxYuR8zMDAQeXl5OHjwoNR27949nD9/Hg0aNJDavL29MW7cOKxfvx6vvvoqli9fLu1zdXVFeHg4fvjhByxatAjLli0rdz2GxBEVqhjxB4Dtcx9ut58JNBoC/DAQ6DALaDxUttKIqOqqW7cu1q9fjz59+kChUGD27NkljoxUlFdeeQWRkZGoU6cOAgICsHjxYjx48KBUQe306dOws7OTthUKBRo3boy+ffvixRdfxFdffQU7OzvMnDkTNWrUQN++fQEAU6ZMQc+ePVGvXj08ePAAO3fuRGBgIABgzpw5CAkJQVBQELKzs7Fp0yZpn7FjUCH9y8sGvn1kSLn964CJKTD5hCwlEVH18PHHH2PMmDFo3bo1XFxcMGPGDKSkpBi8jhkzZiAhIQGjRo2Cqakpxo4di+7du5fqLsLt2rXT2jY1NUVeXh6ioqIwefJkPPPMM8jJyUG7du2wefNm6TSUWq3GhAkTcP36ddjb26NHjx745JNPAOSvBTNr1ixcvXoVVlZWaNu2LdasWaP/jlcAhZD7pNoTSElJgUqlQnJyMuzt7eUuhwpc2AasHvxw+5lFQLPRspVDVN1lZWXhypUrqFWrFiwtLeUup1rSaDQIDAzEkCFDMH/+fLnLMYiSXndlef/miArpV16OdkgBGFKIqNqJi4vDtm3b0L59e2RnZ+Pzzz/HlStXMGLECLlLq3Q4mZb0J+Um8L6Pdlufz+SphYhIRiYmJlixYgWaN2+O0NBQnD59Gtu3b68080KMCUdUSD8SzwHb5wF5mQ/bXr8CWDvJVxMRkUy8vb2xb98+ucuoEhhU6MntfA/4a0HRdoYUIiJ6QgwqVH55OUDcPt0hpf0Mw9dDRERVDoMKlc+dC8AXzXXvG/Ez4N/JsPUQEVGVxKBCZafOKxpS+i/jIm5ERKR3DCpUencvAX+9D+RlFd1Xr5vh6yEioiqPQYVKb80I4O75ou2OtQArR8PXQ0REVZ6s66io1WrMnj0btWrVgpWVFfz9/TF//nzZ70BJxXg0pJiYAa0mAi/tlqceIiI9uHr1KhQKBU6cOFHhz7VixQo4ODhU+PNUJbIGlQULFmDJkiX4/PPPce7cOSxYsAAffPABFi9eLGdZVFhmEvDjCOCLp7XbbVyBOfeA7u8Clrx9ARFVjIiICCgUiiIfPXr0kLu0x/Lz88OiRYu02oYOHYoLFy5U+HN36NABU6ZMqfDnMQRZT/38/fff6Nu3L3r37g0g/x/1xx9/xKFDh+QsiwqciQbWRujeV6+77nYiIj3r0aMHoqKitNqUSqVM1TwZKysrWFlZyV1GpSLriErr1q2xY8cOKV2ePHkSe/fuRc+ePXUen52djZSUFK0PqiAfB+kOKXW6Au1eA7q/Z/CSiKh6UiqV8PDw0PpwdMyfFzdixAgMHap9xWFubi5cXFzw3XffAQD++OMPtGnTBg4ODnB2dsYzzzyDf//9t9jn03V6ZsOGDVAoFNL2v//+i759+8Ld3R22trZo3rw5tm/fLu3v0KED4uLiMHXqVGkUqLjHXrJkCfz9/WFhYYH69evj+++/19qvUCjw9ddfo3///rC2tkbdunWxcePG0v3wivHLL78gKCgISqUSfn5++Oijj7T2f/nll6hbty4sLS3h7u6OQYMGSfvWrVuHhg0bwsrKCs7OzujSpQvS09OfqJ6SyBpUZs6ciWHDhiEgIADm5uZo2rQppkyZgrCwMJ3HR0ZGQqVSSR/e3t4Grria+HcnkHK9aHvj4cBz64BO/wMsVYavi4j0RwggJ93wH3qegxgWFobffvsNaWlpUtvWrVuRkZGB/v37AwDS09Mxbdo0HDlyBDt27ICJiQn69+8PjUZT7udNS0tDr169sGPHDhw/fhw9evRAnz59EB8fDwBYv349atasibfffhu3bt3CrVu3dD5OdHQ0Jk+ejFdffRWxsbF46aWXMHr0aOzcuVPruHnz5mHIkCE4deoUevXqhbCwMNy/f79ctR89ehRDhgzBsGHDcPr0acydOxezZ8/GihUrAABHjhzBpEmT8Pbbb+P8+fP4448/0K5dOwDArVu3MHz4cIwZMwbnzp3Drl27MGDAgAqdWyrrqZ+ff/4Zq1atwurVqxEUFIQTJ05gypQp8PLyQnh4eJHjZ82ahWnTpknbKSkpDCv6lJcDHFsJbJ6u3d7/K8CvDWDjJk9dRKR/uRnAe16Gf943bgIWNmX6lk2bNsHW1lb7Yd54A2+88Qa6d+8OGxsbREdHY+TIkQCA1atX49lnn4WdnR0AYODAgVrf++2338LV1RVnz55FcHBwubrRuHFjNG7cWNqeP38+oqOjsXHjRkycOBFOTk4wNTWFnZ0dPDw8in2chQsXIiIiAi+//DIAYNq0aThw4AAWLlyIjh07SsdFRERg+PDhAID33nsPn332GQ4dOlSuuToff/wxOnfujNmzZwMA6tWrh7Nnz+LDDz9EREQE4uPjYWNjg2eeeQZ2dnbw9fVF06ZNAeQHlby8PAwYMAC+vr4AgIYNG5a5hrKQdUTltddek0ZVGjZsiJEjR2Lq1KmIjIzUebxSqYS9vb3WB+nR358WDSmvXwEaDwNUNQEzC3nqIqJqrWPHjjhx4oTWx7hx4wAAZmZmGDJkCFatWgUgf/Tk119/1RqZv3jxIoYPH47atWvD3t4efn5+ACCNfpRHWloapk+fjsDAQDg4OMDW1hbnzp0r82OeO3cOoaGhWm2hoaE4d+6cVlujRo2kr21sbGBvb4/ExMRy1V7cc168eBFqtRpdu3aFr68vateujZEjR2LVqlXIyMgAkB/QOnfujIYNG2Lw4MFYvnw5Hjx4UK46SkvWEZWMjAyYmGhnJVNT0ycajqNySLkJJMQCf76j3R62jjcWJKqqzK3zRzfkeN4ysrGxQZ06dYrdHxYWhvbt2yMxMRExMTGwsrLSGmno06cPfH19sXz5cnh5eUGj0SA4OBg5OTk6H8/ExKTIqYzc3Fyt7enTpyMmJgYLFy5EnTp1YGVlhUGDBhX7mE/K3Nxca1uhUFTYe6WdnR2OHTuGXbt2Ydu2bZgzZw7mzp2Lw4cPw8HBATExMfj777+xbds2LF68GG+++SYOHjyIWrVqVUg9sgaVPn364N1334WPjw+CgoJw/PhxfPzxxxgzZoycZVUv6feAjwOLtkdsBvxCi7YTUdWgUJT5FIyxat26Nby9vfHTTz9hy5YtGDx4sPTGfu/ePZw/fx7Lly9H27ZtAQB79+4t8fFcXV2RmpqK9PR02Njk/4weXWNl3759iIiIkObBpKWl4erVq1rHWFhYQK1Wl/hcgYGB2Ldvn9Z0h3379qFBgwaP7Xd5FTxnYfv27UO9evVgamoKIH+kqkuXLujSpQveeustODg44M8//8SAAQOgUCgQGhqK0NBQzJkzB76+voiOjtaamqFPsgaVxYsXY/bs2Xj55ZeRmJgILy8vvPTSS5gzZ46cZVVt6lzg2kHgSBQQu073MSofwLulYesiIipGdnY2EhIStNrMzMzg4uIibY8YMQJLly7FhQsXtCaiOjo6wtnZGcuWLYOnpyfi4+Mxc+bMEp+vZcuWsLa2xhtvvIFJkybh4MGD0kTTAnXr1sX69evRp08fKBQKzJ49u8gIh5+fH3bv3o1hw4ZBqVRq1Vvgtddew5AhQ9C0aVN06dIFv/32G9avX691BVF53blzp0jA8vT0xKuvvormzZtj/vz5GDp0KPbv34/PP/8cX375JYD8OUGXL19Gu3bt4OjoiM2bN0Oj0aB+/fo4ePAgduzYgW7dusHNzQ0HDx7EnTt3EBio4w9efRGVWHJysgAgkpOT5S6l8vhtqhBv2Rf/cfQ7IbLT5a6SiPQoMzNTnD17VmRmZspdSpmFh4cLAEU+6tevr3Xc2bNnBQDh6+srNBqN1r6YmBgRGBgolEqlaNSokdi1a5cAIKKjo4UQQly5ckUAEMePH5e+Jzo6WtSpU0dYWVmJZ555RixbtkwUfsu8cuWK6Nixo7CyshLe3t7i888/F+3btxeTJ0+Wjtm/f79o1KiRUCqV0vdGRUUJlUqlVd+XX34pateuLczNzUW9evXEd999p7W/cK0FVCqViIqKKvbn1r59e50/t/nz5wshhFi3bp1o0KCBMDc3Fz4+PuLDDz+UvnfPnj2iffv2wtHRUVhZWYlGjRqJn376Sfo5d+/eXbi6ugqlUinq1asnFi9erLOGkl53ZXn/Vvz3Q6iUUlJSoFKpkJyczIm1paHRAG+XcE+ezm8BbStm6I6I5JOVlYUrV66gVq1asLS0lLscqiZKet2V5f2bNyWsDk6vA355vuRjnt8OeDc3TD1ERESlxKBS1R2JAjZN0b2v+QtAg75ArXYGLYmIiKi0GFSqsqRrukPK2L8AryaGroaIiKjMGFSqKnUusOiRFRf7fAqERMhSDhERUXkwqFRVv07U3p56FlDVkKcWIjIKlfjaCaqE9PV6k3UJfdIzjRrY/wVw+wxwas3D9olHGFKIqrGCxc8KlkEnMoSC19ujq+qWFUdUqpLlnYBbJ7Tb6nYDXOrKUg4RGQdTU1M4ODhI94axtraGQqGQuSqqqoQQyMjIQGJiIhwcHKTVbsuLQaWyEwK4tB1wa1A0pADAgOUGL4mIjE/BHXzLeyM7orJycHAo8c7RpcWgUtnF/lL8GintXgesHAxaDhEZJ4VCAU9PT7i5uRW5wR6Rvpmbmz/xSEoBBpXKLOUmcPjrou0jfgbqdAVMOAWJiLSZmprq7Q2EyBAYVCqj2F/yF3K7uqfovhrNgHrdDV8TERFRBWBQqQw0aiAvG7Cwzv+8bkzRY+r3ArJTgW7zDV8fERFRBWFQMWa3TgEPrgCHvwGu/AU0fQ54EFf0OO+WwPAfDV8fERFRBWNQkVvmA8BUmT9aUuDfP4HVQwF1jvaxx3/Q3nbwAYIHAa0eWdyNiIioimBQqQhpd4BDXwHnNgHd3wXqdM5vv34EyEoGtrwO3LsEDP0hfwVZ92AgfCMQtw84+6vuCbKPGrURqN2+YvtBREQkM4WoxGsqp6SkQKVSITk5Gfb29nKXA+Rm5geNvxcDt2MftvdfBhz/Xvfk19Jy8gdqNgNO/ZS//cZNwMLmyeolIiKSQVnevzmiok/7PgN2vVe0PXps+R6vRkj+JcgdZubfTFAIQFUz/5QPQwoREVUDDCr6IkT+SIq+1O8NDF+t3aZQAJ3n6O85iIiIjByDir7smAfkpOZ/bekADFkJONYCdr4L3DwB2LgAzv5A6BRg2/+Adq/lj5aYWQIQwJkNQPAAIPk6kHEXaDNNtq4QEREZC85R0Ye4v4GonvlfW7sAr/8rXy1ERERGrizv31xj/UnlZDwMKQDw9Hj5aiEiIqpiGFSeRF42sGqwdhtP2RAREekNg8qT+H4AELc3/2uXesC0c7wRIBERkR7xXbW8cjMfhhQA6PkBYO8lXz1ERERVEINKeaUmPPy6VjugdgfZSiEiIqqqGFTKK+XGw69Hbshf44SIiIj0ikGlvE78txibX1vAxFTeWoiIiKooBpXyECL/nj4A0HqSvLUQERFVYQwq5ZF6C8hJAxSmnJtCRERUgRhUyiPxbP5np1qAmYW8tRAREVVhsgYVPz8/KBSKIh8TJkyQs6zHu34k/7PXU/LWQUREVMXJelPCw4cPQ61WS9uxsbHo2rUrBg8eXMJ3GYGr/62f4vO0vHUQERFVcbIGFVdXV63t999/H/7+/mjfvr1MFZVCXjZw/XD+135t5K2FiIioipM1qBSWk5ODH374AdOmTYOimDVJsrOzkZ2dLW2npKQYqryH7l4A8rIAS4f8ZfOJiIiowhjNZNoNGzYgKSkJERERxR4TGRkJlUolfXh7exuuwAL3L+d/dq7DRd6IiIgqmNEElW+++QY9e/aEl1fx98uZNWsWkpOTpY9r164ZsML/3L2Q/9nZ3/DPTUREVM0YxamfuLg4bN++HevXry/xOKVSCaVSaaCqipF4Lv+zW6C8dRAREVUDRjGiEhUVBTc3N/Tu3VvuUh7v9n9rqLgFyVsHERFRNSB7UNFoNIiKikJ4eDjMzIxigKd4eTnAvYv5X3NEhYiIqMLJHlS2b9+O+Ph4jBkzRu5SHu/eRUCTByjtAVVNuashIiKq8mQfwujWrRuEEHKXUToFE2ld6/OKHyIiIgOQfUSlUkm5lf+ZoylEREQGwaBSFrs/zP9s5ylvHURERNUEg0ppCQFk3s//2lIlby1ERETVBINKaWU+ePh1yGj56iAiIqpGGFRKK+Vm/mdrZ8DOXd5aiIiIqgkGldJKuZH/2b6GvHUQERFVIwwqpcWgQkREZHAMKqWV/F9QUTGoEBERGQqDSmlJIyrF392ZiIiI9ItBpbSkoMLF3oiIiAyFQaW0Claltedib0RERIbCoFJa6Yn5n215aTIREZGhMKiURl4OkJWc/7WNq7y1EBERVSMMKqWRcTf/s8IUsHSQtRQiIqLqhEGlNNLv5H+2cQVM+CMjIiIyFL7rlkbhoEJEREQGw6BSGmkFQcVF3jqIiIiqGQaV0si4l//Z2lneOoiIiKoZBpXSyE7J/2ypkrcOIiKiaoZBpTSyCoKKvbx1EBERVTMMKqWRnZr/WWknbx1ERETVDINKaWT/t9ibkiMqREREhsSgUhpZnKNCREQkBwaV0uCpHyIiIlkwqJRGwVU/PPVDRERkUAwqpcGrfoiIiGTBoFIaPPVDREQkCwaVx1HnAnmZ+V/z1A8REZFBMag8TsFpH4BBhYiIyMAYVB6nYCKtuTVgaiZvLURERNWM7EHlxo0beO655+Ds7AwrKys0bNgQR44ckbush3jFDxERkWxkHSJ48OABQkND0bFjR2zZsgWurq64ePEiHB0d5SxLG6/4ISIiko2sQWXBggXw9vZGVFSU1FarVi0ZK9JBGlHhFT9ERESGJuupn40bN6JZs2YYPHgw3Nzc0LRpUyxfvlzOkoqSLk3miAoREZGhyRpULl++jCVLlqBu3brYunUrxo8fj0mTJmHlypU6j8/OzkZKSorWR4XjqR8iIiLZyHrqR6PRoFmzZnjvvfcAAE2bNkVsbCyWLl2K8PDwIsdHRkZi3rx5hi1SunMyT/0QEREZmqwjKp6enmjQoIFWW2BgIOLj43UeP2vWLCQnJ0sf165dq/gipVM/vHMyERGRock6ohIaGorz589rtV24cAG+vr46j1cqlVAqlYYo7SGe+iEiIpKNrCMqU6dOxYEDB/Dee+/h0qVLWL16NZYtW4YJEybIWZa2ghEVC1t56yAiIqqGZA0qzZs3R3R0NH788UcEBwdj/vz5WLRoEcLCwuQsS1tuRv5nCxt56yAiIqqGZF8T/plnnsEzzzwjdxnFy0nP/8wRFSIiIoOTfQl9oycFFWt56yAiIqqGGFQep+DUjzmDChERVQ+5ag2W776M2BvJcpci/6kfoyeNqHCOChERVX27L9zBqG8PSdszegRgfAd/2ephUHkcjqgQEVEVd+VuOhZtv4BDV+7jVnKW1r7Ym/KOqjCoPA5HVIiIqIr6+9JdjPj6YLH7J3eui6Y+DoYrSAcGlZJoNLw8mYiIqpzzCakI+/og7qZl69z/x5S2CPAwjoVOGVRKkpf58Gue+iEiokoqPTsPV++lY9OpWzBVKPD5zkta+xt7O2BCB3809XGEi60FFAqFTJUWxaBSkpyMh18zqBARUSWUkJyF1u/vgEYU3TeipQ/e7RdsVMHkUQwqJcn9b36KuTVgwiu5iYioctl06iYmrj6uc1+jmiq817+hgSsqOwaVkuQUCipERERGLk+tAQD0//JvnH5kDZRnG3vBy8EK284k4PUeAegR7CFHiWXGoFKSglM/XJWWiIiM3IP0HPT4dDdupxSdINvE2wGfDmsChUKBmT0DZKiu/BhUSiKd+uEVP0REZLyEEOj35b4iIcXC1ATrX26NAA87o56HUhIGlZJwRIWIiIzc0bj7GLhkv1ZbwxoqfDa8KUwVCvg4V+73MAaVknANFSIiMiKJKVk4fPUBFv95ETeTMmFhZqq1FsrTtZ0wq2cg6rrbwtqiarzFV41eVJSctPzPPPVDREQyyVNrEH38Bl5bd0rXXumrJWFPoWOAGyzNTQ1XnAEwqJSEp36IiEgm/95JQ9S+K/jhQLzO/e72SrzQpjZMTRQY8FQNOFhbGLhCw2BQKUkuL08mIiLDS83KReeP/irSHlrHGcFeKtRysUGPYI8qG04KY1ApSQ7nqBARkeHN++2s1nZzP0e81ScIwTVUMlUkHwYVHa7cTce+S3fRNvE+fAHA3ErukoiIqIrTdfWOg7U5/p7ZqcpMjC0Prguvw6nrSfjfhlicv3Evv8G06g+tERGRPNKz8/D1nstFQgoAHJ/dtVqHFIAjKjqZ/Lcojqn4bza1qbmM1RARUVV0JzUb0cev473N/xTZ5+9qg9d7BFTaRdr0iUFFh4dBJTe/gSMqRET0hIQQeOf3c/hm75Vij1n63FPoEexpwKqMH4OKDib/BVgzaUSFQYWIiMpv25kEjP3+aLH7Px3WBP6uttVysuzjMKjooCgyosJTP0REVHZ5ag0iog5j76W7RfZN61oP6Tl56FTfDS1rO8tQXeXAoKIDR1SIiKi8NBqBzFw1vtx1CV/s/FdrXzNfR6wd1wopWXlQWfGP4NJgUNGh6GRaBhUiInq8szdTMHH1MVy+m67VPjrUD2/0CoS5af7FtgwppcegooPJfxdtm4GnfoiIqHTupWVj8NK/kZ6j1mp/pVMdvNqtvkxVVX4MKjoUzFHhqR8iIiqN2BvJeGbxXmk7ckBD2Fmawd/VFoGe9jJWVvkxqOhQcNU6L08mIqLiHIt/gNPXk/HN3iuIv58htb/cwR/DW/jIWFnVwqCiQ8EcFTNwwTciInpIoxEYv+ootp65rXP/l2FPoXOgm4GrqtoYVHQwefTUjwmDChFRdZaYmoVr9zN0LnNf4M1egejVkIu16ZusQWXu3LmYN2+eVlv9+vXxzz9FlxM2pILLk80LRlTMlPIVQ0REsknKyMHcjWew4cRNnfv/1zsQtV1tEOSlgru9pYGrqx5kH1EJCgrC9u3bpW0zM9lLKjSZllf9EBFVR+nZeej00S7cTsnWavdSWWJwM28MCqkJbydrmaqrXmRPBWZmZvDw8JC7DC3Sgm/gqR8iouomK1eNoLe2FmkPa+mDF9rWRi0XGxmqqr5kDyoXL16El5cXLC0t0apVK0RGRsLHR/ds6ezsbGRnP0y3KSkpFVKTiQkvTyYiqo4uJabhmcV7pO1OAW7wcbLGwKdqomFN3odHDrIGlZYtW2LFihWoX78+bt26hXnz5qFt27aIjY2FnZ1dkeMjIyOLzGmpCEXmqPDUDxFRlZaTp8HzKw9jz8WH9+R57mkfvNOvoYxVEQAohBBC7iIKJCUlwdfXFx9//DGef/75Ivt1jah4e3sjOTkZ9vb6W1DnWPwDDPjyb1y0HJUfVqaeBVQ19Pb4REQkLyEE/v73HjaduoUfD8UX2f9i21p4o1egNGeR9CslJQUqlapU79+yn/opzMHBAfXq1cOlS5d07lcqlVAqK/4KnPzLk0WhERWe+iEiqgoepOfgy12XsHzPFZ37n3vaB3OeCYKFmYmBK6PiGFVQSUtLw7///ouRI0fKWoeJAjBDoXs18NQPEVGldjz+AT7bcRE7z9/Rub9jfVe80LY2Quu4GLgyehxZg8r06dPRp08f+Pr64ubNm3jrrbdgamqK4cOHy1kWTBSKh6MpAEdUiIgqoaxcNY7GPcCaw9fw28mi66A8ekdjMk6yBpXr169j+PDhuHfvHlxdXdGmTRscOHAArq6ucpYFhQIMKkREldj5hFR0X7S7SLudpRnm9gnCwJCaMlRF5SFrUFmzZo2cT18sBRSwkE79KAATU1nrISKix7v+IP/GgN0+2Y2MHLXWPhdbC+yY1gEqa57Kr2yMao6KsTAxeeTSZM76JiIyWgVXaurydt8g1HO3Q3M/J5ia8Hd5ZcSgooOJQgFzBa/4ISIyZnH30vHa2lM4dPW+zv2H3+wCVzveq62yY1DRwUTBxd6IiIxVrlqDj7ZdwNK//tVq91RZwtLcFMtHNUMdN1uZqiN9K1dQuXbtGhQKBWrWzJ+MdOjQIaxevRoNGjTA2LFj9VqgHBQKBSy4hgoRkdFJysjBqG8P4dT1ZKmtma8jJnWui3b15L0QgypGuYLKiBEjMHbsWIwcORIJCQno2rUrgoKCsGrVKiQkJGDOnDn6rtOgTBSKh+uoMKgQEclu5/lEzN14BnH3MrTa3+gVgBfa1Jbu0UZVT7kuHo+NjUWLFi0AAD///DOCg4Px999/Y9WqVVixYoU+65MFT/0QERkHIQQW77iI0VGHtUKKv6sN9rzeEWPb+TOkVHHlGlHJzc2VlrLfvn07nn32WQBAQEAAbt26pb/qZGKiUMCCk2mJiGS1/997GL78QJH2n8Y+jZa1nWWoiORQrqASFBSEpUuXonfv3oiJicH8+fMBADdv3oSzc+V/8Sg4okJEJJvj8Q/wzd4r2HTq4R++w1t4I7y1H2q72PI+PNVMuYLKggUL0L9/f3z44YcIDw9H48aNAQAbN26UTglVZlpL6HNEhYiowgkhcOF2Gv66kIj3Nv+jtW/1iy3R2p/34KmuyhVUOnTogLt37yIlJQWOjo5S+9ixY2Ftba234uTCoEJEZBiJqVn4IzYBc349o3P/l2FPMaRUc+UKKpmZmRBCSCElLi4O0dHRCAwMRPfu3fVaoBxMFHh4ebIJl5ohItK3XLUGDzJyMGL5QVxKTNPa18zXERM61oGrnRLBNVQyVUjGolzvwn379sWAAQMwbtw4JCUloWXLljA3N8fdu3fx8ccfY/z48fqu07C05qhwRIWISJ9uJmWi9ft/arXZKc1Q280WT/k44KV2/vBQWcpUHRmbcs1IOnbsGNq2bQsAWLduHdzd3REXF4fvvvsOn332mV4LlIOJJhduiqT8DQYVIiK9ycpVY9S3h7Ta/JytcXped/w6IRRv9QliSCEt5RpRycjIgJ2dHQBg27ZtGDBgAExMTPD0008jLi5OrwXKwfLCJsw0z7+zszA1B6/QJyJ6MptP38Ky3Zdx4lqSVnv7eq54p1+wPEVRpVCuoFKnTh1s2LAB/fv3x9atWzF16lQAQGJiIuzt7fVaoBwUJg8HmoQJgwoRUXk9SM/BxB+PYd+le1rtHw1ujIEhNWWqiiqTcgWVOXPmYMSIEZg6dSo6deqEVq1aAcgfXWnatKleC5SFienDr814500iorK6/iAD3++Pw1e7LxfZNya0FkMKlVq5gsqgQYPQpk0b3Lp1S1pDBQA6d+6M/v376604uZgoCo2omDKoEBGVVq5ag8V/XsJnOy5qtQ9t5o3JXerCy8FKpsqosir3tbceHh7w8PDA9evXAQA1a9asEou9AYDC9OGIijDjpC4iotJITMnCO7+fw8aTN7Xaj/yvC1xs+UcflU+5rvrRaDR4++23oVKp4OvrC19fXzg4OGD+/PnQaDT6rtHgTAqd+tFwRIWIqEQ5eRos2fUv2nywUwopddxsMfCpmvh7ZieGFHoi5RpRefPNN/HNN9/g/fffR2hoKABg7969mDt3LrKysvDuu+/qtUhDMzFlUCEiKo2T15LQ94t9Wm1DmtVE5IBGMOVdjUkPyhVUVq5cia+//lq6azIANGrUCDVq1MDLL79c+YOKolBQMWFQISLS5dCV+xjy1X5pu1dDD3wytAmUZqYlfBdR2ZQrqNy/fx8BAQFF2gMCAnD//v0nLkpuhUdU1FzwjYioiMU7LuKjmAvS9pKwp9Aj2AMKBUdRSL/KFVQaN26Mzz//vMgqtJ9//jkaNWqkl8JkZVI4qHAyLRFRckYuxn5/BBZmJjh7MwX30nOkfdumtkM9dzsZq6OqrFxB5YMPPkDv3r2xfft2aQ2V/fv349q1a9i8ebNeC5RFocuT1QqOqBBR9ZWUkYMBX/6Ny3fTde7/Z34PWJrzVA9VnHJd9dO+fXtcuHAB/fv3R1JSEpKSkjBgwACcOXMG33//vb5rNLxCc1TUJuYyFkJEJJ8zN5PR7oOdRUJK3yZeGNXKF3/P7MSQQhWu3OuoeHl5FZk0e/LkSXzzzTdYtmzZExcmq0IjKhpFuX9ERESVUq5ag8FL92vdl8fB2hzrxrWGv6sN56GQQfFdWJfCc1TAvxaIqHoQQiAxNRst39uh1b5yTAu0r+cqU1VU3TGo6FJ4jgqDChFVA1m5aoR9fRBH4x5ote+b2Qk1uOw9yYhBRRetoFKuaTxERJWCRiOw+lA8/rchtsi+f9/rxUXbSHZlCioDBgwocX9SUtKT1GI8CgWVPAYVIqqChBA4dOU+hi47UGTfuPb+mNGjPueikFEoU1BRqVSP3T9q1KhyFfL+++9j1qxZmDx5MhYtWlSux9Cbwvf6UTCoEFHVcvF2KiKiDuNGUqZW+4CnamB27wZwsDZnSCGjUaagEhUVVSFFHD58GF999ZXxLBZXeERFcI4KEVUNy3dfxscxF5CZq5bamvs5YnCIN57ydUQdN1sZqyPSTfY5KmlpaQgLC8Py5cvxzjvvyF1OPgWv+iGiyk8IgX2X7sHHyRq/nbqJD7eel/aZKIBvI5qjQ303GSskejzZg8qECRPQu3dvdOnSxYiCSuE5KgwqRFQ5bTx5E5PXnCjS/mrXenihbW1YWfD3Gxk/WYPKmjVrcOzYMRw+fLhUx2dnZyM7O1vaTklJqZjCCq+jInielogqFyEE5m86h2/3XdFqb1fPFStHN+f8E6pUZAsq165dw+TJkxETEwNLy9Ld+C8yMhLz5s2r4MqgNaKSyxEVIqpErt3PwMurjuH0jWSpbUqXuujXpAb8XGxkrIyofBRCCCHHE2/YsAH9+/eHqWmh0Qu1GgqFAiYmJsjOztbaB+geUfH29kZycjLs7e31V9yDOODT/Im9e7ptRtvWofp7bCIiPVNrBP69k4Z5v53Bvkv3pPY2dVww99kgTpIlo5OSkgKVSlWq92/ZRlQ6d+6M06dPa7WNHj0aAQEBmDFjRpGQAgBKpRJKpbLiiyt06odzVIjImCWmZqHFuzuKtK9+sSVa1XbmaR6q9GQLKnZ2dggODtZqs7GxgbOzc5F2gyt86kdwHRUiMj7/JKQgcvM/+OvCHa32QE97fD6iKfxdOYpCVYPsV/0YJa3LkxlUiMh4HL56H7PWn8alxDSt9mld62FIM294qEo354+osjCqoLJr1y65S8jHERUiMjL30rIR8s72Iu0fD2mMFrWcUNPRWoaqiCqeUQUVo1H4poSyTDUmInooO0+N8T8cK9K+YjQXbKOqj0FFp4fpJE8jYxlEVK3dSs7EX+fvYOb6hxce9Az2wLy+QXCz4ykeqh4YVHQpdMU2F3wjIkPLU2vw4bbz+Oqvy1rty0aGoFuQh0xVEcmDQUUXi4eLImUp+FcLERnW1J9P4reTN7XaFg1twpBC1RKDii4W1ljsuxi7LtxFd4WF3NUQUTWRlJGDbp/sRmJq/sKWPYI88FqP+rzUmKo1BpVixNs2xlFxHZ01nE1LRBUrJ0+DtzbG4sdD16S2oc28sWBQIxmrIjIODCrFMDPNn5uiYVAhogr06faL+GT7Ba02O0szRA5oKFNFRMaFQaUYpib5QSWPQYWIKsi0n05g/fEbWm3rxrVCMz8nmSoiMj4MKsUwM8lfS0XNoEJEepadp8bnf16SQkoNByusHdcKXg5WMldGZHwYVIrBERUiqgjZeWoMX3YAx+KTAADmpgpET2jNdVGIisGgUoyCoMIRFSLSl+sPMvDM4r1IysgFALSt64JvI5rD3JS36iAqDoNKMRhUiEif1h+7jmk/n5S2vxoZgu5cF4XosRhUimHGoEJEeiCEwJrD1zCr0DL4Cwc3ZkghKiUGlWI8nKPCm/0QUfms2HcFc387K23bKs0w99kgDAqpKWNVRJULg0oxOKJCROV18PI9DF12oEj7n9Pbc9IsURkxqBTDpGBERc2gQkSlt+ZQvNbdjgFgapd6GN/BHxZmnDRLVFYMKsXgiAoRlUVCchZe/O4ITt9IltrsLM2w41WOohA9CQaVYhQs+JbLoEJEj3Hoyn0M+Wq/tN3Czwnv9A9GPXc7GasiqhoYVIphblpw6oeTaYlIt5w8DT6KOY+v/rostbWo5YSfX2olY1VEVQuDSjEKFmDKZVAhIh3upGaj+bvbtdo+GtwY/ZvWkKkioqqJQaUYD4MKT/0Q0UMJyVnYfPoW3t708LLj3g09sWBQI9gq+SuVSN/4v6oYZv+d+uGIChEBwLX7GWj7wc4i7R3qu+KLsKdkqIioemBQKYbFfyMqvDyZiOLupaP9h7u02jxVllj/cmt4qnjHY6KKxKBSDLP/gkoOR1SIqr3C9+ip4WCF3ye1gcrKHAqFQsaqiKoHBpVimPPUDxEB2Hk+EUfjHgDIDyn7ZnaSuSKi6oVBpRg89UNUvcXeSMa0n0/gwu00APkTZj8f0VTmqoiqHwaVYpjx8mSiakkIgQmrj2Hz6QSpLcjLHh8NacxTPUQyYFAphnTqh3dPJqpWpv18UgopSjMTfDCoEXo39JT+eCEiw2JQKYa0jkoeT/0QVQcpWbmYsyEWG07cBJB/Vc/KMS24DD6RzBhUisGVaYmqj0fXSBkUUhMfDmrEUz1ERoBBpRhc8I2o6svKVWPuxjNYc/ia1PZSu9qY2rUeQwqRkZD1pOuSJUvQqFEj2Nvbw97eHq1atcKWLVvkLEliwSX0iaq0PLUGT0fu0Aopf73WAbN6BcLS3FTGyoioMFmDSs2aNfH+++/j6NGjOHLkCDp16oS+ffvizJkzcpYF4OGpnzxOpiWqcnLVGoxZeQRJGbkAgAae9jgxpyt8nW1kroyIHiXrqZ8+ffpobb/77rtYsmQJDhw4gKCgIJmqyvfw1I+AEILDwERVhEYjMGjJ3zh5PRkAMKBpDV56TGTEjGaOilqtxtq1a5Geno5WrVrpPCY7OxvZ2dnSdkpKSoXVY17oUsRctYCFGX+JEVV2N5Iy0fHDXdKtMV5sWwtv9ApkSCEyYrIHldOnT6NVq1bIysqCra0toqOj0aBBA53HRkZGYt68eQapq2AdFSD/9I+FvGfJiOgJZOWqMXTZAZy8liS19WrogTd76/5dQ0TGQyGEkHW2aE5ODuLj45GcnIx169bh66+/xl9//aUzrOgaUfH29kZycjLs7e31WleuWoO6b+ZP7D05pxtU1uZ6fXwiqnhCCCzfcxnvbf5HarNTmmHNS08jyEslY2VE1VtKSgpUKlWp3r9lH1GxsLBAnTp1AAAhISE4fPgwPv30U3z11VdFjlUqlVAqlQapy8zk4YgKV6clqpxe+fE4Np26JW37OVvj90ltYaOU/VcfEZWS0f1v1Wg0WqMmclEoFDA3VSBXLbiWClElcys5E8OXHcDVexlS2y/jWyHE10nGqoioPGQNKrNmzULPnj3h4+OD1NRUrF69Grt27cLWrVvlLEtibmqCXLWay+gTVSInryWh7xf7pO2uDdyxeHhTro1CVEnJGlQSExMxatQo3Lp1CyqVCo0aNcLWrVvRtWtXOcuSFJz+4akfIuMnhMCDjFxMXnNcapvZMwDj2vvLWBURPSlZg8o333wj59M/loUZ7/dDVBkIITDuh6PYeua21PbT2KfRsrazjFURkT4Y3RwVYyKtTstl9ImM1s7ziZj20wk8+G+VWQD44fmWDClEVQSDSgkKVqfN4YgKkdFRawTGfncEO/5J1Gr/JrwZ2tR1kakqItI3BpUSFIyo5OYxqBAZk1y1Bk3mbUN6jhoA0K6eK17rVh8NvOxhasJVZomqEgaVEijN8q8S4IgKkfGIv5eBdh/ulLbrudsiKqI5AwpRFcWgUoKCybTZuQwqRHLTaAQ+2X4Bi/+8JLV52Fti29T2MlZFRBWNQaUEyoKgwlM/RLJKTMnChNXHcPjqA632v2d2kqkiIjIUBpUSPAwqapkrIaqehBD434ZYrDoYL7V1CXTDuPb+aObHVWaJqgMGlRIUzFHhiAqR4cWcvY2x3x9B4dumLn3uKfQI9pSvKCIyOAaVEijN80dUchhUiAxGCIEFf5zH0r/+1WrfNb0D/FxsZKqKiOTCoFICnvohMqzE1CyMjjqMMzdTpLZJnepgUue6MPtvuQAiql4YVEognfrhVT9EFUqtEdh2JgGRW/5B/P38Ox4/XdsJPzzfkgGFqJpjUCkBr/ohqnjZeWpM/ekENp9OkNqmda2HVzrVgULBtVGIqjsGlRLw1A9RxRFC4Lv9cVj850XcTcsBAPRu6IlXu9VDbVdbmasjImPBoFICjqgQVYycPA3e3nQGPxzIv+zYztIM7/QLRt8mNWSujIiMDYNKCZTmnKNCpG85eRqM+vYgDly+DwDo18QL7w9sBMv//r8RERXGoFKCghEV3uuHSD9WH4zHG9GnAQBmJgrM7xeMYc29OReFiIrFoFICzlEh0o88tQbjfjiK7ecSpbbPhjdFr4ZcvI2ISsagUgLelJDoye25eAcjvzmk1fbD8y3Rpq6LTBURUWXCoFICLqFPVH5qjUDk5nP4eu8Vqa17kDuWPhfCUz1EVGoMKiXgqR+i8nv393P4dt/DkPK/3oF4vk0thhQiKhMGlRIU3OuHIypEZfP9/qtSSOlY3xWLhjWFyspc5qqIqDJiUCkBl9AnKpufDsdjxi+npe1uDdyxbFQzGSsiosqOQaUEvDyZqHSSM3Mx7vuj2H/5ntQW0doPM3sGyFgVEVUFDColeHjVD+eoEBUn9kYyZvxySrrjsZfKEt9ENEegp73MlRFRVcCgUgJe9UNUsmW7/8V7m/+RttvWdcHyUc24yiwR6Q2DSgl4rx8i3S7eTkXvz/ZqnRZdM/ZpPF3bWcaqiKgqYlApwcOrfnjqhwjIv+PxjnOJeHXtSa2Qsn1ae9Rx4x2PiUj/GFRKUHDqJ1ctoNEImJhw/QeqvrJy1Zi4+pjWMvhP13bCVyOb8dJjIqowDColKDj1A+Rf+WNpwvPuVD3lqTUYvHQ/Tt9Iltp2v9YRPs7WMlZFRNUBg0oJCgeV7FwNJwhStXTw8j0MXXZA2n6pXW283iMAphxhJCIDYFApgZmpCUwUgEYUzFPh8DZVLxtP3sSkH49L2691r48JHevIWBERVTcmjz+k4kRGRqJ58+aws7ODm5sb+vXrh/Pnz8tZUhEF81SyuDotVSP303OwcOt5TFnzMKRM7FgH49r7y1gVEVVHso6o/PXXX5gwYQKaN2+OvLw8vPHGG+jWrRvOnj0LGxsbOUuTWFmYIjNXjSxe+UPVgBACm07dwiuFRlEa1lBh9YstYWfJEUUiMjxZg8off/yhtb1ixQq4ubnh6NGjaNeunUxVabO2MMX9dCA9O0/uUogqVE6eBs99fRCHrt6X2noEeeDT4U2kkUUiIkMzqjkqycn5VxQ4OTnp3J+dnY3s7GxpOyUlpcJrsrHI/xFl5HBEhaqu6w8y8Pq6U1oh5b3+DTGipY+MVRERGVFQ0Wg0mDJlCkJDQxEcHKzzmMjISMybN8+gdVkr8/+S5IgKVUUP0nPwy7HrWPDHP8hVCwDAUz4OWPpcCNzsLWWujojIiILKhAkTEBsbi7179xZ7zKxZszBt2jRpOyUlBd7e3hVal7VFflDJ5I0JqYr55eh1zIo+jZz/bhHh42SNDwc1Qksug09ERsQogsrEiROxadMm7N69GzVr1iz2OKVSCaVSacDKAOv/Tv2kZzOoUNWx/th1vLr2pLQ9q2cAXmhbm2ujEJHRkTWoCCHwyiuvIDo6Grt27UKtWrXkLEcnm/9GVDJyeOqHKj8hBL7ecwXvbj4HAGhRywlfPRcCRxsLmSsjItJN1qAyYcIErF69Gr/++ivs7OyQkJAAAFCpVLCyspKzNIm1kiMqVDVk5aoxZc0J/HEm///ZoJCaWDCwEUdRiMioyRpUlixZAgDo0KGDVntUVBQiIiIMX5AOHFGhquB2ShZeWX1cuqrn5Q7+mN6tPm+0SURGT/ZTP8ZOmqPCoEKV1MlrSXjum4NIzcqDlbkpvo1ojlb+nDBLRJWDUUymNWY2yoIRFZ76ocrn58PX8PovpwAANRyssGhYEzT3071OERGRMWJQeYyCEZUMzlGhSubrPZfxzu/5k2btLM3w87hWqOFgHHO/iIhKi0HlMQrWUeGpH6os8tQa/HgoXgopoXWcsWhoU7jaGfbSfiIifWBQeQxrLqFPlci1+xlo+8FOrbZPhjZhSCGiSotB5TFsuIQ+VRLXH2Rg9IrD0vbEjnUwtWs9Xn5MRJUag8pjcESFjF1KVi4WxVzEt/uuSG1Tu9TD5C51ZayKiEg/GFQe4+FVPxxRIeMTeyMZfb/YB7Um/1L/eu62+GhwEzSsqZK5MiIi/WBQeQwbjqiQEdJoBDaevInXfzklhZRXOtXBpM51YW5qInN1RET6w6DyGNYWD9dR0WgEV/Ik2Qkh8Povp7Du6HUA+ZceR0U0RzOuj0JEVRCDymPYKB/+iDJy1bBV8kdG8hFCYPmey1JI6dXQA2/3DYaLLa/qIaKqie+6j6E0M4G5qQK5aoGUzFwGFZLNyWtJmLX+NM7eSgEAvNkrEC+2qy1zVUREFYsnsx9DoVBAZWUOIP/qCiI5XEpMxYjlB6SQMryFN0MKEVULHB4oBXtLc9xNy0FKJq/8IcNbfTAeb0SfBgD4OVtjyXMhCPS0l7kqIiLDYFApBbuCEZVMjqiQ4Vy7n4HnVx7GhdtpAABXOyW+f74lvJ2sZa6MiMhwGFRKoeDUTzKDChlArlqDDcdvYMYvp/DflcfoWN8Vi4Y1lV6LRETVBYNKKdhb5v+YOEeFKtqt5Ey0ivxTq23pcyHoEewhU0VERPJiUCkFe+nUD+eoUMW5/iADk9eckLY7Bbhh0bAmsLfkKAoRVV8MKqVQ8EbBUz9UUSasOobfT9+Stl9qXxszewRAoeACg0RUvTGolAIvT6aKotEIfLvvihRS7C3N8GVYCNrUdZG5MiIi48CgUgr2Vv/NUeGICulRVq4a7/x+Fj8ciJfadr/eEQ7WFjJWRURkXBhUSoGnfkjftp5JwOvrTkmvqXHt/fF69/q8lxQR0SMYVErh4akfTqalJ3M7JQtvRp/G9nOJUtusngEY264256MQEenAoFIK9lzwjfTgrwt38MrqY1LgDfCww2fDm6Keu53MlRERGS8GlVIoWEclKSMHQgj+5UtlkqfWYNH2i/hi1yUIATSsocIHgxpxGXwiolJgUCmFGo5WMDVRID1HjYSULHiqrOQuiSoJjUZg+tqT2HDiJgCgT2MvLBzcCEozU5krIyKqHBhUSkFpZgpfZ2tcvpOOy3fSGVSoVFYdjMOb0bHS9kvtamNat3oMKUREZcCgUkqeKktcvpOOxNQsuUshI6fRCHz250Us2n5Ranuvf0OMaOkjY1VERJUTg0opudoqAQB3UrNlroSM2aEr9zHn11j8k5AKAKjtYoMlz4WgvgcnzBIRlQeDSim52jGokG5CCEQfv4ENJ25i94U7Uvuzjb3w6bAmnHxNRPQEGFRKiUGFdMnKVeOFlUew99JdqW3gUzUxvoM/6rjZylgZEVHVYCLnk+/evRt9+vSBl5cXFAoFNmzYIGc5JXKzswQAJDKo0H+Oxj1A78/2SCHFxVaJlWNa4KMhjRlSiIj0RNYRlfT0dDRu3BhjxozBgAED5CzlsQpGVBhUKDNHjbc3ncWPh/Lv0WNlboq5zzbA0OacLEtEpG+yBpWePXuiZ8+ecpZQal4O+Zck33iQyUXfqqnsPDXeWB+LX45dl9p6BHlg7rNB8FBZylgZEVHVVanmqGRnZyM7++GIRkpKisGeu4aDFRQKIDNXjbtpOdIIC1UPsTeSEfb1Qa0bUy4a2gT9mtaQsSoioqpP1jkqZRUZGQmVSiV9eHt7G+y5LcxM4Gmf/1dz/P0Mgz0vyUujEdh48iaeWbxXCike9pY4NrsrQwoRkQFUqhGVWbNmYdq0adJ2SkqKQcOKt5M1biZn4fqDDIT4OhrsecnwNBqBHw/Ha60s6+tsjU2vtIGdpbmMlRERVS+VKqgolUoolfKdcvF1tsbBK/fx75102WqginU/PQe/n76F2RtitdptlWb4NqI5QwoRkYFVqqAit/oe+Xe7PZ9guLkxZBiJqVmY/OMJ7L98r8i+yAENMbwFr+ghIpKDrEElLS0Nly5dkravXLmCEydOwMnJCT4+xvfGEPjfMugFy6NT5afRCPzv11isPhgvtVlbmMJTZYkvwp5CwH/hlIiI5CFrUDly5Ag6duwobRfMPwkPD8eKFStkqqp4Bfdrib+fgfTsPNgoOSBVWak1AhtP3sCn2y/i6r2Hk6NrOlph65R2/LclIjISsv427tChA4QQcpZQJs62SrjaKXEnNRsXbqeiqQ8n1FZG28/exkcxF3Du1sNTeHXcbPH1qGbwc7GRsTIiInoU/2wsowAPO9xJzcY/CQwqlc2t5EzM3nAG28/dltq6BLpjSpe6CPKy5yJ+RERGiEGljAI97bHn4l2cuZksdylUSievJWHxn5ew++Id5ORpAAD+rjb4amQI6rjZyVwdERGVhEGljJr7OWHZ7svYeOImZvQI4OWqRipPrcHK/XGYv+msVnuLWk4Ib+WHXg09OIJCRFQJMKiUUacAN/g5W+PqvQx8tO0C5j4bJHdJVMjNpEws/etffH8gDoWnP9lbmuGToU3QKcCNAYWIqBJhUCkjUxMFWvm74Oq9eHy3/yre6tOAb3wyS8vOw8q/r2LR9gvIVT9MJ0ozEzzTyAvDW3ijmZ+TjBUSEVF5MaiUw+TOdfHjoXhoBPDvnTTOczAwIQSSMnKxcv9V3EnNxqpCa6AUeL5NLUzrWo+XGRMRVXL8LV4OHipLtKrtjP2X72HTqVuY0oVBxRD+vZOGfl/sQ2pWns79gZ726NbAHeGt/eBkY2Hg6oiIqCIwqJTTwJCa2H/5Hr7bH4eJHevAzLRS3Yi60ridkoU1h65h3793cejK/SL7m/s5YnCINwaF1ISJCU/BERFVNQwq5dSviRfmbzqL++k5OHMzBY29HeQuqdLLylUDAO6kZuPqvXT8cCAOW8/cLnKclbkpfhnfGr7O1jy1Q0RUxfG3fDmZmZqgZS0nbDt7G9HHb8DfzRY2FqacWFsGao3AuVsp2HU+EVvP3MbpG8WvTTO+gz96BXsiyMueIydERNUIg8oTGNbCG9vO3saKv69ixd9XUd/dDhtfCYXSzFTu0ozCndRspGXnITUrF+uP3cDNpEyYKBTIUWtwKzlLawn7R9VysYGPkzW6BLphWAsfmPPUGhFRtcSg8gQ61HNDx/qu2Hn+DgDg/O1U/H3pHkLruEChQLV4c81Ta7D+2A2cupGEjGw11P9dkfPXhTulfozmfo4I8lKhpqMVOtR3hZ+zDef8EBERAEAhKtNdAR+RkpIClUqF5ORk2Nvby1JDnlqDk9eT8N3+OPx64qbUbqc0w8ZX2qBWJb/J3aXENCjNTJCYmo0Ff/yDQ1fuo7G3AzKy8+DrbI0Lt9MQfz+jxMdwsrHA/fQcaXtsu9p4yscRWblqtK/nCkdeoUNEVK2U5f2bIypPyMzUBCG+TsjO1WgFldTsPEREHcKHgxqjRS3jWmwsLTsPd1OzkZiajfoedriUmIaVf1/F4av3EeSlwvZzt+GpsoRaI5CYml3k+09eSwIAXExM02qv62aLjBw17CzNUNfdDlO71IWTjQUcrPODiFojYMr5JUREVAYcUdETtUZg+LIDOHS16CW07vZKzO8bjAZe9th25jZuJmWihqMVNAJ4ppEn3O0tkZiahf3/3oNaI9CytjNqOFghJ0+DBxk5UFmZw9K86LyX7Dw1ElOyYas0g0YIONlYQKFQQKMR+OviHfx8+Brqe9jBxsIM6Tl5sLc0x4YTN3Dq+pPdUPHNXoGwUZohK1cNhQLoUN+t0o8cERGR4ZTl/ZtBpQKoNQIzfzmFtUevl+v7rS1MEeLriOPxSUjLzoO9pRlslGa4lZwFF1sLNPBSYXcxc0AUCkABQFPGf9WuDdyhsjKHiQKIvZECS3MThPg64vk2tWFiAjhYWcDCjPNGiIjoyTGoGImsXDV+OBCHtUeu4/ztVIM+t72lGVKy8qA0M4G/qy00Iv+0S313O3QLckeQlwo1HKxw6U4aHKzN4WZnadD6iIio+mJQMTJqjcDCbedxKykT4zr4Y9+le/B1skaXBu6IPn4dR+MeoE0dFzzl6whThQLfH4jDDwfi0NzPCR0D3LD97G3suXgXmblqDG3mDWulKW4mZcLP2QZBNVSo724HW0szbI1NQIivI8xMFfB3tdV5uoiIiEhuDCpERERktMry/s1JB0RERGS0GFSIiIjIaDGoEBERkdFiUCEiIiKjxaBCRERERotBhYiIiIwWgwoREREZLQYVIiIiMloMKkRERGS0GFSIiIjIaDGoEBERkdFiUCEiIiKjxaBCRERERotBhYiIiIyWmdwFPAkhBID820UTERFR5VDwvl3wPl6SSh1UUlNTAQDe3t4yV0JERERllZqaCpVKVeIxClGaOGOkNBoNbt68CTs7OygUCr0+dkpKCry9vXHt2jXY29vr9bGNEftbtbG/VV916zP7W7kJIZCamgovLy+YmJQ8C6VSj6iYmJigZs2aFfoc9vb2VeJFUVrsb9XG/lZ91a3P7G/l9biRlAKcTEtERERGi0GFiIiIjBaDSjGUSiXeeustKJVKuUsxCPa3amN/q77q1mf2t/qo1JNpiYiIqGrjiAoREREZLQYVIiIiMloMKkRERGS0GFSIiIjIaDGo6PDFF1/Az88PlpaWaNmyJQ4dOiR3SeUSGRmJ5s2bw87ODm5ubujXrx/Onz+vdUxWVhYmTJgAZ2dn2NraYuDAgbh9+7bWMfHx8ejduzesra3h5uaG1157DXl5eYbsSrm8//77UCgUmDJlitRW1fp748YNPPfcc3B2doaVlRUaNmyII0eOSPuFEJgzZw48PT1hZWWFLl264OLFi1qPcf/+fYSFhcHe3h4ODg54/vnnkZaWZuiuPJZarcbs2bNRq1YtWFlZwd/fH/Pnz9e6V0hl7+/u3bvRp08feHl5QaFQYMOGDVr79dW/U6dOoW3btrC0tIS3tzc++OCDiu6aTiX1Nzc3FzNmzEDDhg1hY2MDLy8vjBo1Cjdv3tR6jKrS30eNGzcOCoUCixYt0mqvTP3VG0Fa1qxZIywsLMS3334rzpw5I1588UXh4OAgbt++LXdpZda9e3cRFRUlYmNjxYkTJ0SvXr2Ej4+PSEtLk44ZN26c8Pb2Fjt27BBHjhwRTz/9tGjdurW0Py8vTwQHB4suXbqI48ePi82bNwsXFxcxa9YsObpUaocOHRJ+fn6iUaNGYvLkyVJ7Verv/fv3ha+vr4iIiBAHDx4Uly9fFlu3bhWXLl2Sjnn//feFSqUSGzZsECdPnhTPPvusqFWrlsjMzJSO6dGjh2jcuLE4cOCA2LNnj6hTp44YPny4HF0q0bvvviucnZ3Fpk2bxJUrV8TatWuFra2t+PTTT6VjKnt/N2/eLN58802xfv16AUBER0dr7ddH/5KTk4W7u7sICwsTsbGx4scffxRWVlbiq6++MlQ3JSX1NykpSXTp0kX89NNP4p9//hH79+8XLVq0ECEhIVqPUVX6W9j69etF48aNhZeXl/jkk0+09lWm/uoLg8ojWrRoISZMmCBtq9Vq4eXlJSIjI2WsSj8SExMFAPHXX38JIfJ/EZibm4u1a9dKx5w7d04AEPv37xdC5P/HMjExEQkJCdIxS5YsEfb29iI7O9uwHSil1NRUUbduXRETEyPat28vBZWq1t8ZM2aINm3aFLtfo9EIDw8P8eGHH0ptSUlJQqlUih9//FEIIcTZs2cFAHH48GHpmC1btgiFQiFu3LhRccWXQ+/evcWYMWO02gYMGCDCwsKEEFWvv4++kemrf19++aVwdHTUej3PmDFD1K9fv4J7VLKS3rgLHDp0SAAQcXFxQoiq2d/r16+LGjVqiNjYWOHr66sVVCpzf58ET/0UkpOTg6NHj6JLly5Sm4mJCbp06YL9+/fLWJl+JCcnAwCcnJwAAEePHkVubq5WfwMCAuDj4yP1d//+/WjYsCHc3d2lY7p3746UlBScOXPGgNWX3oQJE9C7d2+tfgFVr78bN25Es2bNMHjwYLi5uaFp06ZYvny5tP/KlStISEjQ6q9KpULLli21+uvg4IBmzZpJx3Tp0gUmJiY4ePCg4TpTCq1bt8aOHTtw4cIFAMDJkyexd+9e9OzZE0DV6++j9NW//fv3o127drCwsJCO6d69O86fP48HDx4YqDflk5ycDIVCAQcHBwBVr78ajQYjR47Ea6+9hqCgoCL7q1p/S4tBpZC7d+9CrVZrvUkBgLu7OxISEmSqSj80Gg2mTJmC0NBQBAcHAwASEhJgYWEh/acvULi/CQkJOn8eBfuMzZo1a3Ds2DFERkYW2VfV+nv58mUsWbIEdevWxdatWzF+/HhMmjQJK1euBPCw3pJezwkJCXBzc9Pab2ZmBicnJ6Pr78yZMzFs2DAEBATA3NwcTZs2xZQpUxAWFgag6vX3UfrqX2V6jReWlZWFGTNmYPjw4dJN+apafxcsWAAzMzNMmjRJ5/6q1t/SqtR3T6bSmzBhAmJjY7F37165S6kw165dw+TJkxETEwNLS0u5y6lwGo0GzZo1w3vvvQcAaNq0KWJjY7F06VKEh4fLXJ3+/fzzz1i1ahVWr16NoKAgnDhxAlOmTIGXl1eV7C89lJubiyFDhkAIgSVLlshdToU4evQoPv30Uxw7dgwKhULucowKR1QKcXFxgampaZGrQG7fvg0PDw+ZqnpyEydOxKZNm7Bz507UrFlTavfw8EBOTg6SkpK0ji/cXw8PD50/j4J9xuTo0aNITEzEU089BTMzM5iZmeGvv/7CZ599BjMzM7i7u1ep/np6eqJBgwZabYGBgYiPjwfwsN6SXs8eHh5ITEzU2p+Xl4f79+8bXX9fe+01aVSlYcOGGDlyJKZOnSqNnlW1/j5KX/2rTK9x4GFIiYuLQ0xMjDSaAlSt/u7ZsweJiYnw8fGRfn/FxcXh1VdfhZ+fH4Cq1d+yYFApxMLCAiEhIdixY4fUptFosGPHDrRq1UrGyspHCIGJEyciOjoaf/75J2rVqqW1PyQkBObm5lr9PX/+POLj46X+tmrVCqdPn9b6z1Hwy+LRN0m5de7cGadPn8aJEyekj2bNmiEsLEz6uir1NzQ0tMjl5hcuXICvry8AoFatWvDw8NDqb0pKCg4ePKjV36SkJBw9elQ65s8//4RGo0HLli0N0IvSy8jIgImJ9q8sU1NTaDQaAFWvv4/SV/9atWqF3bt3Izc3VzomJiYG9evXh6Ojo4F6UzoFIeXixYvYvn07nJ2dtfZXpf6OHDkSp06d0vr95eXlhddeew1bt24FULX6WyZyz+Y1NmvWrBFKpVKsWLFCnD17VowdO1Y4ODhoXQVSWYwfP16oVCqxa9cucevWLekjIyNDOmbcuHHCx8dH/Pnnn+LIkSOiVatWolWrVtL+gst1u3XrJk6cOCH++OMP4erqapSX6+pS+KofIapWfw8dOiTMzMzEu+++Ky5evChWrVolrK2txQ8//CAd8/777wsHBwfx66+/ilOnTom+ffvqvJy1adOm4uDBg2Lv3r2ibt26RnO5bmHh4eGiRo0a0uXJ69evFy4uLuL111+Xjqns/U1NTRXHjx8Xx48fFwDExx9/LI4fPy5d5aKP/iUlJQl3d3cxcuRIERsbK9asWSOsra1luXy1pP7m5OSIZ599VtSsWVOcOHFC63dY4Staqkp/dXn0qh8hKld/9YVBRYfFixcLHx8fYWFhIVq0aCEOHDggd0nlAkDnR1RUlHRMZmamePnll4Wjo6OwtrYW/fv3F7du3dJ6nKtXr4qePXsKKysr4eLiIl599VWRm5tr4N6Uz6NBpar197fffhPBwcFCqVSKgIAAsWzZMq39Go1GzJ49W7i7uwulUik6d+4szp8/r3XMvXv3xPDhw4Wtra2wt7cXo0ePFqmpqYbsRqmkpKSIyZMnCx8fH2FpaSlq164t3nzzTa03rcre3507d+r8PxseHi6E0F//Tp48Kdq0aSOUSqWoUaOGeP/99w3VRS0l9ffKlSvF/g7buXOn9BhVpb+66Aoqlam/+qIQotCyjkRERERGhHNUiIiIyGgxqBAREZHRYlAhIiIio8WgQkREREaLQYWIiIiMFoMKERERGS0GFSIiIjJaDCpEVKUoFAps2LBB7jKISE8YVIhIbyIiIqBQKIp89OjRQ+7SiKiSMpO7ACKqWnr06IGoqCitNqVSKVM1RFTZcUSFiPRKqVTCw8ND66Pgrq0KhQJLlixBz549YWVlhdq1a2PdunVa33/69Gl06tQJVlZWcHZ2xtixY5GWlqZ1zLfffougoCAolUp4enpi4sSJWvvv3r2L/v37w9raGnXr1sXGjRsrttNEVGEYVIjIoGbPno2BAwfi5MmTCAsLw7Bhw3Du3DkAQHp6Orp37w5HR0ccPnwYa9euxfbt27WCyJIlSzBhwgSMHTsWp0+fxsaNG1GnTh2t55g3bx6GDBmCU6dOoVevXggLC8P9+/cN2k8i0hO574pIRFVHeHi4MDU1FTY2Nlof7777rhAi/47e48aN0/qeli1bivHjxwshhFi2bJlwdHQUaWlp0v7ff/9dmJiYiISEBCGEEF5eXuLNN98stgYA4n//+5+0nZaWJgCILVu26K2fRGQ4nKNCRHrVsWNHLFmyRKvNyclJ+rpVq1Za+1q1aoUTJ04AAM6dO4fGjRvDxsZG2h8aGgqNRoPz589DoVDg5s2b6Ny5c4k1NGrUSPraxsYG9vb2SExMLG+XiEhGDCpEpFc2NjZFTsXoi5WVVamOMzc319pWKBTQaDQVURIRVTDOUSEigzpw4ECR7cDAQABAYGAgTp48ifT0dGn/vn37YGJigvr168POzg5+fn7YsWOHQWsmIvlwRIWI9Co7OxsJCQlabWZmZnBxcQEArF27Fs2aNUObNm2watUqHDp0CN988w0AICwsDG+99RbCw8Mxd+5c3LlzB6+88gpGjhwJd3d3AMDcuXMxbtw4uLm5oWfPnkhNTcW+ffvwyiuvGLajRGQQDCpEpFd//PEHPD09tdrq16+Pf/75B0D+FTlr1qzByy+/DE9PT/z4449o0KABAMDa2hpbt27F5MmT0bx5c1hbW2PgwIH4+OOPpccKDw9HVlYWPvnkE0yfPh0uLi4YNGiQ4TpIRAalEEIIuYsgoupBoVAgOjoa/fr1k7sUIqokOEeFiIiIjBaDChERERktzlEhIoPhmWYiKiuOqBAREZHRYlAhIiIio8WgQkREREaLQYWIiIiMFoMKERERGS0GFSIiIjJaDCpERERktBhUiIiIyGgxqBAREZHR+j8DqVtMYJUJfAAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.372446208750755, 4.408008573983182, 3.8654470329006205, 3.43820219721083, 3.0788994657786515, 2.776198476256388, 2.5085109058576793, 2.2772815310250762, 2.0682286378614863, 1.891585909966633, 1.7339235257809904, 1.5984956555866776, 1.4688003106117813, 1.36047718115042, 1.2573771235789821, 1.2163683124416207, 1.1240049691797704, 1.0631060737666578, 0.9999874184139865, 0.9536026681136053, 0.909282773529088, 0.8782606733385969, 0.8184408648421555, 0.7917987101085857, 0.7773635974509401, 0.761407089598744, 0.7311803426004864, 0.7341639765724352, 0.7039198818708409, 0.6766366386802936, 0.6667393478615804, 0.6479832044574593, 0.6176852072594607, 0.6002752520591371, 0.5838595282483724, 0.5762542013838632, 0.5694861956902496, 0.5774377162568864, 0.578052692853075, 0.5768809969403879, 0.5753070315577964, 0.5769955792641382, 0.5541251030672657, 0.5313514652899781, 0.5092816777717196, 0.5236484086412805, 0.511769317065693, 0.5257803828217789, 0.5212664028171504, 0.5128699593978738, 0.5236081756381594, 0.5303117199114042, 0.5245172809257136, 0.5043666957391059, 0.4856149168778086, 0.48678430915026283, 0.4873531028695023, 0.4873753928415224, 0.4875898059372765, 0.47262753194826623, 0.4820327222450792, 0.47357167312749043, 0.4685618579253662, 0.46552481402424883, 0.4640259386680448, 0.4648137205228577, 0.4518354139315343, 0.4427749825979345, 0.43774099696277446, 0.44803602961714284, 0.4588301671702019, 0.46199222755405656, 0.46038178488583015, 0.4514900282415223, 0.45928458219262586, 0.47144665314158885, 0.4514248661342246, 0.4426095651408901, 0.45120109703904293, 0.45358615116509293, 0.4666362715376922, 0.45775156870298583, 0.4621446206650676, 0.46284654286843285, 0.4599045634107207, 0.45654708370661384, 0.45539693508589374, 0.44625651526046417, 0.45072313366165284, 0.44537213882107235, 0.4491438514565481, 0.45254829629927407, 0.4492581452402527, 0.4515379487945101, 0.4542068502886721, 0.4655807903504215, 0.45960821040322475, 0.45378693207629567, 0.4535934094937461, 0.464610240860749, 0.4643012507588988, 0.4595095694026433, 0.4568945200202954, 0.4506726632479495, 0.44282650586022454, 0.4364478199216737, 0.43891074894002385, 0.43530191535284585, 0.44172102373456795, 0.45068693825696693, 0.4513756805870779, 0.45935122031239406, 0.4510269462873241, 0.44906514666399383, 0.45973671613549133, 0.4520562034524336, 0.4453189684268822, 0.44161127260203525, 0.4422466698944122, 0.4340471253040501, 0.4426076005922886, 0.4389283808096937, 0.43786916125015585, 0.45455394297958124, 0.44779555427703355, 0.45058179317221486, 0.44972942298509727, 0.44385331515468107, 0.44379889030590325, 0.4452003822765285, 0.44435150771206083, 0.4480465082353124, 0.4544982100807056, 0.4444574344161875, 0.447176600443924, 0.4491998748817394, 0.4579876959944751, 0.4472656146712127, 0.44935018615019645, 0.4524843733696876, 0.46103092383335553, 0.4495067004237662, 0.4550787540287286, 0.4560027682092662, 0.4577809843603409, 0.45807589941331733, 0.4582968813909485, 0.45414414981926066, 0.4476326607695704, 0.4450302300800138, 0.45040927331872765, 0.4551596752802378, 0.45518951764791127, 0.4543001179730868, 0.45479417878479395, 0.45125112842607645, 0.4523302823273677, 0.454312152401524, 0.4577758543869087, 0.4630497307950239, 0.463139923083983, 0.4646261321338101, 0.4664786497337195, 0.4742435281035262, 0.46692132880239456, 0.4606196037008112, 0.4598141950233142, 0.4619272751742548, 0.46045540023476084, 0.4640328819910125, 0.46929888204813125, 0.4702161579428747, 0.46712099702766324, 0.46642081209762437, 0.4587757639370359, 0.46209483975333043, 0.4640090930264582, 0.46455022038809635, 0.4684057896248987, 0.4719614810290994, 0.4760187534368074, 0.4715418245683431, 0.474862353112377, 0.47911668265643154, 0.48376137237620803, 0.4777712197002518, 0.47705801175853807, 0.4827037091782654, 0.4873453800104263, 0.4837067455699753, 0.4829773062104195, 0.48330970550391844, 0.4768635807420018, 0.4799966806169524, 0.4770486019803214, 0.4783283704915841, 0.4828736766994252, 0.4898436629009583, 0.49254195752693974, 0.4954076562523619, 0.4957273098675631, 0.4945782418095713, 0.48929221988122706, 0.49602422660009826, 0.49504947621222234, 0.4942569631030077, 0.4951425532847679, 0.49102148973281423, 0.4968869913563459, 0.4914396119361235, 0.48769537432727056, 0.49687120108585586, 0.4951611167058325, 0.4953627648252023, 0.4908583697389388, 0.4988455977605808, 0.4978485166792261, 0.5018668053950576, 0.503991275826439, 0.5033833584950053, 0.5101527096328056, 0.5067631689846891, 0.5007555780119158, 0.5011639775026469, 0.5068076855780713, 0.5060675997655215, 0.5053412736949323, 0.5107752939251284, 0.5102225241171244, 0.5132837839465401, 0.5220596294552373, 0.5127143053355665, 0.519974759616008, 0.5156401905327604, 0.5179030330765793, 0.5190996370408063, 0.5207894402996565, 0.5238120927125663, 0.5259673476944725, 0.5211719254751714, 0.5195892772764779, 0.5209318468140249, 0.5187244672263608, 0.5202907402027995, 0.5195121193254325, 0.5191822725852507, 0.5178911794034277, 0.5238945472559826, 0.5310493305997487, 0.535385655911392, 0.5309236071105958, 0.5373408156098284, 0.5405340318765348, 0.5337627813841875, 0.5345926356944409, 0.540646991287711, 0.540430424518883, 0.5351951047975366, 0.5388624568985582, 0.5385798258540857, 0.5314882132179627, 0.5358290319979113, 0.5391181066513012, 0.537327069882629, 0.5419280376039998, 0.537919484096904, 0.5480203186475424, 0.550068768196825, 0.5523899920681022, 0.552788468886996, 0.5556132505665141, 0.5495877756982428, 0.5545331294459898, 0.5543742320806345, 0.5520532059584244, 0.5540705572441638, 0.5558316700285169, 0.5563592127751388, 0.5589058474033552, 0.5634510429213961, 0.5553818257846932, 0.5618919491115235, 0.5626663421992366, 0.551758199611517, 0.5599809646142841, 0.5650124611068897, 0.561460749923348, 0.5610295646798039, 0.5682072592282308, 0.5691165692652459, 0.5705457180686355, 0.5696906391357992, 0.5699734829740858, 0.5705123400084537, 0.5685496224138497, 0.5689538970009069, 0.5769109864600998, 0.5734415190542363, 0.5791772148972381, 0.5789478960768198, 0.5797102535106923, 0.5847509165358254, 0.5875269007065602, 0.5872643855042177, 0.5866506258158253, 0.5911002934326982, 0.5915588815210389, 0.5937877790803477, 0.5887661991204065, 0.5980780432338998, 0.6022374738198057, 0.597971228496479, 0.5980787763571388, 0.5987537839461389, 0.6028856244208916, 0.6012418549991855, 0.6028169203702219, 0.605540615853972, 0.6060284773129744, 0.6066100417276776, 0.60587332324758, 0.607981345708629, 0.6097254526086078, 0.6164818854259405, 0.6211395109156981, 0.6186356029744875, 0.6269084437335187, 0.6241224356120648, 0.630098993097279, 0.635836970262248, 0.6336388870427627, 0.6379533761575716, 0.6410798103156551, 0.6437086369995378, 0.6451232572146829, 0.6445940321491759, 0.6457591098378807, 0.6552199731966151, 0.6498914914833706, 0.6628387583973424, 0.6569847763895226, 0.6652323617107981, 0.6620595886934435, 0.6701659225873471, 0.6725742046273137, 0.6741800598128367, 0.6704360817372701, 0.6698523543443351, 0.6749583086831945, 0.6862186447178761, 0.6829915931754884, 0.6831398884932837, 0.6840173161595182, 0.6845274635934773, 0.6863245359782504, 0.6921228485588482, 0.6996908256142368, 0.6983140427946116, 0.6970401547688473, 0.7052565989976262, 0.7022865122085417, 0.7082837749384882, 0.7097219103436622, 0.716446788948818, 0.7132988265987661, 0.7236022519283177, 0.7141061917546937, 0.7200837912501531, 0.7236352154008514, 0.7311925416473731, 0.7290314912938869, 0.7363924858950798, 0.7430681007292169, 0.7441261643215509, 0.743526669334734, 0.7511841696937417, 0.7531711990564596, 0.7591190461978614, 0.7600850291653856, 0.7561675032914512, 0.767880275794089, 0.768173976460616, 0.7680626018579648, 0.7749183369296587, 0.7718200889866434, 0.7785388666560694, 0.7844445198724537, 0.7854105041578072, 0.7898757751040074, 0.7925759189150624, 0.7985971292637313, 0.8061075480115836, 0.8047580439733255, 0.8071263060366879, 0.8151870175603407, 0.8138988757752189, 0.8220907999556168, 0.8213464643573353, 0.8313252234795044, 0.8336726030942142, 0.8373657852423065, 0.8385999870465978, 0.8493380654802035, 0.8481764937385621, 0.8495218938447099, 0.8548149359168301, 0.8598635895521451, 0.860767087912176, 0.8615777170526108, 0.8756985509306942, 0.873127290161931, 0.882539690176573, 0.889255476923952, 0.8930972196081968, 0.8845524085753145, 0.897355699787925, 0.8936887091175971, 0.9044870603640167, 0.9032808393083038, 0.9117394912899404, 0.9182815401934649, 0.918016989465801, 0.915618985425773, 0.9200665936409996, 0.9266996746498686, 0.9225342998180005, 0.9302694765052988, 0.9382334717802953, 0.9391798533483023, 0.9371792933513114, 0.9480929625873799, 0.9475565363396855, 0.9557953710086176, 0.949684477867954, 0.9628081974388872, 0.9586025929129776, 0.9671500348865715, 0.9671577953409125, 0.9782756320536379, 0.9768735319832879, 0.9914037346506372, 0.9979356611424824, 1.0046294017757653, 1.0102541723818361, 1.0120418200675732, 1.0105517932300974, 1.0207568114015542, 1.0161673184114097, 1.0271280478918796, 1.0335498003723589, 1.0322271673290606, 1.0344391456467725, 1.0457463774866715, 1.0477584163008886, 1.0535982314970256, 1.0597032073942292, 1.0608018686544618, 1.0671170896047186, 1.0694531079810332, 1.0761439225535532, 1.07935222572842, 1.0871026834267628, 1.0873220582123795, 1.0991383145281968, 1.1013878242602653, 1.1113702371967795, 1.1075578881534005, 1.1170863341181092, 1.1170644644531773, 1.1209956816619764, 1.1199356470347552, 1.1264833360630615, 1.1325208736730084, 1.1375682562520841, 1.1408859367656277, 1.1465617578914633, 1.1493593193366047, 1.1554081720045999, 1.1517373401804585, 1.1554842185447678, 1.1688520560064783, 1.1706094126322504, 1.1826581517003585, 1.184591008235458, 1.1969162971624987, 1.1982110361703553, 1.1932194163420815, 1.1972787061641108, 1.204366759136894, 1.2128054928902858, 1.2157551316808202, 1.2280706182352885, 1.2350135574410988, 1.236054599830343, 1.237137517953604, 1.249669138366895, 1.254335697377379, 1.261864701110305, 1.2693984907388554, 1.2737005218721293, 1.278514267404811, 1.2830728841000671, 1.2890947384126834, 1.2897811108510244, 1.2974167037135775, 1.2986314151168987, 1.3081493385013427, 1.3053359296463223, 1.3095523890958491, 1.313828143790921, 1.3250446143705028, 1.3214287716279307, 1.329549308131099, 1.3346106090986205, 1.3403529038587128, 1.354281115904311, 1.3571838618044316, 1.3605839675842022, 1.3678454316634674, 1.3732213897498904, 1.38877331615759, 1.3812311653509028, 1.3947459228308143, 1.3972169326156636, 1.4046439640475767, 1.4041605681650782, 1.4096753531377721, 1.4116923526962812, 1.4235437546197234, 1.4439742656300099, 1.4346009111289943, 1.4382479857853647, 1.4482471045211573, 1.448306385148552, 1.4569971325075768, 1.461256000501058, 1.4620661780285003, 1.47387850552241, 1.4802130458430294, 1.4776453439269388, 1.4853530723796113, 1.4884118300866767, 1.4952989486198547, 1.5019443019620085, 1.5009576539423752, 1.512460113638135, 1.5138866625405631, 1.5166665837781044, 1.5310479116191866, 1.5358964837293207, 1.5490192598709487, 1.5512196351191563, 1.5559865839255653, 1.5622510769656075, 1.5590875463509564, 1.5626066900390514, 1.569132942197251, 1.581580309751509, 1.58735643011157, 1.5968961945539504, 1.603045735527321, 1.6155446489039211, 1.6139296359641422, 1.6188818544480965, 1.622526749999818, 1.626175154227064, 1.642837788372074, 1.6447531845713914, 1.6432309969792132, 1.6495421126773189, 1.6647619355542267, 1.6586138376729362, 1.666477681759665, 1.6738466197139699, 1.675464638784213, 1.6820925957033492, 1.6827231469045596, 1.6917131633021536, 1.6942372955936065, 1.6980856384663372, 1.7034251934561737, 1.7204162740845463, 1.7142891234563968, 1.7318650855122064, 1.732783822253645, 1.7361976490372226, 1.738831675754666, 1.7478441166801506, 1.7546152410186435, 1.7615228964755028, 1.755291278439406, 1.7746119966488558, 1.784679546061048, 1.7938832036933803, 1.7947184970021046, 1.7948844445067973, 1.807579247383251, 1.8081357085728196, 1.8203156518613044, 1.828670514338391, 1.8334555744213008, 1.8334929089058742, 1.84162240389068, 1.8474706071832048, 1.8614931902543177, 1.8624886040576518, 1.8690291546207338, 1.8761619282450017, 1.8777916124042267, 1.8823468233775786, 1.888144941988181, 1.9004596791708182, 1.905338548146902, 1.9084860980314131, 1.9186054020630257, 1.929095905084272, 1.9354406095846424, 1.9347925652153601, 1.9387759515483365, 1.9507780048661285, 1.951096480576629, 1.954219107597425, 1.9588715392194802, 1.958695064086964, 1.9668446014267105, 1.9703387520100746, 1.9786651874904475, 1.9785836662361358, 1.9821190510071867, 1.9857322451934314, 1.9969553328645748, 2.004193015641764, 2.0041100312859412, 1.9998316440971362, 2.0193609939314716, 2.0265070282388224, 2.0156890441191666, 2.029002527530979, 2.023800118352033, 2.048234033455408, 2.045949629798821, 2.042375577469593, 2.055045459796482, 2.055180463971145, 2.058986917936218, 2.0679067885945357, 2.077893614872056, 2.078168861807426, 2.0869537339671416, 2.099581407645872, 2.095721501306665, 2.101954191343719, 2.1032205707091527, 2.1154206016031463, 2.1085555315513576, 2.124241746028508, 2.1398997209134483, 2.1459264337428405, 2.1490386915094595, 2.1466089813052673, 2.1539358020183297, 2.169771536960875, 2.168079005617216, 2.163762216346407, 2.1742381268466255, 2.183627719668488, 2.1887598651728757, 2.2002350285722363, 2.200356794649195, 2.196301247654153, 2.204509881479378, 2.2014754928695486, 2.2209770761922862, 2.219909569201222, 2.2180790087995335, 2.240350862482619, 2.240164821664101, 2.2412386804513424, 2.2512273470043485, 2.252189758860561, 2.2601138406469183, 2.272511415345363, 2.271313071272916, 2.2675234323649773, 2.2789393343385744, 2.289022622600685, 2.2947548669231055, 2.303568279251382, 2.29958943843413, 2.3067727042347226, 2.305863543386341, 2.3077676830526324, 2.316042823893962, 2.3238416478733566, 2.3251808832250083, 2.3281290230602933, 2.347108045908867, 2.3551222653403565, 2.365745289383876, 2.362283895013672, 2.3700997475893026, 2.3730447286341665, 2.376726977603972, 2.3922811737289513, 2.398351798862214, 2.394590332574774, 2.38967924154226, 2.408395206586661, 2.40664592496855, 2.4199755878448657, 2.428025318851745, 2.4262005789612067, 2.4334428268900314, 2.444021220675968, 2.4409411232238085, 2.4498109232699585, 2.445573689026452, 2.451840144645355, 2.4630011709977015, 2.4730844060824624, 2.475926432165567, 2.4855305658196287, 2.489525717073666, 2.494590858777332, 2.496036982379182, 2.5121675819864606, 2.5142360463330538, 2.5191966743948413, 2.5205718790681995, 2.525683881658972, 2.526474848396506, 2.531546005092985, 2.5360557766144995, 2.5471596184505527, 2.556385783356385, 2.5570476405523554, 2.5595108727790135, 2.558497022495268, 2.5628770786209225, 2.573931668056394, 2.581214901434828, 2.579568369764122, 2.5795712199619825, 2.590165098964944, 2.6027891931840466, 2.5969039748216987, 2.6106583316550678, 2.6160345623945873, 2.617733504288807, 2.6218545270700093, 2.6386627232007043, 2.627846272494918, 2.6305097891373976, 2.6322026044121314, 2.644619569427598, 2.6549156757277985, 2.6509281617283835, 2.6648765666539456, 2.668358083502818, 2.663998360172563, 2.6694753324951135, 2.6849059055152025, 2.6857572489073727, 2.6910017173340277, 2.696014805749281, 2.703712653798552, 2.7012659116369506, 2.7050094243104383, 2.7214252688161853, 2.716556868385203, 2.710803035152063, 2.7314362622410715, 2.717399812882697, 2.732107557765475, 2.7488203511951936, 2.7407436801035168, 2.7634364482523286, 2.765055789418294, 2.776168018280336, 2.770241889104043, 2.784003953621342, 2.791605344557396, 2.795440759891087, 2.801795063873028, 2.8094009956352046, 2.8084731843746886, 2.813557105412563, 2.809242378014898, 2.8119148073621667, 2.8317662412048388, 2.8396406366221023, 2.81939489631788, 2.8375706061630748, 2.8368535605323353, 2.843844533172958, 2.8550212081247928, 2.859479327280351, 2.878458876758031, 2.878221684312956, 2.880096737874369, 2.895835106114683, 2.8970033422306423, 2.8970841496504316, 2.9103337458635, 2.9078113690368896, 2.9167996420380384, 2.928552000774329, 2.927884999760191, 2.943009052250322, 2.9322581713217826, 2.9452510593169094, 2.952059330631677, 2.9585239454115726, 2.9526593797729745, 2.9609006842274495, 2.9750307428439338, 2.9713968856304778, 2.9814290044053777, 2.976314469162682, 2.984144505148191, 2.9920208674507154, 3.0005700584495867, 3.005574279456537, 3.003123575765354, 3.0128108123347626, 3.008669845918313, 3.020448200070162, 3.0356810821610862, 3.0367294270228338, 3.047538596852562, 3.048243838414019, 3.06327942723971, 3.0700140057720775, 3.0678087619746526, 3.087808362167176, 3.093397752455264, 3.0882246563588316, 3.0918360091453767, 3.103227899277216, 3.1014921030990483, 3.110170100404015, 3.117397279777392, 3.1284102785580856, 3.1255524919412045, 3.136692337215501, 3.137858879088665, 3.1429266460865177, 3.141761239165419, 3.1560459389822304, 3.1639413290700986, 3.1781595132487173, 3.1728182272194627, 3.189626419949275, 3.198762381593444, 3.198693274922956, 3.1893173756026405, 3.208785388093722, 3.212923490330395, 3.21605959961858, 3.2157981806852076, 3.2250236950463664, 3.217599326744307, 3.225204034035761, 3.235843240929284, 3.2377626480303485, 3.251409658430252, 3.2605509945293973, 3.2869022240351016, 3.270616256806012, 3.286100744108416, 3.292986762185761, 3.316377885520605, 3.3160621083820887, 3.3034553583882493, 3.3143318456856026, 3.3233471245245574, 3.322895587164543, 3.3217154591228497, 3.3265172858500107, 3.3193710483777723, 3.3305731214185825, 3.337858151061098, 3.3457008638917523, 3.34796756241401, 3.3554961633542746, 3.3449172878870326, 3.3636783842399964, 3.3748298176559013, 3.372262745380922, 3.375796913667841, 3.3821252392931886, 3.395495139007624, 3.3890327308206962, 3.3989974604741606, 3.3998909953593963, 3.419126517873265, 3.419410509747158, 3.4105085491392, 3.4310221749198098, 3.4293800582660605, 3.436589725806728, 3.4437153560062153, 3.452606182943549, 3.4505351905396853, 3.452051634847953, 3.4609967748047334, 3.4770289329670105, 3.4610747294768105, 3.475812383997531, 3.4744318065303212, 3.472346617924748, 3.489452443232166, 3.492870414798204, 3.507451168939562, 3.5000191729973578, 3.5103168642569593, 3.5001108196710207, 3.515158524061056, 3.5141649381527382, 3.509559510647202, 3.5287460673277304, 3.535631789588014, 3.5489672462357578, 3.549109922041607, 3.5459986039158844, 3.5456840373964424, 3.554385473733763, 3.575452050312494, 3.5796325660216346, 3.5775633597431757, 3.5861684011414985, 3.593043649652594, 3.599287008522119, 3.6141336366297026, 3.606163219828017, 3.610671590623801, 3.6191420744112226, 3.6246984621945213, 3.6395179508391906, 3.65253952854847, 3.6433710082193325, 3.649972809517834, 3.6729623417460715, 3.673069274906999, 3.6756712176210242, 3.681281771114778, 3.6756486933571333, 3.685328001056283, 3.6823733643076735, 3.684489010955383, 3.705036692676686, 3.6917579221672074, 3.7064517930339242, 3.704513615520181, 3.714644759451831, 3.704682709279297, 3.7238024743295766, 3.725459999056166, 3.7334601546006954, 3.7528636987099047, 3.755129004324406, 3.7613490496683855, 3.75920654836295, 3.7648465882682327, 3.779432862715289, 3.778377169169352, 3.772576996902836, 3.773321360585749, 3.7839659639341265, 3.791101659951498, 3.7800695067151224, 3.7864451681013493, 3.789121981889461, 3.8041918019293908, 3.8093833336118568, 3.8201976409814327, 3.828202836626129, 3.8352953175727382, 3.84729234333402, 3.8394082006954355, 3.835051026504411, 3.8520183312657936, 3.8582954644056335, 3.8556601608562393, 3.8542045081109455, 3.863115946758893, 3.8558915775358797, 3.8650971166738586, 3.8613780983187014, 3.881091599202531, 3.883190886734303, 3.8917616168220492, 3.8915037240188015, 3.8927184227209803, 3.8913476402209097, 3.8966198065286695, 3.892152999355429, 3.912564224913801, 3.9089457463237265, 3.92151061459327, 3.9239562663723864, 3.922068417294662, 3.919800229729518, 3.9166951724601655, 3.9351274685365536, 3.9384395270306145, 3.9346777289031833, 3.9437644933419906, 3.936494357968198, 3.9536558936484316, 3.9561697229023847, 3.9572190230055755, 3.9608514044946337, 3.973276012749825, 3.967679251076614, 3.982924166896721, 3.9855918703515294, 3.989593464021681, 3.987347875335074, 3.9866863239597095, 4.017236547413373, 4.025248779084051, 4.007923199705517, 4.0133312989328305, 4.012130708237741, 4.015449254889724, 4.013385040707887, 4.016863594851217, 4.018408727245013, 4.034777165919792, 4.036273692554844, 4.048803459263011, 4.044837026345291, 4.058459133839439, 4.054846923796307, 4.055944143440357, 4.052757648455146, 4.063667830445156, 4.069815285984701, 4.068933148987162, 4.068741807054944, 4.082894226218488, 4.089747206627502, 4.089200713435499, 4.092301368214931, 4.093314131120169, 4.101510038238024, 4.104295415542674, 4.111810656791515, 4.105268933001375, 4.116281668342687, 4.124848798062771, 4.1062608342878955, 4.120331359411642, 4.122193634726509, 4.119984913722625, 4.128912836208453, 4.149421130921625, 4.154245593867278, 4.162661213404242, 4.1340422570037365, 4.157444408735421, 4.165626307210878, 4.157350467835115, 4.177026529900933, 4.165200260247652, 4.189210450285226, 4.186364260793816, 4.198904487999569, 4.18652223535163, 4.206222426962784, 4.215032671714815, 4.213573036894318, 4.211641927852655, 4.223111557796529, 4.23250979243656, 4.242213380484885, 4.236561746878277, 4.233376494360231, 4.2572553154362955, 4.252710263465212, 4.252591347978227, 4.260092651764278, 4.253168400197376, 4.275880586656382, 4.274794648664469, 4.264632328633274, 4.263196343362046, 4.26545598305147, 4.26831458361821, 4.275319234003682, 4.2743766864282255, 4.272374238528845, 4.280202814053793, 4.2726780216604325, 4.293715787630188, 4.294059934412608, 4.309147402420645, 4.310141373284201, 4.307489266989289, 4.31907760264539, 4.327775293612654, 4.335172807201535, 4.333282889088618, 4.355273591785239, 4.350452656084554, 4.35558191817903, 4.3506586811931856, 4.362628775454669, 4.373436926377079, 4.375132175397308, 4.380258575437571, 4.384242806726377, 4.3761397309041214, 4.379978815101655, 4.387010848884682, 4.395281835721203, 4.413357336091692, 4.413008393910009, 4.417714209272511, 4.418734046956104, 4.420656602063733, 4.434319775106122, 4.410566701979822, 4.4296391587267765, 4.433625686576642, 4.43564376724783, 4.450404437765879, 4.448138672407324, 4.457116611302947, 4.452805959827576, 4.462327267193746, 4.465789870075972, 4.462681570864239, 4.465713525130001, 4.487558796628859, 4.488403145351388, 4.492751927427075, 4.483526997210504, 4.487305097822236, 4.488553332811315, 4.477316291031826, 4.501266344225562, 4.50139358913823, 4.509514006288506, 4.522326879160269, 4.5336920235615885, 4.524006838246235, 4.52876760688242, 4.5495849322631265, 4.531875963660778, 4.528475631820938, 4.545996398456713, 4.543253339711504, 4.5372441661478184, 4.5758523991033515, 4.577148075379013, 4.580489946548051, 4.577327181924362, 4.583012486818097, 4.572079088939173, 4.577296591281402, 4.594979026758541, 4.591357047622021, 4.600380838121313, 4.582536438801536, 4.579374476389007, 4.590695514955705, 4.600974974392192, 4.611245567842164, 4.6214109683967255, 4.624798678543073, 4.623171007368709, 4.628454844763266, 4.607933905262743, 4.626540620513722, 4.616294820851354, 4.640054300951685, 4.646465453134109, 4.656967066196069, 4.664745069255328, 4.6624881554500215, 4.658664207505112, 4.670360629589322, 4.670837618781306, 4.678901408194469, 4.683937883396143, 4.696034897270518, 4.689147759966664, 4.702107884329716, 4.684852090887405, 4.694939397013602, 4.708067033499619, 4.698659295298005, 4.7033472878635045, 4.719782926390268, 4.7221800044432225, 4.721258821052652, 4.725260597083615, 4.719240522420127, 4.750165791472144, 4.74863412963574, 4.740174630205284, 4.7435094839582375, 4.763178954638808, 4.770603846064886, 4.756189596773783, 4.771808654463373, 4.78015493865667, 4.780733906289447, 4.778481594633485, 4.774226364349997, 4.798941040718889, 4.793692090311642, 4.803246527923502, 4.805865658492991, 4.813689397173587, 4.81361581326027, 4.790102581881431, 4.802434623344745, 4.8011136457418715, 4.814052651849973, 4.8094671032429845, 4.814213291798929, 4.832680397446488, 4.815093734653716, 4.823816166180925, 4.830920566171795, 4.838894904129927, 4.848327042362669, 4.8403942330541705, 4.848373561429912, 4.863557730390693, 4.860960645328258, 4.855816109740204, 4.866864493688893, 4.852429602135201, 4.860780782840868, 4.873602692044357, 4.871713334127852, 4.876126601846379, 4.891604728407011, 4.891023855167275, 4.885734190338907, 4.889075826877415, 4.90022803070827, 4.897136569669359, 4.893680999192546, 4.895342739193466, 4.917490587084427, 4.913285580946834, 4.916135109789155, 4.912799755211208, 4.920714686920859, 4.918003265789476, 4.915910064578135, 4.9350915735464, 4.937729613680909, 4.930663950376502, 4.936585958952343, 4.935276384289952, 4.942996238674597, 4.944356706951402, 4.933284161711886, 4.953621938205178, 4.949795817340138, 4.950571964068995, 4.944925290835468, 4.946693677726158, 4.952094414428986, 4.967042188269359, 4.967514449171447, 4.979773798547194, 4.980989320273211, 4.976520984049067, 4.987554968613564, 4.98820248170909, 4.983507388296746, 4.989203325719894, 5.008395678669018, 5.007732673424746, 5.026137768529279, 5.032315079552557, 5.025183353314883, 5.047449660642505, 5.0486529027207885, 5.0535960209769275, 5.050807633158379, 5.050474417156962, 5.036771833656808, 5.038756392178262, 5.069196327836051, 5.076779508610029, 5.0714657309049445, 5.078174915278317, 5.062012752269555, 5.060431382677233, 5.076573046002209, 5.0804755597285745, 5.089685240514744, 5.094178704859407, 5.0996523426514555, 5.101619979230045, 5.087188004688356, 5.117120686395215, 5.12174830271631, 5.109530594873415, 5.11550552820789, 5.10209845503085, 5.113194240529122, 5.130633645294961, 5.113843920120236, 5.11689957990885, 5.118985056305954, 5.116048409521615, 5.140865214881845, 5.133509480527914, 5.1160143845704775, 5.134597311730524, 5.129669522896937, 5.126273339625813, 5.150904538216568, 5.157719385688181, 5.169446469449439, 5.171297950861568, 5.1652369665761855, 5.165203658516388, 5.196930047992349, 5.204687028092812, 5.203695775436612, 5.206998302080424, 5.2201582854658755, 5.215191562147744, 5.220125377314685, 5.21585086667204, 5.218151260392445, 5.228995385920722, 5.209219470411272, 5.227499276607955, 5.225021181937977, 5.236457313525057, 5.232996802044772, 5.241663333308208, 5.245133018322315, 5.265285106342354, 5.265140932458579, 5.265057509342061, 5.265406350024002, 5.278685713933007, 5.289301892493717, 5.269025439145917, 5.285819011531392, 5.284736536923055, 5.297580333937828, 5.30496612993886, 5.2999820672316185, 5.310120162752333, 5.291213834398393, 5.312715828171793, 5.288793190852548, 5.309027778125187, 5.313177886317272, 5.314737245196362, 5.318324384891158, 5.319591557101129, 5.308539541562502, 5.311079178933269, 5.304099087491597, 5.3215545393370105, 5.34157163277216, 5.33332214081376, 5.339254733415687, 5.338566006790868, 5.34123398579328, 5.330790263946036, 5.360542393718073, 5.359647801552383, 5.353414637877853, 5.376290326178863, 5.3613818948213385, 5.378794778353999, 5.373975014093336, 5.385668603578985, 5.3810061828615465, 5.386914354346045, 5.383532418882578, 5.36770143596198, 5.383591508250844, 5.404813685498638, 5.422586065217361, 5.419508602478924, 5.444932495361726, 5.425933720752872, 5.433900294410952, 5.44052733329468, 5.455036836094326, 5.44495530298895, 5.439199886929083, 5.458462958480586, 5.46705275866572, 5.454179119233719, 5.45257910355288, 5.464293815933788, 5.481189482019771, 5.476903148299386, 5.474207138910551, 5.487166549545939, 5.4789024616395485, 5.478319941854543, 5.469910428612319, 5.479393522203655, 5.499084647992668, 5.478064216713739, 5.477440123032512, 5.487838381433021, 5.5029377981051395, 5.506997902811508, 5.506745304303043, 5.505182321279909, 5.498800040859279, 5.49623063857842, 5.495226240202066, 5.502641965028468, 5.506293246428343, 5.505370492399439, 5.516874854411327, 5.531937583822214, 5.526321028121727, 5.526120740165124, 5.532028956065573, 5.528508856553631, 5.5604827975608755, 5.556906754547548, 5.571848735726192, 5.581565865668634, 5.562293618750273, 5.549661722557072, 5.564797150040987, 5.566379649045364, 5.570550052125838, 5.594593087685003, 5.588860969468029, 5.601726575240659, 5.58660443908879, 5.573944376506066, 5.589784020395911, 5.597671324526167, 5.601736183413908, 5.621892633483599, 5.612947257009963, 5.6020041872527955, 5.617891886076082, 5.634623009828453, 5.649814782566324, 5.656498591839859, 5.6486119796063665, 5.642055243185199, 5.657698562249926, 5.659688592889083, 5.64757053581681, 5.662661525403058, 5.6508665465354, 5.63122647630552, 5.645687668075065, 5.6657646335044145, 5.649152726524201, 5.664688627016621, 5.65917492669141, 5.688694362663325, 5.678085241669386, 5.687803793097225, 5.690109322995746, 5.699660003208731, 5.702923799226195, 5.710513029840869, 5.7061809604425155, 5.711469184882235, 5.719408939794629, 5.71202005032867, 5.72514362870721, 5.71668675949449, 5.71713199465407, 5.74031333198895, 5.72492821974501, 5.71977727776269, 5.732692313718655, 5.728117942914394, 5.723621915760951, 5.74446328649671, 5.738988633074772, 5.752244354563597, 5.744297588800658, 5.7674266397118545, 5.754909537062097, 5.768557667869583, 5.758363546125234, 5.784883688795769, 5.778028349465707, 5.762392071108726, 5.7629197078544125]\n",
      "[4.498395010436141, 3.7659469382492556, 4.250152470641717, 4.47628599137772, 4.832897515961173, 5.104850826461035, 5.412916458208588, 5.597429816372896, 5.804054199319218, 5.908302990952189, 6.041571297928105, 6.192376567882752, 6.290169808749142, 6.4138568988220594, 6.509017227053468, 6.580371578375603, 6.640484333339927, 6.700479935219623, 6.775078162872881, 6.770515353924674, 6.843364641998531, 6.881245744172266, 6.910811623900621, 6.968227967326593, 7.019253737565894, 7.044851798624704, 7.077433043233671, 7.110911604895232, 7.158783293672764, 7.172874273742578, 7.179501295970385, 7.25464236057704, 7.2284346786049865, 7.275708415998909, 7.263044722871742, 7.253821561291036, 7.2861998231764575, 7.306662777021142, 7.315190781925948, 7.328673458543316, 7.302940813218607, 7.3514691591443775, 7.325565133446812, 7.34299880989118, 7.374947049493147, 7.389660710609979, 7.418426593124628, 7.405829116484176, 7.3847416381945, 7.397254255989004, 7.403986248344881, 7.409654245814265, 7.442028278137444, 7.411533126655887, 7.401392005157393, 7.422575385019327, 7.453884507482643, 7.429361504774456, 7.426276337163673, 7.427306529693032, 7.405463604081867, 7.387063269274248, 7.400240069114142, 7.413995547580048, 7.41142853136482, 7.398648186081763, 7.416170023064656, 7.419486930892335, 7.389962527153916, 7.4311602921136775, 7.427989231106338, 7.442529866034665, 7.4295622982163545, 7.421239071394674, 7.420449765076107, 7.418671707630954, 7.389932467062476, 7.451447174881221, 7.433877808069439, 7.451360913451695, 7.427122906466423, 7.43612993218931, 7.417278090197928, 7.420644348946345, 7.395933950127712, 7.425866827865618, 7.4230689167982415, 7.460579340258741, 7.422622126497395, 7.421198721447858, 7.400096466695346, 7.4144630209670845, 7.433910187925518, 7.394537622578888, 7.399273347186275, 7.404064641671764, 7.4124257795085535, 7.439183942727435, 7.435919268365416, 7.445286233988645, 7.409024222601777, 7.39264520783056, 7.4212213179990725, 7.424235431142938, 7.414343055790629, 7.465533800310423, 7.4808707122878015, 7.454564218680844, 7.4477726222230665, 7.440324139065427, 7.469506982414791, 7.4905833786740175, 7.487881762244432, 7.436862902898739, 7.4468949718622035, 7.455050174387603, 7.476818093499306, 7.494806854166206, 7.506469068610222, 7.4977936718993865, 7.462061371089343, 7.483854455973934, 7.488960883253434, 7.480576017743415, 7.478298327667429, 7.49068088867594, 7.490560488620521, 7.439453130392557, 7.472125423127676, 7.500708983566141, 7.510835847940027, 7.544305504837156, 7.510978490314774, 7.520780803112163, 7.52467201725165, 7.513476128388301, 7.557011295799326, 7.530988278553287, 7.519943379984268, 7.515235962549491, 7.540252847914717, 7.542065417170011, 7.590082104737017, 7.609908258191718, 7.587086696853487, 7.566069948391556, 7.57568248276261, 7.591203363271207, 7.574144199093193, 7.557936696528978, 7.608449590753244, 7.644528981000192, 7.621958392934206, 7.661508432219025, 7.6499707777840475, 7.616092898684861, 7.626279928734541, 7.626274063852232, 7.6293962885721465, 7.6379408398445685, 7.629395078721211, 7.692084036501701, 7.658878347177963, 7.6848740958313355, 7.727985992910818, 7.698047438631993, 7.727408298082645, 7.73367741623913, 7.764772297670947, 7.730874977535017, 7.756870253742486, 7.77285877748557, 7.791460715040881, 7.790923045628926, 7.754376567122437, 7.792549948300728, 7.770595061841151, 7.786561019277184, 7.786614617916312, 7.800240072880487, 7.781946254380951, 7.832152525000815, 7.849212086430531, 7.843228331571503, 7.7952842452666875, 7.833235246717873, 7.808654899111717, 7.856387031625436, 7.813326967442343, 7.860825274122489, 7.804194191294402, 7.818399994647051, 7.834036826918508, 7.834324048512837, 7.825516913913495, 7.874340438157545, 7.826019892788835, 7.8731961862306115, 7.898196952301849, 7.891493787832095, 7.846836040903427, 7.8780172332146385, 7.8633505831072155, 7.886668070724774, 7.915552021309486, 7.898982537490651, 7.94864389428676, 7.886338257243118, 7.914918052337674, 7.934136556938448, 7.951427225158661, 7.97375946786127, 7.9374960500907, 7.958623053217709, 8.01124179541193, 7.977744964358235, 7.9983284276657205, 8.02302108447706, 7.9826655984437584, 7.989431600462586, 7.997614494057939, 8.03505976807038, 8.043810227956811, 8.0408765553089, 8.049167073895525, 8.017872084238471, 8.005660168091813, 8.022174394993877, 8.04787933585772, 8.060222416158593, 8.0382326491562, 8.072234085743434, 8.121729575024801, 8.073568379173864, 8.110137053674359, 8.04434699600346, 8.094526813211564, 8.105723694739709, 8.11048017245208, 8.139082825594643, 8.125791144118898, 8.163996032487946, 8.110346429388889, 8.165295150353227, 8.124377981151747, 8.1838492223417, 8.122977999270894, 8.145714920409047, 8.176155313091686, 8.191706168038133, 8.200339339265986, 8.223284139569481, 8.214664218204137, 8.206268398564987, 8.235969388496416, 8.20431750504611, 8.207063348043702, 8.224128442852203, 8.207079013711448, 8.267056850155784, 8.261626868778073, 8.257701985502946, 8.219836298237022, 8.293634669570698, 8.270078190182623, 8.312723976575441, 8.319120508368751, 8.35235977432225, 8.35377917014436, 8.2993231160145, 8.383835580205046, 8.377135564144135, 8.382809586800743, 8.351711439011472, 8.410789224799029, 8.38557636491117, 8.427561206831621, 8.415978186080942, 8.46315504954133, 8.475691553926634, 8.495577509982416, 8.458785065126547, 8.506532990280412, 8.463178371047645, 8.412910497738078, 8.457350143879367, 8.399796523395437, 8.451554539437321, 8.41851203617632, 8.4508869243357, 8.43375695740375, 8.435808922050995, 8.452778623550822, 8.454757124018176, 8.469885575580118, 8.47837978728428, 8.47911120669861, 8.48039678772566, 8.497714254348582, 8.516908117822807]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(eval_losses, label='Evaluation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(train_losses)\n",
    "print(eval_losses)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS well done ! EOS\n",
      "BOS 做 得 好 ！ EOS\n",
      "幾時出去！\n",
      "\n",
      "BOS be serious . EOS\n",
      "BOS 咪 玩 啦 。 EOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/xc_yp0q16ps2v4zqf84_bbxc0000gn/T/ipykernel_77389/1649215179.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  attn.data.masked_fill(mask, -1e6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "佢哋會減肥。\n",
      "\n",
      "BOS be serious . EOS\n",
      "BOS 認 真 啲 啦 。 EOS\n",
      "男模特別地。\n",
      "\n",
      "BOS it matters . EOS\n",
      "BOS 重 要 。 EOS\n",
      "升職業關係㗎。\n",
      "\n",
      "BOS no problem . EOS\n",
      "BOS 冇 問 題 。 EOS\n",
      "冇到佢嚟。\n",
      "\n",
      "BOS say please . EOS\n",
      "BOS 講 唔 該 吖 。 EOS\n",
      "你講得好多。\n",
      "\n",
      "BOS start over . EOS\n",
      "BOS 重 新 嚟 過 啦 。 EOS\n",
      "啱晒啲衫。\n",
      "\n",
      "BOS who talked ? EOS\n",
      "BOS 邊 個 講 嘢 ？ EOS\n",
      "邊個都唔係㗎？\n",
      "\n",
      "BOS who yelled ? EOS\n",
      "BOS 邊 個 嗌 呀 ？ EOS\n",
      "嗰個高佬邊個嚟？\n",
      "\n",
      "BOS please stay . EOS\n",
      "BOS 請 留 步 。 EOS\n",
      "唔該你，唔該。\n",
      "\n",
      "BOS please stay . EOS\n",
      "BOS 請 你 留 低 啦 。 EOS\n",
      "唔該你。\n",
      "\n",
      "BOS talk slower . EOS\n",
      "BOS 講 慢 少 少 。 EOS\n",
      "個嘢講嘅。\n",
      "\n",
      "BOS abandon ship ! EOS\n",
      "BOS 棄 船 呀 ！ EOS\n",
      "買正一樣嘢喎！\n",
      "\n",
      "BOS drive slowly . EOS\n",
      "BOS 揸 慢 啲 。 EOS\n",
      "喺九龍笪。\n",
      "\n",
      "BOS speak slower . EOS\n",
      "BOS 講 慢 少 少 。 EOS\n",
      "男模特兒公仔。\n",
      "\n",
      "BOS tom screamed . EOS\n",
      "BOS T o m 尖 叫 。 EOS\n",
      "將啲花出去。\n",
      "\n",
      "BOS say something . EOS\n",
      "BOS 講 啲 嘢 啦 。 EOS\n",
      "佢一便講嘢啊。\n",
      "\n",
      "BOS drive carefully . EOS\n",
      "BOS 小 心 啲 揸 車 。 EOS\n",
      "香港迪士嘅電話。\n",
      "\n",
      "BOS UNK christmas ! EOS\n",
      "BOS 聖 誕 節 快 樂 ！ EOS\n",
      "海鮮好禁嘅！\n",
      "\n",
      "BOS i get it . EOS\n",
      "BOS 我 明 。 EOS\n",
      "我有利。\n",
      "\n",
      "BOS i get it . EOS\n",
      "BOS 我 g e t 到 。 EOS\n",
      "我仆喺家企喺門。\n",
      "\n",
      "BOS i 'm here . EOS\n",
      "BOS 我 喺 度 。 EOS\n",
      "我有覺意好似落地。\n",
      "\n",
      "BOS do n't cry . EOS\n",
      "BOS 唔 好 喊 啦 。 EOS\n",
      "唔好咁大嘅。\n",
      "\n",
      "BOS i 'm angry . EOS\n",
      "BOS 我 好 嬲 。 EOS\n",
      "我會話畀你。\n",
      "\n",
      "BOS i 'm ready . EOS\n",
      "BOS 我 準 備 好 喇 。 EOS\n",
      "我有打嘢。\n",
      "\n",
      "BOS i 'm right . EOS\n",
      "BOS 我 係 啱 嘅 。 EOS\n",
      "我鑿得好少。\n",
      "\n",
      "BOS it 's easy . EOS\n",
      "BOS 好 容 易 噃 。 EOS\n",
      "即係咁多。\n",
      "\n",
      "BOS come on in ! EOS\n",
      "BOS 入 嚟 啦 ！ EOS\n",
      "五個西瓜！\n",
      "\n",
      "BOS do n't move . EOS\n",
      "BOS 咪 郁 。 EOS\n",
      "唔好整色整水。\n",
      "\n",
      "BOS how 's work ? EOS\n",
      "BOS 返 工 返 成 點 呀 ？ EOS\n",
      "喺邊點啊？\n",
      "\n",
      "BOS i know him . EOS\n",
      "BOS 我 識 佢 。 EOS\n",
      "我夠知佢都係陰吓佢。\n",
      "\n",
      "BOS is it safe ? EOS\n",
      "BOS 安 唔 安 全 㗎 ？ EOS\n",
      "一蛇係幾多纈？\n",
      "\n",
      "BOS it 's windy . EOS\n",
      "BOS 好 大 風 。 EOS\n",
      "呢個係我嘅暗個嘅。\n",
      "\n",
      "BOS let me die . EOS\n",
      "BOS 俾 我 死 咗 去 算 啦 。 EOS\n",
      "我同埋你條頭。\n",
      "\n",
      "BOS take a nap . EOS\n",
      "BOS UNK 一 陣 啦 。 EOS\n",
      "將個人踩埋。\n",
      "\n",
      "BOS time is up . EOS\n",
      "BOS 夠 鐘 喇 。 EOS\n",
      "夠時候喇。\n",
      "\n",
      "BOS we are men . EOS\n",
      "BOS 我 哋 係 男 人 。 EOS\n",
      "我哋哋哋到冇人。\n",
      "\n",
      "BOS what is it ? EOS\n",
      "BOS 呢 個 咩 嚟 㗎 ？ EOS\n",
      "佢身上嘅大題？\n",
      "\n",
      "BOS who did it ? EOS\n",
      "BOS 邊 個 做 架 ？ EOS\n",
      "搵邊位\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])  #原来的英文\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])  #原来的中文\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))           #翻译后的中文\n",
    "\n",
    "\n",
    "for i in range(10, 49):\n",
    "    translate_dev(i)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/xc_yp0q16ps2v4zqf84_bbxc0000gn/T/ipykernel_77389/1649215179.py:25: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1647.)\n",
      "  attn.data.masked_fill(mask, -1e6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS well done ! EOS\n",
      "BOS 做 得 好 ！ EOS\n",
      "喇！\n",
      "\n",
      "BOS be serious . EOS\n",
      "BOS 咪 玩 啦 。 EOS\n",
      "佢好容易㗎。\n",
      "\n",
      "BOS be serious . EOS\n",
      "BOS 認 真 啲 啦 。 EOS\n",
      "嗰隻生會好有電。\n",
      "\n",
      "BOS it matters . EOS\n",
      "BOS 重 要 。 EOS\n",
      "佢做緊嘢。\n",
      "\n",
      "BOS no problem . EOS\n",
      "BOS 冇 問 題 。 EOS\n",
      "你間超級佢。\n",
      "\n",
      "BOS say please . EOS\n",
      "BOS 講 唔 該 吖 。 EOS\n",
      "你話佢哋搭生意。\n",
      "\n",
      "BOS start over . EOS\n",
      "BOS 重 新 嚟 過 啦 。 EOS\n",
      "就去咗佢。\n",
      "\n",
      "BOS who talked ? EOS\n",
      "BOS 邊 個 講 嘢 ？ EOS\n",
      "咪喐？\n",
      "\n",
      "BOS who yelled ? EOS\n",
      "BOS 邊 個 嗌 呀 ？ EOS\n",
      "邊個噉行啊？\n",
      "\n",
      "BOS please stay . EOS\n",
      "BOS 請 留 步 。 EOS\n",
      "唔該你。\n",
      "\n",
      "BOS please stay . EOS\n",
      "BOS 請 你 留 低 啦 。 EOS\n",
      "唔該你。\n",
      "\n",
      "BOS talk slower . EOS\n",
      "BOS 講 慢 少 少 。 EOS\n",
      "佢個女打波。\n",
      "\n",
      "BOS abandon ship ! EOS\n",
      "BOS 棄 船 呀 ！ EOS\n",
      "隻船食水嘅！\n",
      "\n",
      "BOS drive slowly . EOS\n",
      "BOS 揸 慢 啲 。 EOS\n",
      "呢粒事我喺度整緊。\n",
      "\n",
      "BOS speak slower . EOS\n",
      "BOS 講 慢 少 少 。 EOS\n",
      "呢個煲嘅大戲。\n",
      "\n",
      "BOS tom screamed . EOS\n",
      "BOS T o m 尖 叫 。 EOS\n",
      "藥煲衡得。\n",
      "\n",
      "BOS say something . EOS\n",
      "BOS 講 啲 嘢 啦 。 EOS\n",
      "大家都係老細嘅。\n",
      "\n",
      "BOS drive carefully . EOS\n",
      "BOS 小 心 啲 揸 車 。 EOS\n",
      "機嘅功課室係心。\n",
      "\n",
      "BOS UNK christmas ! EOS\n",
      "BOS 聖 誕 節 快 樂 ！ EOS\n",
      "崖門！\n",
      "\n",
      "BOS i get it . EOS\n",
      "BOS 我 明 。 EOS\n",
      "我諗到都係條線。\n",
      "\n",
      "BOS i get it . EOS\n",
      "BOS 我 g e t 到 。 EOS\n",
      "我好鍾意。\n",
      "\n",
      "BOS i 'm here . EOS\n",
      "BOS 我 喺 度 。 EOS\n",
      "我而家喺呢度。\n",
      "\n",
      "BOS do n't cry . EOS\n",
      "BOS 唔 好 喊 啦 。 EOS\n",
      "唔好行，開刀。\n",
      "\n",
      "BOS i 'm angry . EOS\n",
      "BOS 我 好 嬲 。 EOS\n",
      "我高佬踩卜。\n",
      "\n",
      "BOS i 'm ready . EOS\n",
      "BOS 我 準 備 好 喇 。 EOS\n",
      "我好鍾意食晏。\n",
      "\n",
      "BOS i 'm right . EOS\n",
      "BOS 我 係 啱 嘅 。 EOS\n",
      "我講嘢乸手。\n",
      "\n",
      "BOS it 's easy . EOS\n",
      "BOS 好 容 易 噃 。 EOS\n",
      "升咗五咪五鑊啦。\n",
      "\n",
      "BOS come on in ! EOS\n",
      "BOS 入 嚟 啦 ！ EOS\n",
      "咁門都唔熄啦！\n",
      "\n",
      "BOS do n't move . EOS\n",
      "BOS 咪 郁 。 EOS\n",
      "唔好爆我啲嘢。\n",
      "\n",
      "BOS how 's work ? EOS\n",
      "BOS 返 工 返 成 點 呀 ？ EOS\n",
      "邊個工㗎？\n",
      "\n",
      "BOS i know him . EOS\n",
      "BOS 我 識 佢 。 EOS\n",
      "我夠知佢。\n",
      "\n",
      "BOS is it safe ? EOS\n",
      "BOS 安 唔 安 全 㗎 ？ EOS\n",
      "膠蛇幾長？\n",
      "\n",
      "BOS it 's windy . EOS\n",
      "BOS 好 大 風 。 EOS\n",
      "我同吓條濕晒。\n",
      "\n",
      "BOS let me die . EOS\n",
      "BOS 俾 我 死 咗 去 算 啦 。 EOS\n",
      "我同佢着數我。\n",
      "\n",
      "BOS take a nap . EOS\n",
      "BOS UNK 一 陣 啦 。 EOS\n",
      "將幅相睇化電掣度。\n",
      "\n",
      "BOS time is up . EOS\n",
      "BOS 夠 鐘 喇 。 EOS\n",
      "夠時間時都係喼住。\n",
      "\n",
      "BOS we are men . EOS\n",
      "BOS 我 哋 係 男 人 。 EOS\n",
      "我哋好鍾意時。\n",
      "\n",
      "BOS what is it ? EOS\n",
      "BOS 呢 個 咩 嚟 㗎 ？ EOS\n",
      "乜嘢咁有啊？\n",
      "\n",
      "BOS who did it ? EOS\n",
      "BOS 邊 個 做 架 ？ EOS\n",
      "邊個邊個係邊個㗎？\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open the file for appending\n",
    "with open('output.txt', 'a') as f:\n",
    "    # Loop through the numbers and translate them\n",
    "    for i in range(10, 49):\n",
    "        en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])  #原来的英文\n",
    "        f.write(en_sent + '\\n')\n",
    "\n",
    "        cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])  #原来的中文\n",
    "        f.write(cn_sent + '\\n')\n",
    "\n",
    "        mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "        mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "        bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "\n",
    "        translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "        translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "        f.write('\\n')\n",
    "        trans = []\n",
    "        for word in translation:\n",
    "            if word != \"EOS\":\n",
    "                trans.append(word)\n",
    "            else:\n",
    "                break\n",
    "        translated = \"\".join(trans)           #翻译后的中文\n",
    "        f.write(translated + '\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "        # Print the output to the console\n",
    "        print(en_sent)\n",
    "        print(cn_sent)\n",
    "        print(translated)\n",
    "        print()\n",
    "\n",
    "# Close the file\n",
    "f.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
